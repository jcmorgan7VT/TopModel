---
title: "MapsForStoryboard1"
format: html
editor_options: 
  chunk_output_type: console
---
Not just maps for storyboard but also figures for storyboard

11/13/24
Making more finalized versions of maps for storyboard

1/20/25
Revisiting, and actually working on figures

```{r setup}
#loading packages
library(pacman)
p_load(tidyverse, terra, tidyterra, whitebox, scales, wesanderson, caret, plotly,ggnewscale, sf, rgeoboundaries, elevatr, patchwork, ggspatial, zoo)
#whitebox::install_whitebox()

#reading in final format data for summer 23
data_23 <- read_csv("./DataForMary/HB_stic.csv")
#reading in final format data for summer 24
data_24 <- read_csv("./summer2024/STICS2024.csv")

data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

bind23 <- data_23 %>% 
  select(datetime, ID, wshed, binary, mins)
bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)
```
# Figure 1: maps and distributions of topo variables
## Maps
```{r NH-map-HB-location}
#make map of NH, pane to show where hubbard brook is
usa <- rgeoboundaries::geoboundaries("USA", "adm1")
NH <- usa[usa$shapeName == "New Hampshire",]
NH_outline <- vect(NH)
mdt <- get_elev_raster(locations = NH, z = 10, clip = "locations")
plot(mdt)
big <- rast(mdt) %>% 
  crop(NH_outline) %>% 
  mask(NH_outline)
writeRaster(big, "./w3_dems/NH.tif", overwrite = TRUE)

NH_path <- "./w3_dems/NH.tif"
hillshade_out <- "./w3_dems/NH_hillshade.tif"
wbt_hillshade(
  dem = NH_path,
  output = hillshade_out,
)
hill <- rast(hillshade_out)
hill2 <- crop(hill, NH_outline)
hill3 <- mask(hill2, NH_outline)
plot(hill)

#read in shapefil of watershed boundaries for HB
#path to file
HB_bounds <- "./HB/hbef_wsheds/hbef_wsheds.shp"
sheds <- vect(HB_bounds)
sheds <- terra::project(sheds, crs(hill))
plot(sheds)
#create boundaries of HB for showing location in NH
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
plot(m1)
HB_bounds <- vect(ext(m1, cells=NULL), crs = crs(m1))
HB_bounds <- terra::project(HB_bounds, crs(hill))

plot(HB_bounds)


#use bounds of sheds to crop NH dem
ybounds <- c(43.916,43.96219)
xbounds <- c(-71.80451, -71.69687)
plot(hill, xlim = xbounds, ylim = ybounds)
gauge <- c(43.939574, -71.703100)


NH_map <- ggplot()+
  geom_spatraster(data = hill3)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"), na.value = NA)+
    new_scale_fill() +
  geom_spatraster(data = big, alpha = 0.5)+
  
  #plot rectangle to show HB extent
      geom_sf(data = HB_bounds, color = "black", alpha = 0, lwd = 1.2) +

    #geom_point(aes(y=43.939574, x=-71.703100), colour="black", pch = 20, size = 8)+
   scale_fill_hypso_c(palette = "dem_screen")
NH_map

ggsave("NH.png", plot = NH_map, scale = 1, limitsize = FALSE, #height = 11,width = 4, units = "in"
       bg = NULL,)

ggplot()+
  geom_spatraster(data = hill3)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"), na.value = NA)+
    new_scale_fill() +
  geom_spatraster(data = big, alpha = 0.5)+
    geom_sf(data = sheds, fill = "darkslategray3", color = "black", alpha = 0.3) +
   scale_fill_hypso_c(palette = "dem_screen")+
  lims(x = xbounds,
       y = ybounds)
```
```{r whole-Valley}
#use HB dem from previous maps, but just don't crop it, include outline of the whole valley, and the outlines of each study shed
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
plot(m1)

#old hillshade
valley_hill <- rast("./HB/1m hydro enforced DEM/wall_hillshade.tif")
#old hillshade has a different angle, emphasizes the lineaments in the east better, but new one looks niver
#see if a newly calculated one looks better
hillshade_out <- "./HB/1m hydro enforced DEM/new_hillshade.tif"
wbt_hillshade(
  dem = dem,
  output = hillshade_out,
)
valley_hill2 <- rast(hillshade_out)
plot(valley_hill2)


#get the outlines for each watershed
#W3
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)
#FB
fb_shed <- "./fb_dems/fb_shed.tif"
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)
#ZZ
zz_shed <- "./zz_dems/zz_shed.tif"
zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)



valley_plot <- ggplot()+
  geom_spatraster(data = valley_hill2, maxcell = 5e+05)+#make max cells bigger on final version
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = m1, alpha = 0.5)+
  geom_sf(data = fb_outline, fill = NA, color = "black", alpha = 0.3, lwd = 1.5) +
  geom_sf(data = w3_outline, fill = NA, color = "black", alpha = 0.3, lwd = 1.5) +
  geom_sf(data = zz_outline, fill = NA, color = "black", alpha = 0.3, lwd = 1.5) +

  #geom_sf(data = fb_net, colour = "darkslategray3") +#, lwd = 3) +
    #geom_sf(data = lcc, colour = "midnightblue") + #, pch = 19, size = 6) +
  #geom_sf(data = fb_pour, colour = "black") + #, pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen")+#, limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0.5, "cm"), 
                              pad_y = unit(0.5, "cm")) #, line_width = 3, text_cex = 5, tick_height = 20)

ggsave("valley.png", plot = valley_plot, limitsize = FALSE, units = "in", bg = NULL,)

#combine 3 watershed shapes into one layer, and either label or make distinct colors

```
```{r FB-topography-map}
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
plot(m1)

#reading in final format data for summer 23
data_23 <- read_csv("./DataForMary/HB_stic.csv")
#reading in final format data for summer 24
data_24 <- read_csv("./summer2024/STICS2024.csv")

#define the rectangular area that will be shown on final map
#determined for each watershed, from figures4poster script
ybounds <- c(4868850,4869650)
xbounds <- c(279350, 280450)
plot(m1, xlim = xbounds, ylim = ybounds)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
plot(crop1)
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./fb_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
fb_crop <- "./fb_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
fb_net <- vect("./carrieZigZag/FB_network.shp")
plot(fb_net)

###pour point to define where the watershed boundary is
#manually type coords from windows computer
fb_pour_coords <- data.frame("easting" = 280400,
                             "northing" = 4869120)
#convert to SpatVector object
fb_pour <- vect(fb_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
fb_pour_filename <- "./fb_dems/fb_pour.shp"
#writeVector(fb_pour, fb_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
######
#breach and fill I guess
fb_crop <- "./fb_dems/1mdem_crop.tif"

fb_breached <- "./fb_dems/1mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = fb_crop,
  output = fb_breached,
  dist = 1,
  fill = TRUE)

fb_filled <- "./fb_dems/1mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = fb_breached,
  output = fb_filled
)
#calculate flow accumulation and direction
fb_flowacc <- "./fb_dems/1mdem_fb_flowacc.tif"
wbt_d8_flow_accumulation(input = fb_filled,
                         output = fb_flowacc)
plot(rast(fb_flowacc))
fb_d8pt <- "./fb_dems/1mdem_fb_d8pt.tif"
wbt_d8_pointer(dem = fb_filled,
               output = fb_d8pt)
plot(rast(fb_d8pt))


#delineate streams
fb_streams <- "./fb_dems/fb_streams.tif"
wbt_extract_streams(flow_accum = fb_flowacc,
                    output = fb_streams,
                    threshold = 8000)
plot(rast(fb_streams))
plot(as.lines((as.polygons(rast(fb_streams)))),
     xlim = c(279800, 279900), ylim = c(4869100,4869200))
#results in weird lines, figure out how to simplify
topo_streams <- as.lines(as.polygons(rast(fb_streams)))

points(lcc)
#snap pour point to streams
fb_pour_snap <- "./fb_dems/fb_pour_snap.shp"
wbt_jenson_snap_pour_points(pour_pts = fb_pour_filename,
                            streams = fb_streams,
                            output = fb_pour_snap,
                            snap_dist = 10)
fb_pour_snap_read <- vect("./fb_dems/fb_pour_snap.shp")
plot(rast(fb_streams), 
     xlim = c(280200, 280410),
     ylim = c(4869300, 4869000))
points(fb_pour_snap_read, pch = 1)

fb_shed <- "./fb_dems/fb_shed.tif"
wbt_watershed(d8_pntr = fb_d8pt,
              pour_pts = fb_pour_snap,
              output = fb_shed)

plot(rast(fb_shed))
#convert raster of watershed area to vector for final mapping
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)
plot(fb_outline)

#get sensor locations from STIC data, format
locs <- data_23 %>% 
  filter(wshed == "FB") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))
plot(lcc)

#assign destination for hillshade calculation
hillshade_out <- "./fb_dems/1mdem_hillshade.tif"
wbt_hillshade(
  dem = fb_crop,
  output = hillshade_out,
)
hill <- rast(hillshade_out)
plot(hill)

fb_slope <- "./fb_dems/1mdem_slope.tif"
wbt_slope(dem = fb_filled,
          output = fb_slope,
          units = "degrees")

fb_twi <- "./fb_dems/1mdem_twi.tif"
wbt_wetness_index(sca = fb_flowacc, #flow accumulation
                  slope = fb_slope,
                  output = fb_twi)


#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.
fb_map <- ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = fb_outline, fill = NA, color = "black", alpha = 0.3) +#, lwd = 3) +
  geom_sf(data = fb_net, colour = "darkslategray3") +#, lwd = 3) +
    geom_sf(data = lcc, colour = "midnightblue") + #, pch = 19, size = 6) +
  geom_sf(data = fb_pour, colour = "black") + #, pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen", limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0.5, "cm"), 
                              pad_y = unit(0.5, "cm")) #, line_width = 3, text_cex = 5, tick_height = 20)

fb_map

#Map of TWI not included
# twi_output <- "./fb_dems/10mdem_twi.tif"
# 
# plot(rast(twi_output), xlim = xbounds, ylim = ybounds)
# ggplot()+
#   geom_spatraster(data = rast(twi_output))+
#   theme_void()+
#   lims(x = xbounds, y = ybounds)+
#   theme(legend.position = "")+
#     geom_sf(data = fb_outline, fill = NA, color = "black", alpha = 0.3) +#, lwd = 3) +
#     geom_sf(data = lcc, colour = "midnightblue", pch = 1) + #, pch = 19, size = 6) +
#    scale_fill_hypso_c(palette = "arctic")+
#   theme(rect = element_rect(fill = "transparent", color = NA))+
#   ggspatial::annotation_scale(location = 'tr', pad_x = unit(0.5, "cm"), 
#                               pad_y = unit(0.5, "cm"))


```
```{r try-NAIP-UNFINISHED}
#read in NAIP imagery for true color composite image
color <- sprc(c("./HB/Original zips/m_4307102_ne_19_030_20230823.jp2",
       "./HB/Original zips/m_4307102_nw_19_030_20230823.jp2",
       "./HB/Original zips/m_4307102_se_19_030_20230823.jp2",
       "./HB/Original zips/m_4307102_sw_19_030_20230823.jp2",
       "./HB/Original zips/m_4307103_ne_19_030_20230823.jp2",
       "./HB/Original zips/m_4307103_nw_19_030_20230823.jp2"))

stackT <- stack("./HB/Original zips/m_4307103_nw_19_030_20230823.jp2")


plot(rast("./HB/Original zips/m_4307103_nw_19_030_20230823.jp2"))
naip_csf_br <- brick(naip_csf_st)
inMemory(naip_csf_br)

plotRGB(naip_csf_br,
        r = 1, g = 2, b = 3,
        main = "RGB image \nColdsprings fire scar")
big_image <- merge(color)
```
## Topo variables
```{r topo-distributions}
#get the areas of each watershed, and the whole valley
#delineate shed and stream using 3, 5, 10m dem, then plot distribution of elevation and TWI

dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 3)
#plot(m10)

#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./HB/1m hydro enforced DEM/dem3m.tif", overwrite = TRUE)
m3_path <- "./HB/1m hydro enforced DEM/dem3m.tif"

fb_elev <- m10 %>% 
  crop(fb_outline) %>% 
  mask(fb_outline)
zz_elev <- m10 %>% 
  crop(zz_outline) %>% 
  mask(zz_outline)
w3_elev <- m10 %>% 
  crop(w3_outline) %>% 
  mask(w3_outline)

plot(fb_elev)
summary(zz_elev$dem1m)
summary(fb_elev$dem1m)
summary(w3_elev$dem1m)

hist(zz_elev$dem1m)
hist(fb_elev$dem1m)
hist(w3_elev$dem1m)

zz_dist <- as_tibble(data.frame("elev" = zz_elev$dem1m, "site" = "zz"))
fb_dist <- as_tibble(data.frame("elev" = fb_elev$dem1m, "site" = "fb"))
w3_dist <- as_tibble(data.frame("elev" = w3_elev$dem1m, "site" = "w3"))

all_dist <- rbind(zz_dist, fb_dist, w3_dist)

ggplot(all_dist)+
  geom_boxplot(aes(x=site, y=dem1m))

#now calculate and display the distribution of TWI for each watershed as boxplot
#calculate TWI for the whole valley, then just extract like above
#fill and breach 3m dem
breach_output <- "./HB/1m hydro enforced DEM/dem3m_breach.tif"
wbt_breach_depressions_least_cost(
  dem = m3_path,
  output = breach_output,
  dist = 3,
  fill = TRUE)

fill_output <- "./HB/1m hydro enforced DEM/dem3m_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
slope_output <- "./HB/1m hydro enforced DEM/dem3m_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
twi_output <- "./HB/1m hydro enforced DEM/dem3m_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

#crop and mask TWI by each of my watersheds
twi_valley <- rast(twi_output)
fb_twi <- twi_valley %>% 
  crop(fb_outline) %>% 
  mask(fb_outline)
zz_twi <- twi_valley %>% 
  crop(zz_outline) %>% 
  mask(zz_outline)
w3_twi <- twi_valley %>% 
  crop(w3_outline) %>% 
  mask(w3_outline)

#convert raster values to tibble
zz_tib <- as_tibble(data.frame("twi" = zz_twi$dem3m_twi, "site" = "zz"))
fb_tib <- as_tibble(data.frame("twi" = fb_twi$dem3m_twi, "site" = "fb"))
w3_tib <- as_tibble(data.frame("twi" = w3_twi$dem3m_twi, "site" = "w3"))

all_twi <- rbind(zz_tib, fb_tib, w3_tib)

ggplot(all_twi)+
  geom_boxplot(aes(x=site, y=dem3m_twi))

ggplot(all_twi)+
  geom_histogram(aes(y=after_stat(density),
                     fill=site, 
                     x=(dem3m_twi)), 
                 alpha=0.6, position="identity")
ggplot(all_twi)+
  geom_histogram(aes(y=after_stat(density),
                     fill=site, 
                     x=log(dem3m_twi)))

#create plots of distribution
```

# Figure 3: Distribution of flow permanence and duration of flow
## Flow Permanence
```{r prepare-discharge}
#from TestingFrameworks script

#read in discharge from W3-- input to Carrie's model, discharge in L/s
#q <- read_csv("https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-hbr.1.17&entityid=efc477b3ef1bb3dd8b9355c9115cd849")
#write.csv(q, "HB_5minQ.csv")
q <- read_csv("HB_5minQ.csv")

#input discharge needs to be in mm/day?
#reference to understand difference between daily mean and instantaneous streamflow:
#https://hydrofunctions.readthedocs.io/en/master/notebooks/DailyMean_vs_Instant.html

#creating minute column, used to filter out higher temporal resolution measurements for plotting
data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

#find the range of dates that I need discharge for
start <- min(data_23$datetime)
stop <- max(data_23$datetime)

#filtering discharge down to the range of dates
q_23 <- q %>% 
  filter(DATETIME > start & DATETIME < stop) %>% 
  #convert to mm/day.
  #converting instantaneous streamflow to mm/day by taking measurement, and scaling   it up as if that was the discharge for the whole day. It is not, it is just at that   moment, but should fix any units/order of magnitude issues
  mutate("Q_mm_day" = Discharge_ls * 0.001 * 86400 / 420000 * 1000) 
q_23$mins <- minute(q_23$DATETIME)

#removing times that are not coincident with STIC observations
q_23_f <- filter(q_23, mins %in% c(0, 30))

ggplot(q_23_f, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#also read in provisional 2024 data
q_24 <- read_csv("w3_discharge_24.csv")

#find the range of dates that I need discharge for
start24 <- ymd_hms("2024-05-15 00:00:00 UTC")
stop24 <- max(data_24$datetime)

#filtering discharge down to the range of dates
q_24 <- q_24 %>% 
  mutate(datetime = mdy_hm(datetime)) %>% 
  filter(datetime > start24 & datetime < stop24)  
q_24$mins <- minute(q_24$datetime)

#removing times that are not coincident with STIC observations
q_24_f <- filter(q_24, mins %in% c(0, 30))

ggplot(q_24_f, aes(x  = datetime, y = Q_mmperday))+
  geom_line()+
  labs(title = "Discharge from W3, May to July 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()
```
```{r flow-permanence-W3}
#just summer 2023
data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

pks_23 <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>% 
  #left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()
#just summer 2024
pks_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number) %>% 
  #left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()

both <- inner_join(pks_23, pks_24, by = "ID")
ggplot()+
  geom_point(data = both,aes(x = pk.x, y = pk.y))+
  labs(title = "Change in pk from '23 to '24, W3",
       x = "2023",
       y = "2024")+
  geom_abline(slope=1, intercept=0)+
  theme_classic()

#both summers
#just rbind summers 23 and 24
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_w3 <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "W3")
```
```{r flow-permanence-FB}
#both summers
#just rbind summers 23 and 24
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_fb <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "FB")
```
```{r flow-permanence-ZZ}
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_zz <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "ZZ")
```
```{r combine-boxplot}
rbind(pks_w3, pks_fb, pks_zz) %>% 
  ggplot(aes(x = wshed, y = pk))+
  geom_boxplot()+
  geom_jitter(width = 0.1, alpha = 0.5)+
  theme_classic()+
  labs(title = "Distributions of Flow Permanence",
       x = "Watershed",
       y = "Flow Permanence")
```

```{r}
#what is the fewest number of times you would need to observe a location to determine the actual local persistency?
#what about if you visited over a range of wetness conditions? Like every 10th quartile
```

## Average Duration of Flow
```{r dof-w3}
#successfully calculating the average duration of flow for an event
bind23 <- data_23 %>% 
  select(datetime, ID, wshed, binary, mins)
bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)

dof_w3 <- rbind(bind23, bind24) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
  group_by(group, ID) %>%
  summarise(state = first(binary), 
            timeperiod = dplyr::last(datetime) - dplyr::first(datetime)) %>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
  group_by(ID) %>% 
  filter(state == 1) %>% 
  summarise(avg_days_flowing = mean(timeperiod)) %>% 
  mutate(wshed = "W3")

```
```{r dof-fb}
dof_fb <- rbind(bind23, bind24) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
  group_by(group, ID) %>%
  summarise(state = first(binary), 
            timeperiod = dplyr::last(datetime) - dplyr::first(datetime)) %>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
  group_by(ID) %>% 
  filter(state == 1) %>% 
  summarise(avg_days_flowing = mean(timeperiod)) %>% 
  mutate(wshed = "FB") %>% 
  #remove outlier left out over winter
  filter(ID != 32)
```
```{r dof-zz}
dof_zz <- rbind(bind23, bind24) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
  group_by(group, ID) %>%
  summarise(state = first(binary), 
            timeperiod = dplyr::last(datetime) - dplyr::first(datetime)) %>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
  group_by(ID) %>% 
  filter(state == 1) %>% 
  summarise(avg_days_flowing = mean(timeperiod)) %>% 
  mutate(wshed = "ZZ")
```
```{r combine-boxplot}
rbind(dof_w3, dof_fb, dof_zz) %>% 
  ggplot(aes(x = wshed, y = (avg_days_flowing)))+
  geom_boxplot()+
  geom_jitter(width = 0.1, alpha = 0.5)+
  theme_classic()+
  labs(title = "Distributions of Average Duration of Flow",
       x = "Watershed",
       y = "Duration of Flow (days)")

```

#Figure 4: How often does the network wet up, dry down, etc?
##Figure out how to test this behavior
```{r}
test <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(ID == 1)%>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) #%>% 
  filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"
```
First thing to try that I have tried before: determine all unique combinations of wet and dry sensors
```{r}
#find all unique combinations of wet and dry sensors
hush2 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  pivot_wider(names_from = ID, values_from = binary)

hush3 <- hush2[1:5,-1]
hush3
hush3[-1, ]

hush3[-5,] - hush3[-1, ]

# xy.list <- as.list(as.data.frame(t(hush2[,-1])))
# length(xy.list)
# length(unique(data$datetime))
# #unique combinations of flow
# length(unique(xy.list))

#perhaps subset it so that I am working with one pair of consecutive sensors at a time
hush3[-5,15:17] - hush3[-1, 15:17]
hush3[,15:17]

#how often does 17 activate before 16?
#filter to every time that 17 transitions to flowing- value of -1
l <- 20 #length of time
hush3 <- hush2[1:l,-1] #get the number of time steps equal to l, get rid of time id col

hush3[-l,16:17] - hush3[-1, 16:17] #find the change with lagged cols, output does not have last delta

#find every time that 17 is -1
hush4 <- hush3[-l,16:17] - hush3[-1, 16:17]
colnames(hush4) <- c("up", "down")

hush4

#how often does the upstream sensor go to -1 after the downstream sensor?
require(data.table)
data.table:::duplist(hush4[, 3:5]) 

hush4 %>% 
mutate(group = data.table::rleid(down)) %>%
  group_by(group) %>%
  summarise(state = first(down),
            rows = length(group)) #%>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days"))

#how can I calculate seqeunce of activation?
data <- data.frame(
  Var1 = c(1, 0, 1, 1, 0),
  Var2 = c(0, 1, 1, 0, 1),
  Var3 = c(1, 1, 1, 1, 0)
)

target_sequence <- c(1, 0, 1)

matches <- apply(data, 1, function(row) all(row == target_sequence))
count <- sum(matches)
proportion <- count / nrow(data)

cat("Number of matches:", count, "\n")
cat("Proportion of matches:", proportion, "\n")



#one idea- fit a logistic regression of every sensor to every other sensor. If a sensor is always flowing when another is, it would be a good predictor
mod <- hush2[,16:17]
colnames(mod) <- c("up", "down")

mod2 <- hush2
colnames(mod2) <- as.character(paste0("r_",colnames(hush2)))

model <- glm(r_1 ~.,family=binomial(link='logit'),data=mod2)
summary(model)
```
```{r}
#how often does x activate after y?

data <- data.frame(
  Var1 = c(1, 0, 1, 1, 0, 1, 1),
  Var2 = c(0, 1, 1, 0, 1, 0, 0),
  Var3 = c(1, 1, 1, 1, 0, 1, 1)
)
target_sequence <- matrix(
  c(1, 0, 1,
    0, 1, 1),
  nrow = 2, byrow = TRUE
)

# Flatten the target sequence for comparison
target_vector <- as.vector(t(target_sequence))

# Use rollapply to create sliding windows
windows <- rollapply(data, width = nrow(target_sequence), by.column = FALSE, FUN = function(x) as.vector(t(x)))

# Check for matches
matches <- apply(windows, 1, function(window) all(window == target_vector))

# Get indices of matches
match_indices <- which(matches)

cat("Matching sequence found at indices:", match_indices, "\n")


#now test on my data
mod3 <- select(mod2,r_15, r_16, r_17)
target_sequence <- matrix(
  c(
    0, 0, 1,
    0, 1, 1,
    1, 1, 1),
  nrow = 3, byrow = TRUE
)
target_vector <- as.vector(t(target_sequence))

# Use rollapply to create sliding windows
windows <- rollapply(mod3, width = nrow(target_sequence), by.column = FALSE, FUN = function(x) as.vector(t(x)))

# Check for matches
matches <- apply(windows, 1, function(window) all(window == target_vector))

# Get indices of matches
match_indices <- which(matches)
length(match_indices)

###another attempt

# Example data
mod3 <- select(mod2,r_15, r_16, r_17)


# Define window size
window_size <- 6

# Create sliding windows
windows <- rollapply(
  mod3,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Get most common sequence
most_common_sequence <- names(sorted_counts)[1]
cat("Most common sequence:", most_common_sequence, "\n")
cat("Frequency:", sorted_counts[1], "\n")

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
print(sequence_df)

```

```{r}
#find the most common sequence of activation during the rising limb, or times when slope of discharge is positive
mod3 <- drop_na(mod2[,-1])
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))
#make it so that there cannot be a sequence without change

# Define window size
window_size <- 4

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
View(sequence_df)
```

```{r}
#convert this result into a plot
#number of panes dictated by window size
window_size

number_in_list <- 4
#left join the sequence to the locations of the sensors and plot
head(sequence_df)

status <- data.frame("ID" = as.numeric(rep(substr(colnames(no_dupes), 3,4),window_size)),
                     "status" = unlist(strsplit(sequence_df$Sequence[number_in_list], split = "")),
                     "timestep" = c(rep(1, 31), rep(2, 31), rep(3, 31), rep(4, 31)))

#get sensor locations from STIC data, format
locs <- data_23 %>% 
  filter(wshed == "W3") %>% 
  select(ID, lat, long) %>% 
  unique() %>% 
  left_join(status, by = "ID")
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))

ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = w3_outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = w3_net, colour = "darkslategray3") +
    geom_sf(data = lcc, aes(colour = status), pch = 19) +
  facet_wrap(~timestep)+
   scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))
```

```{r w3-map}
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
plot(m1)

#get sensor locations from STIC data, format
locs <- data_23 %>% 
  filter(wshed == "W3") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))
plot(lcc)
#define the rectangular area that will be shown on final map
ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
plot(m1, xlim = xbounds, ylim = ybounds)
points(lcc)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
plot(crop1)
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
writeRaster(crop1, "./w3_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
w3_crop <- "./w3_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
w3_net <- vect("./carrieZigZag/w3_network.shp")
plot(w3_net)

###pour point to define where the watershed boundary is
#manually type coords from windows computer
 
 
w3_pour_coords <- data.frame("easting" = 281537.46,
                             "northing" = 4870424.50)
#convert to SpatVector object
w3_pour <- vect(w3_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
w3_pour_filename <- "./w3_dems/w3_pour.shp"
writeVector(w3_pour, w3_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
w3_crop <- "./w3_dems/1mdem_crop.tif"

w3_breached <- "./w3_dems/1mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = w3_crop,
  output = w3_breached,
  dist = 1,
  fill = TRUE)

w3_filled <- "./w3_dems/1mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = w3_breached,
  output = w3_filled
)
#calculate flow accumulation and direction
w3_flowacc <- "./w3_dems/1mdem_w3_flowacc.tif"
wbt_d8_flow_accumulation(input = w3_filled,
                         output = w3_flowacc)
plot(rast(w3_flowacc))
w3_d8pt <- "./w3_dems/1mdem_w3_d8pt.tif"
wbt_d8_pointer(dem = w3_filled,
               output = w3_d8pt)
plot(rast(w3_d8pt))


#delineate streams
w3_streams <- "./w3_dems/w3_streams.tif"
wbt_extract_streams(flow_accum = w3_flowacc,
                    output = w3_streams,
                    threshold = 8000)
plot(rast(w3_streams))
points(lcc)
#snap pour point to streams
w3_pour_snap <- "./w3_dems/w3_pour_snap.shp"
wbt_jenson_snap_pour_points(pour_pts = w3_pour_filename,
                            streams = w3_streams,
                            output = w3_pour_snap,
                            snap_dist = 10)
w3_pour_snap_read <- vect("./w3_dems/w3_pour_snap.shp")
plot(rast(w3_streams), 
     xlim = c(281400, 282000),
     ylim = c(4870400, 4870800))
points(w3_pour_snap_read, pch = 1)

w3_shed <- "./w3_dems/w3_shed.tif"
wbt_watershed(d8_pntr = w3_d8pt,
              pour_pts = w3_pour_snap,
              output = w3_shed)

plot(rast(w3_shed))
#convert raster of watershed area to vector for final mapping
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)
plot(w3_outline)



#assign destination for hillshade calculation
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
wbt_hillshade(
  dem = w3_crop,
  output = hillshade_out,
)
hill <- rast(hillshade_out)
plot(hill)

#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.





```

Whenever there is a change in state, does it follow the wetting up, drying down, or flow permanence hypothesis?

```{r}
#testing pairs of sensors
#filter to two upstream/downstream sensors
mod3 <- drop_na(mod2[,-1]) %>% select("r_22", "r_23")
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))
#make it so that there cannot be a sequence without change

# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001", "0011", "0111", "1110", "1100")

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations

```
Use above chunk to write a function to take an upstream and downstream sensor, and determine what proportion of the times it changes state follow the systematic behavior I am looking for.

```{r define-function}
#testing pairs of sensors
#filter to two upstream/downstream sensors
calc_support <- function(up, down, input){
#up <- "r_10"
#down <- "r_6"
incase <- data.frame(up = c(0,0), down = c(0,0))
colnames(incase) <- c(up, down)
mod3 <- input %>% select(up,down, -datetime)
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all)) 
#make it so that there cannot be a sequence without change

#error when both locations flowed the whole time; 
if(length(no_dupes[,1]) == 1) no_dupes <- rbind(no_dupes, no_dupes, incase)

# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
#View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001", "0011", "0111", "1110", "1100", "1101", "0100")
#subset from the 16 possibilities, except it is fewer because
#0000, 1111, 0101, 1010 are removed...

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down, points, total)
#error handling- in situation where both points flowed 100% of the time
if(length(no_dupes[,1]) == 1) output$points <- NA
if(length(no_dupes[,1]) == 1) output$total <- NA

return(output)
}

calc_support("r_10", "r_6", input)
```

```{r define-function-no-pandering}
#testing pairs of sensors
#filter to two upstream/downstream sensors
calc_support <- function(up, down, input){
#up <- "r_10"
#down <- "r_6"

mod3 <- input %>% select(up,down, -datetime)
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))
#make it so that there cannot be a sequence without change

#error when both locations flowed the whole time; 

# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
#View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")
#ones that I included before: c("0011", "1110", "1100")
#subset from the 16 possibilities, except it is fewer because
#0000, 1111, 0101, 1010 are removed...

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down, points, total)

return(output)
}

calc_support("r_10", "r_6", input)
```

```{r function-final-version}
calc_support <- function(up, down, input){
#up <- routes$up[i] #input
#down <- routes$down[i]
incase <- data.frame(up = c(0,0), down = c(0,0))
colnames(incase) <- c(up, down)
mod3 <- input %>% select(up,down, -datetime)
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))
#all flowing all the time?
check <- nrow(no_dupes)
#make it so that there cannot be a sequence without change

#error when both locations flowed the whole time; 
if(check == 1) no_dupes <- rbind(no_dupes, no_dupes, incase)

# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
#View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")
#subset from the 16 possibilities, except it is fewer because
#0000, 1111, 0101, 1010 are removed...

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down, points, total)
#error handling- in situation where both points flowed 100% of the time
if(check == 1) output$points <- NA
if(check == 1) output$total <- NA

return(output)
}

calc_support("r_28", "r_6", input)
```

#finalized versions of analysis functions
Finished on 2/21/25, fixed calc_support and the nest of for loops so that they work smoothly with any set of routes, inputs, and timesteps.

Structure of functions:
- fantastic_four (routes, shed("W3", "FB", "ZZ"))
|- for loop, 4 different test timescales
  |- calculate hierarchy (routes, input, timestep (as duration))
    |- for loop, number of pairs dictated by routes
      |- iterate groups (up, down, input, timestep (as duration))
        |- for loop, number of groups of uninterrupted dates, dictated by timestep
          |- calc_support (up, down, input) 
        
```{r refined-functions}
#chunk to trouble shoot non-functioning instances to figure out why they are not working
#instance not working:
#run_scenario(routes_w3_pk, "pk", "W3", "daily")

routes <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

input <- rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
        filter(wshed == "W3", mins %in% c(0, 30)) %>%
      #filter(wshed == "W3", hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary) #%>% 
  #filter(datetime < stop24 & datetime > start24)


calc_support <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  output$total <- NA
  output$points <- NA
  return(output)
} 
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")

output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")


sub <- filter(sequence_df, Sequence %in% supports)
output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(output)}
}

#test function
calc_support("r_23", "r_6", input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

iterate_groups("r_23", "r_6", input, hours(1))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- iterate_groups(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down) %>%
    summarise(total = sum(total),
              points = sum(points)) %>% 
    mutate(prop = points/total)
  return(final_output)
}

calculate_hierarchy(routes, input, minutes(30))
#calculate_hierarchy(routes, input, days(1))


#make a function to loop through the four possible timesteps, and combine the output just for ease of applying this many different variations
fantastic_four <- function(routes, shed){
  theFour <- c("30mins", "hourly", "4hr", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(bind23, bind24) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "hourly"){
    input <- rbind(bind23, bind24) %>%
      filter(wshed == shed, mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(1)
  } 
  else if(timescale == "4hr"){
    input <- rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(0,4,8,12,16,20,24), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(4)
  } 
  else if(timescale == "daily"){
    input <- rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == "W3", hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}


#final conclusion- do need to break it up into groups! gets rid of 1-4 erroneous pairs, which will matter more at lower temporal resolutions
#for each scenario create the routes, inputs, and specify the timestep
```
Iterating through each pair of sensors, using the upstream sensor as the id, and the downstream sensor as the secondary or dependent. Seeing how often pairs of sensors wet up or dry down relative to each other.
#Running analysis using refined functions
```{r relative-position}
#set routes for all 3 watersheds
routes_w3 <- read_csv("w3_flowrouting.csv") %>% 
  rename("up" = drains_from, "down" = sensor) %>% 
  select(up, down) %>% 
  filter(down != 100, up != 0)
routes_fb <- read_csv("fb_flowrouting.csv") %>% drop_na()
routes_zz <- read_csv("zz_flowrouting.csv") %>% drop_na()

#run calc_support for all sheds and timesteps for relative position
all_position <- rbind(fantastic_four(routes_w3, "W3"),
                      fantastic_four(routes_fb, "FB"),
                      fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Relative Position")

write_csv(all_position, "./hierarchy_analysis_results/relativePosition.csv")
```
```{r flow-permanence}
#make list that make pairs of sites based on local persistency
routes_w3 <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- pks_fb %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- pks_zz %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

all_pk <- rbind(fantastic_four(routes_w3, "W3"),
                fantastic_four(routes_fb, "FB"),
                fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Flow Permanence")

write_csv(all_pk, "./hierarchy_analysis_results/flowPermanence.csv")
```
```{r duration-of-flow}
#make list that make pairs of sites based on local persistency
routes_w3 <- dof_w3 %>% 
  arrange(desc(avg_days_flowing)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- dof_fb %>% 
  arrange(desc(avg_days_flowing)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- dof_zz %>% 
  arrange(desc(avg_days_flowing)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

all_dof <- rbind(fantastic_four(routes_w3, "W3"),
                 fantastic_four(routes_fb, "FB"),
                 fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Duration of Flow")

write_csv(all_pk, "./hierarchy_analysis_results/durationOfFlow.csv")
```

##looping by drainage area UNFINISHED
Lost in a file hell
```{r create-3m-network}
streams <- "./HB/1m hydro enforced DEM/dem3m_streams.tif"
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

wbt_extract_streams(flow_accum = flowacc_output,
                    output = streams,
                    threshold = 500)
plot(rast(streams))
#w3 zoom
ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
plot(rast(streams), xlim = xbounds, ylim = ybounds)
#points(lcc)
#zz () zoom
ybounds <- c(4866400,4867500)
xbounds <- c(277200, 277650)

#use these networks, snap points to them before extracting values
plot(vect("./HB/hbstream/ZZ_subcatchment_flowlines.shp"))
plot(rast(streams), add = TRUE, alpha = 0.5)

plot(vect("./HB/hbstream/FB_subcatchment_flowlines.shp"))
plot(rast(streams), add = TRUE, alpha = 0.5)

plot(vect("./HB/hbstream/hb42_master_startend.shp"))
plot(rast(streams), add = TRUE, alpha = 0.5)


```
```{r extract-drainage-area-W3}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
#flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"
locs <- data_24 %>% 
  filter(wshed == "W3") %>% 
  select(number, lat, long) %>% 
  rename("ID" = number) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(rast(flowacc_output)))
plot(lcc)
#snap points to lines using wbt
#use 5 m lines? or convert 1 m dem based shapefiles to raster
#for some reason 5m w3 raster is not working, but I can use the other ones I do believe
# found already snapped W3 STIC locations
# To make the raster work: read in using raster package, then save as a new file
test <- raster::raster("HB/hbstream/w3_5m.tif")
writeRaster(test, "actualW3_shed.tif", overwrite = TRUE)
#path for point locations, before snapping
w3_lcc_path <- "./mapsForStoryboardFiles/w3_stics.shp"
writeVector(lcc, w3_lcc_path, overwrite = TRUE)
#path for point locations after snapping
w3_lcc_path_snap <- "./mapsForStoryboardFiles/w3_stics_snapped.shp"
#snap the points with whitebox
wbt_jenson_snap_pour_points(pour_pts = w3_lcc_path,
                            streams = "actualW3_shed.tif",
                            output = w3_lcc_path_snap,
                            snap_dist = 10)
#testing and s eeing that 
plot(rast("actualW3_shed.tif"), col = 'purple')

plot(test)
plot(vect(w3_lcc_path_snap), add = TRUE)
plot(vect(w3_lcc_path), add = TRUE, col = "red")
plot(vect("w3_stic_locs_snap.shp"))
w3_stics <- (vect("w3_stic_locs_snap.shp"))

#calculate for W3

plot(rast("HB/hbstream/zz_5m.tif"))
W3_uaa_ex <- extract(rast(flowacc_output), w3_stics) #should be in units of 3 m2, so 200 is 600 m2 I 

#make list that makes pairs of sites based on topography
```
```{extract-drainage-area-FB}
#calculate for FB
locs <- data_24 %>% 
  filter(wshed == "FB") %>% 
  select(number, lat, long) %>% 
  rename("ID" = number) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
fb_lcc <- terra::project(locs_shape, crs(rast(flowacc_output)))

plot(vect("./HB/hbstream/FB_subcatchment_flowlines.shp"))
old_stream <- vect("./HB/hbstream/FB_subcatchment_flowlines.shp")
old_stream_rast <- terra::rasterize(old_stream, rast(flowacc_output))
plot(old_stream_rast)
writeRaster(old_stream_rast, "actualFB_shed.tif", overwrite = TRUE)


#snap points to streamlines, use 5m FB streams
#read in raster and re-save
test <- raster::raster("HB/hbstream/fb_5m.tif")
writeRaster(test, "actualFB_shed.tif", overwrite = TRUE)
#path to save fb stic locations
fb_lcc_path <- "./mapsForStoryboardFiles/fb_stics.shp"
writeVector(fb_lcc, fb_lcc_path, overwrite = TRUE)
#path for snapped stic locations
fb_lcc_path_snap <- "./mapsForStoryboardFiles/fb_stics_snapped.shp"
fb_streams <- "actualFB_shed.tif"
wbt_jenson_snap_pour_points(pour_pts = fb_lcc_path,
                            streams = fb_streams,
                            output = fb_lcc_path_snap,
                            snap_dist = 20)
#before extracting, I should snap points to stream network based on this resolution perhaps... ugh

#something about this doesn't seem right...
FB_uaa_ex <- extract(rast(flowacc_output), vect(fb_lcc_path_snap))
ybounds <- c(4868850,4869650)
xbounds <- c(279350, 280450)
plot(rast(fb_streams), xlim = xbounds, ylim = ybounds)

plot(vect(fb_lcc_path_snap), add = TRUE)

```
```{r extracting-drainage-ZZ}
#calculate for FB
locs <- data_24 %>% 
  filter(wshed == "ZZ") %>% 
  select(number, lat, long) %>% 
  rename("ID" = number) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
zz_lcc <- terra::project(locs_shape, crs(rast(flowacc_output)))

#snap points to streamlines, use 5m FB streams
#read in raster and re-save
test <- raster::raster("HB/hbstream/zz_5m.tif")
writeRaster(test, "actualZZ_shed.tif", overwrite = TRUE)
#path to save fb stic locations
zz_lcc_path <- "./mapsForStoryboardFiles/zz_stics.shp"
writeVector(zz_lcc, zz_lcc_path, overwrite = TRUE)
#path for snapped stic locations
zz_lcc_path_snap <- "./mapsForStoryboardFiles/zz_stics_snapped.shp"
zz_streams <- "actualZZ_shed.tif"
wbt_jenson_snap_pour_points(pour_pts = zz_lcc_path,
                            streams = zz_streams,
                            output = zz_lcc_path_snap,
                            snap_dist = 20)
#before extracting, I should snap points to stream network based on this resolution perhaps... ugh

ZZ_uaa_ex <- extract(rast(flowacc_output), vect(zz_lcc_path_snap))
plot(rast(zz_streams))

plot(vect(zz_lcc_path_snap), add = TRUE)
```

```{r drainage-area}
#make list that make pairs of sites based on local persistency
routes_w3 <- W3_uaa_ex %>% 
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- FB_uaa_ex %>%
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
  drop_na() %>% 
  select(up, down)

routes_zz <- ZZ_uaa_ex %>%
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>%
  drop_na() %>% 
  select(up, down)

all_uaa <- rbind(fantastic_four(routes_w3, "W3"),
                 fantastic_four(routes_fb, "FB"),
                 fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Drainage Area")

write_csv(all_pk, "./hierarchy_analysis_results/drainageArea.csv")
```


##Compile all of my results so far 2
Consolidate the gains!!
```{r consolidating-progress}
#prop stands for proportion or percentage of transitions that support the hypothesis
w3_result_dof <- data.frame("prop" = w3_pairs_dof$prop, "type" = "dof", "shed" = "W3")
w3_result_topo <- data.frame("prop" = w3_pairs_topo$prop, "type" = "topo", "shed" = "W3")

fb_result_dof <- data.frame("prop" = fb_pairs_dof$prop, "type" = "dof", "shed" = "FB")
fb_result_topo <- data.frame("prop" = fb_pairs_topo$prop, "type" = "topo", "shed" = "FB")

zz_result_dof <- data.frame("prop" = zz_pairs_dof$prop, "type" = "dof", "shed" = "ZZ")
zz_result_topo <- data.frame("prop" = zz_pairs_topo$prop, "type" = "topo", "shed" = "ZZ")


sults_so_far <- rbind(w3_result_position, w3_result_pk, w3_result_dof, w3_result_topo, 
                      fb_result_position, fb_result_pk, fb_result_dof, fb_result_topo,
                      zz_result_position, zz_result_pk, zz_result_dof, zz_result_topo)

sults_so_far %>% 
  ggplot(aes(x = shed, y = prop, color = type)) +
  geom_boxplot()+
  theme_classic()+
    geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Watershed",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  facet_wrap(~type)

sults_so_far %>% 
  ggplot(aes(x = shed, y = prop, color = type)) +
  geom_boxplot()+
  theme_classic()+
    geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Watershed",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")

#instead of boxplots, show the ecdf and one-sided violin plot
sults_so_far %>% 
  ggplot(aes(x = shed, y = prop, fill = type)) +
  geom_violin()+
  theme_classic()+
    #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Watershed",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_fill_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")

#one sided violin plot
p_load(devtools)
devtools::install_github("psyteachr/introdataviz")
library(introdataviz)

sults_so_far %>% 
  ggplot(aes(x = shed, y = prop, color = type)) +
  geom_split_violin()+
  theme_classic()+
    #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Watershed",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  facet_wrap(~type)

#plot ecdf instead of boxplots or violin plots
plot(ecdf(w3_hourly$prop))
plot(ecdf(alldat$prop))
ggplot()+
  stat_ecdf(data = w3_pairs, aes(prop))+
  stat_ecdf(data = w3_new, aes(prop), color = "blue")

sults_so_far %>% 
  ggplot(aes(prop, color = type)) +
  stat_ecdf(geom = "line")+
  theme_classic()+
    #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
    geom_vline(xintercept = 0.5, lty = 2)+

  labs(x = "Prop",
       y = "Percentage of values less than or equal",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  facet_wrap(~shed)+
  lims(x = c(0.5,1),
       y = c(0.5, 1))

#maybe it would look better to add the randomly generated hierarchies for a comparison

```
##find times when the network goes from very dry to very wet
```{r}
#plot the number of sensors activated through time

```


Try for a sequence of 3 times- also shows that this systematic wetting is very common
```{r window3}
calc_support <- function(up, down){
mod3 <- drop_na(mod2[,-1]) %>% select(all_of(up),all_of(down))
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))
#make it so that there cannot be a sequence without change

# Define window size
window_size <- 3

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
#View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:

supports <- c("000111", "110100", "011101", "110111", "000100")
#subset from the 16 possibilities, except it is fewer because
#0000, 1111, 0101, 1010 are removed...

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down, points, total)
return(output)
}

calc_support("r_22", "r_23")
```

#final plot
```{r}
#boxplots or distributions of the percentage of state changes that follow some system, different colors/boxes for each scheme or system, and x axis is the number of sensors.

#facet for each watershed
```

#Figure 6: relationship between flow permanence and topography, dof and topography
Conclusion- No clear relationship, this might not make the final cut.
```{r flow-permanence}
#drainage area
W3_topo_pk <- W3_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(pks_w3, by = "ID")
FB_topo_pk <- FB_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(pks_fb, by = "ID")
ZZ_topo_pk <- ZZ_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(pks_zz, by = "ID")

rbind(W3_topo_pk, FB_topo_pk, ZZ_topo_pk) %>% 
  ggplot(aes(x = (uaa), y = pk, color = wshed))+
  geom_point()+
    facet_wrap(~wshed, scales = "free")+
  theme_classic()+
  labs(title = "Flow permanence versus drainage area",
       x = "Drainage area",
       y = "Flow Permanence")
```
```{r dof}
rbind(dof_w3, dof_fb, dof_zz) %>% 
  ggplot(aes(x = wshed, y = (avg_days_flowing)))+
  geom_boxplot()+
  geom_jitter(width = 0.1, alpha = 0.5)+
  theme_classic()+
  labs(title = "Distributions of Average Duration of Flow",
       x = "Watershed",
       y = "Duration of Flow (days)")

#drainage area
W3_topo_dof <- W3_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(dof_w3, by = "ID")
FB_topo_dof <- FB_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(dof_fb, by = "ID")
ZZ_topo_dof <- ZZ_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(dof_zz, by = "ID")

rbind(W3_topo_dof, FB_topo_dof, ZZ_topo_dof) %>% 
  ggplot(aes(x = (uaa), y = avg_days_flowing, color = wshed))+
  facet_wrap(~wshed, scales = "free")+
  geom_point()+
  theme_classic()+
  labs(title = "dof versus drainage area",
       x = "Drainage area",
       y = "Duration of Flow (days)")
```

###ALSO TEST DEPTH TO BEDROCK

#crazy idea
Test every possible combination of pairs of sensors, see which one works the most hierarchically
```{r figuring-it-out}
#maybe best way to do this is make random lists of up and downstream sensors like 1000 times, then run through hierarchical testing algorithm
combos <- W3_uaa_ex %>% 
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) %>% 
  mutate(up = paste0("r_",up),
         down = paste0("r_",down)) %>% 
  select(up)

combos <- unique(combos$up)

install.packages("gtools")  # Install the package if not already installed
library(gtools)
set.seed(123)  # For reproducibility
iterations <- 10
sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
sample <- as_tibble(sample_perms, .name_repair = "minimal")
#colnames(sample) <- paste0("I_",seq(1, iterations, 1))
colnames(sample) <- seq(1, iterations, 1)


#write for loop to iterate through possibilities and determine which has the highest hierarchy score
# code from loop chunks
input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)
#now make loop to loop through all pairs of sensors
t = 2
routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- mini %>% mutate(down = lag(up))%>% 
    drop_na()
for(t in 1:iterations){
  routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- routes %>% mutate(down = lag(up))%>% 
    drop_na()
    
  output <- find_score(routes, input, t)
  if(t == 1) alldat2 <- output
    if(t > 1) alldat2 <- rbind(alldat2, output)
}

find_score <- function(routes, input, t){
  for(x in 1:length(routes$up)){
  
    out <- calc_support(routes$up[x], routes$down[x], input)
    if(x == 1) alldat <- out
    if(x > 1) alldat <- rbind(alldat, out)

  }
  alldat$prop <- alldat$points/alldat$total
  alldat$iteration <- t

  return(alldat)
}

alldat2 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point()+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed))

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))

```
Finding 1000 random combinations
```{r random-W3}
set.seed(124)  # For reproducibility
iterations <- 1000

combos <- W3_uaa_ex %>% 
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) %>% 
  mutate(up = paste0("r_",up),
         down = paste0("r_",down)) %>% 
  select(up)

combos <- unique(combos$up)
sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
sample <- as_tibble(sample_perms, .name_repair = "minimal")
#colnames(sample) <- paste0("I_",seq(1, iterations, 1))
colnames(sample) <- seq(1, iterations, 1)


find_score <- function(routes, input, t){
  for(x in 1:length(routes$up)){
  
    out <- calc_support(routes$up[x], routes$down[x], input)
    if(x == 1) alldat <- out
    if(x > 1) alldat <- rbind(alldat, out)

  }
  alldat$prop <- alldat$points/alldat$total
  alldat$iteration <- t

  return(alldat)
}

for(t in 1:iterations){
  routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- routes %>% mutate(down = lag(up))%>% 
    drop_na()
    
  output <- find_score(routes, input, t)
  if(t == 1) alldat3 <- output
    if(t > 1) alldat3 <- rbind(alldat3, output)
}

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))
random_w3 <- alldat3
#alldat is seed 123
random_w3 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed), size = 3)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")
random_w3_2 <- alldat3

#alldat3 is seed 124
alldat3 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed))+
  theme_classic()

```
```{r random-FB}
set.seed(123)  # For reproducibility
iterations <- 1000

combos <- FB_uaa_ex %>% 
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) %>% 
  mutate(up = paste0("r_",up),
         down = paste0("r_",down)) %>% 
  select(up)

combos <- unique(combos$up)
sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
sample <- as_tibble(sample_perms, .name_repair = "minimal")
#colnames(sample) <- paste0("I_",seq(1, iterations, 1))
colnames(sample) <- seq(1, iterations, 1)


find_score <- function(routes, input, t){
  for(x in 1:length(routes$up)){
  
    out <- calc_support(routes$up[x], routes$down[x], input)
    if(x == 1) alldat <- out
    if(x > 1) alldat <- rbind(alldat, out)

  }
  alldat$prop <- alldat$points/alldat$total
  alldat$iteration <- t

  return(alldat)
}

for(t in 1:iterations){
  routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- routes %>% mutate(down = lag(up))%>% 
    drop_na()
    
  output <- find_score(routes, input, t)
  if(t == 1) alldat3 <- output
    if(t > 1) alldat3 <- rbind(alldat3, output)
}

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))
random_fb <- alldat3

alldat3 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed), size = 3)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")

```
```{r random-ZZ}
set.seed(123)  # For reproducibility
iterations <- 1000

combos <- ZZ_uaa_ex %>% 
  rename("up" = ID, "uaa" = '10mdem_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) %>% 
  mutate(up = paste0("r_",up),
         down = paste0("r_",down)) %>% 
  select(up)

combos <- unique(combos$up)
sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
sample <- as_tibble(sample_perms, .name_repair = "minimal")
#colnames(sample) <- paste0("I_",seq(1, iterations, 1))
colnames(sample) <- seq(1, iterations, 1)


find_score <- function(routes, input, t){
  for(x in 1:length(routes$up)){
  
    out <- calc_support(routes$up[x], routes$down[x], input)
    if(x == 1) alldat <- out
    if(x > 1) alldat <- rbind(alldat, out)

  }
  alldat$prop <- alldat$points/alldat$total
  alldat$iteration <- t

  return(alldat)
}

for(t in 1:iterations){
  routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- routes %>% mutate(down = lag(up))%>% 
    drop_na()
    
  output <- find_score(routes, input, t)
  if(t == 1) alldat3 <- output
    if(t > 1) alldat3 <- rbind(alldat3, output)
}

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))
random_zz <- alldat3

alldat3 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed), size = 3)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")

```
```{r combine-and-plot}
random_w3$shed = "W3"
random_fb$shed = "FB"
random_zz$shed = "ZZ"

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))

rbind(random_w3, random_fb, random_zz) %>% group_by(iteration, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd, shape = shed))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(color = type), size = 3)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  labs(title = "1000 random organizations",
       y = "Proportion sd",
       x = "Proportion mean")

rbind(random_w3, random_fb, random_zz) %>% group_by(iteration, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd, shape = shed))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(color = type), size = 3)+
  theme_classic()+
  facet_wrap(~shed)+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  labs(title = "Random organizations by watershed",
       y = "Proportion sd",
       x = "Proportion mean")
```

#iterate to find the best combination
```{r iterating-smart-W3}
#pick a sensor randomly, then test every other sensor to see which has the best hierarchical behavior with that one. Proceed down the list without replacement
# combos <- W3_uaa_ex %>% 
#   rename("up" = ID, "uaa" = '10mdem_flowacc') %>%
#   arrange(desc(uaa)) %>% 
#   mutate(down = lag(up)) %>% 
#    drop_na() %>% 
#   select(up, down) %>% 
#   mutate(up = paste0("r_",up),
#          down = paste0("r_",down)) %>% 
#   select(up)

#combos <- unique(combos$up)

input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- unique(colnames(input[-1]))
#find the starting point

for(t in 1:length(combos)){
 #t <- 1 
begin <- combos[t]
options_initial <- combos[combos != begin]
options <- combos[combos != begin]

routes <- data.frame("up" = rep(begin, length(options_initial)),
                     "down" = options_initial)
for(x in 1:length(options_initial)){
#x <- 1

  for(i in 1:length(options)){
        out <- calc_support(routes$up[i], routes$down[i], input)
        if(i == 1) inner_out <- out
        if(i > 1) inner_out <- rbind(inner_out, out)
  }
  
  inner_out$prop <- inner_out$points/inner_out$total
  
        kep <- inner_out[inner_out$prop == max(inner_out$prop),]
        kep <- kep[1,]
  options <- options[options != kep$down[1]]
  routes <- data.frame("up" = rep(kep$down[1], length(options)),
                     "down" = options)
print(paste0("x = ",x))

  if(x == 1) big_keep <- kep
  if(x > 1) big_keep <- rbind(big_keep, kep)
}
big_keep$start <- begin
if(t == 1) biggest_keep <- big_keep
  if(t > 1) biggest_keep <- rbind(biggest_keep, big_keep)
}
smart_iterate_w3 <- biggest_keep
smart_iterate_w3 %>% group_by(start) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point()+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed))
```
```{r iterating-smart-FB}
#pick a sensor randomly, then test every other sensor to see which has the best hierarchical behavior with that one. Proceed down the list without replacement
input <- rbind(bind23, bind24) %>%
  filter(wshed == "FB", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)
#find the starting point
combos <- unique(colnames(input[-1]))

#troubleshoot
#t <- 1
for(t in 1:length(combos)){
  
begin <- combos[t]
options_initial <- combos[combos != begin]
options <- combos[combos != begin]

routes <- data.frame("up" = rep(begin, length(options_initial)),
                     "down" = options_initial)
for(x in 1:length(options_initial)){
#x <- 5

  for(i in 1:length(options)){
#i <- 1
        out <- calc_support(routes$up[i], routes$down[i], input)
        print(paste0("i = ",i))

        if(i == 1) inner_out <- out
        if(i > 1) inner_out <- rbind(inner_out, out)
  }
  
  inner_out$prop <- inner_out$points/inner_out$total
  
        kep <- inner_out[inner_out$prop == max(inner_out$prop),]
        kep <- kep[1,]
  options <- options[options != kep$down[1]]
  routes <- data.frame("up" = rep(kep$down[1], length(options)),
                     "down" = options)
print(paste0("x = ",x))

  if(x == 1) big_keep <- kep
  if(x > 1) big_keep <- rbind(big_keep, kep)
}
big_keep$start <- begin
print(paste0("finished with",begin))
if(t == 1) biggest_keep <- big_keep
  if(t > 1) biggest_keep <- rbind(biggest_keep, big_keep)
}

smart_iterate_fb <- biggest_keep

biggest_keep %>% group_by(start) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(shape = 17)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed), size = 2)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  labs(title = "Iteratively determined best hierarchy",
       y = "Proportion sd",
       x = "Proportion mean")

```
```{r fixed-function}

#calc_support <- function(up, down, input){
#up <- routes$up[i] #input
#down <- routes$down[i]
up <- "r_28"
down <- "r_6"
incase <- data.frame(up = c(0,0), down = c(0,0))
colnames(incase) <- c(up, down)
mod3 <- input %>% select(up,down, datetime)#, -datetime)
no_dupes <- mod3 %>%
  #filter(row_number() == 1 | !apply(. == lag(.), 1, all))
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all))

#filter no dupes to drying/wetting, then plot these on a hydrograph
filter(no_dupes, up == )
no_dupes_q <- no_dupes %>% rename(DATETIME = datetime) %>% 
  left_join(q_23_f, by = "DATETIME")

ggplot(q_23_f, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  geom_point(data = no_dupes_q)+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()+
  lims(x = c(min(q_23_f$DATETIME), max(q_23_f$DATETIME)))

#should I distinguish between wetting and drying sequences?
#all flowing all the time?
check <- nrow(no_dupes)
#make it so that there cannot be a sequence without change

#error when both locations flowed the whole time; 
if(check == 1) no_dupes <- rbind(no_dupes, no_dupes, incase)

# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
#View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")
#subset from the 16 possibilities, except it is fewer because
#0000, 1111, 0101, 1010 are removed...

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down, points, total)
#error handling- in situation where both points flowed 100% of the time
if(check == 1) output$points <- NA
if(check == 1) output$total <- NA

return(output)
}

calc_support("r_28", "r_6", input)
```

#maps of hierarchical behavior
```{r prepare-sensor-locs}
sults_so_far

pk_prop <- w3_pairs_pk %>% select(up, prop) %>% 
  mutate(ID = as.numeric(substr(up, 3, 4))) %>% 
  select(ID, prop)

locs <- data_23 %>% 
  filter(wshed == "W3") %>% 
  select(ID, lat, long) %>% 
  unique() %>% 
  left_join(pk_prop, by = "ID")


locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))

```
```{r map-template}
#map of watershed 3 with depth to bedrock
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
hill <- rast(hillshade_out)

#dem
#dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
#m1 <- rast(dem)
w3_crop <- "./w3_dems/1mdem_crop.tif"
m1 <- rast(w3_crop)
#watershed boundary
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)

#w3 network- thing I need to change
#read in shapefile of stream converted in ARC
vect_stream_path <- "./AGU24posterAnalysis/vector_stream/vector_stream.shp"
#stream as a vector
vect_stream <- vect(vect_stream_path)
#plot(vect_stream)
#crop to watershed boundary
w3_stream_crop <- crop(vect_stream, w3_outline)
plot(w3_stream_crop)
#or i could use old classification


ggplot()+
  geom_spatraster(data = hill, show.legend=FALSE)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5, , show.legend=FALSE)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
    new_scale_fill() +
  geom_sf(data = w3_outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = w3_stream_crop, colour = "midnightblue") +
    #geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 6) +
  geom_sf(data = lcc, aes(fill = prop), pch = 21, size = 3) +
  theme(rect = element_rect(fill = "transparent", color = NA))+
  scale_fill_gradient(low = "black",
                         high = "white",
                         name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"))+
  ggspatial::annotation_scale(location = 'br', pad_y = unit(1, "cm"), pad_x = unit(1, "cm"))+
  ggtitle("Flow Permanence (Pk), W3")
```
```{r define-plotting-function}
#use above code to write a function to plot the prop for each hierarchy and each watershed

#first, specify input dataset
#wshed inputs = c("W3", "FB", "ZZ")
#hierarchy = c("position", "pk", "dof", "uaa")
plot_in_space <- function(dataset, watershed, hierarchy){
  
  ##get correct geospatial files depending on input watershed
if(watershed == "W3") {
  #background rasters
  hill <- rast("./w3_dems/1mdem_hillshade.tif")
  m1 <- rast("./w3_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./w3_dems/w3_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./AGU24posterAnalysis/vector_stream/vector_stream.shp")
  stream_crop <- crop(vect_stream, w3_outline)
  
} else if(watershed == "FB"){
  #background rasters
  hill <- rast("./fb_dems/1mdem_hillshade.tif")
  m1 <- rast("./fb_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./fb_dems/fb_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./carrieZigZag/FB_network.shp")
  stream_crop <- crop(vect_stream, outline)
  
} else if(watershed == "ZZ"){
  #background rasters
  hill <- rast("./zz_dems/1mdem_hillshade.tif")
  m1 <- rast("./zz_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./zz_dems/zz_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./carrieZigZag/zigzag_streams.shp")
  stream_crop <- crop(vect_stream, outline)
  
} else {
  stop("Not a study watershed!")
}
  
  #take the input dataset of props, and format for plotting
  prop <- dataset %>% select(up, prop) %>%
    mutate(ID = as.numeric(substr(up, 3, 4))) %>%
    select(ID, prop)
  
  locs <- data_24 %>%
    filter(wshed == watershed) %>%
    rename("ID" = number) %>% 
    select(ID, lat, long) %>%
    unique() %>%
    left_join(prop, by = "ID") %>% 
    drop_na()
  
  locs_shape <- vect(locs, geom = c("long", "lat"), crs = "+proj=longlat +datum=WGS84")
  #reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
  lcc <- terra::project(locs_shape, crs(m1))



#specify plotting depending on hierarchy being tested

if(hierarchy == "position") {
  hierarchy_color <- "dodgerblue3"
  hierarchy_label <- "Relative Position"
  
} else if(hierarchy == "pk"){
  hierarchy_color <- "#FFA400"
  hierarchy_label <- "Flow Permanence"
  
} else if(hierarchy == "dof"){
  hierarchy_color <- "#397367"
  hierarchy_label <- "Duration of Flow"
  
} else if(hierarchy == "uaa"){
  hierarchy_color <- "#7E6B8F"
  hierarchy_label <- "Drainage Area"
  
} else {
  stop("Not a hierarchy anticipated!")
}

#final plot
named_plot <- ggplot()+
  geom_spatraster(data = hill, show.legend=FALSE)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = m1, alpha = 0.5, , show.legend=FALSE)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
    new_scale_fill() +
  geom_sf(data = outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = stream_crop, colour = "midnightblue", alpha = 0.3) +
    #geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 6) +
  geom_sf(data = lcc, aes(fill = prop), pch = 21, size = 3) +
  theme(rect = element_rect(fill = "transparent", color = NA))+
  scale_fill_gradient(low = "white",
                         high = hierarchy_color,
                         name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"))+
  ggspatial::annotation_scale(location = 'br', pad_y = unit(1, "cm"), pad_x = unit(1, "cm"))+
  ggtitle(paste0(hierarchy_label,", ", watershed))
named_plot
  #ggsave(paste0("hierarchy_maps/",watershed,"_",hierarchy,".jpg"), named_plot)

}
```
```{r run-function-for-scenarios}
#position
plot_in_space(w3_pairs, "W3", "position")
plot_in_space(fb_pairs, "FB", "position")
plot_in_space(zz_pairs, "ZZ", "position")

#pk
plot_in_space(w3_pairs_pk, "W3", "pk")
plot_in_space(fb_pairs_pk, "FB", "pk")
plot_in_space(zz_pairs_pk, "ZZ", "pk")

#dof
plot_in_space(w3_pairs_dof, "W3", "dof")
plot_in_space(fb_pairs_dof, "FB", "dof")
plot_in_space(zz_pairs_dof, "ZZ", "dof")

#topo
plot_in_space(w3_pairs_topo, "W3", "uaa")
plot_in_space(fb_pairs_topo, "FB", "uaa")
plot_in_space(zz_pairs_topo, "ZZ", "uaa")





```

After meeting on 2/6/25, I am going to plot the proportion of time each point follows the hierarchy using a square divided into 4 sections, one for each hierarchy
#showing all hierarchies on one plot- square plot (pigs in space)
```{r define-function}
#change to have 4 inputs, the 4 hierarchies... actually better to format hierarchy before plotting
format_outputs <- function(dataset, name){
  dataset %>% select(up, prop) %>%
    mutate(ID = as.numeric(substr(up, 3, 4))) %>%
    select(ID, prop)
}

position_formatted <- format_outputs(w3_pairs) %>% rename("Pos" = prop)
pk_formatted <- format_outputs(w3_pairs_pk) %>% rename("Pk" = prop)
dof_formatted <- format_outputs(w3_pairs_dof) %>% rename("Dof" = prop)
topo_formatted <- format_outputs(w3_pairs_topo) %>% rename("Topo" = prop)

#reformat to take 4 formatted datasets as input
pigs_in_space <- function(position_formatted,
                          pk_formatted, 
                          dof_formatted,
                          topo_formatted,
                          watershed, offset, size){
  
  ##get correct geospatial files depending on input watershed
if(watershed == "W3") {
  #background rasters
  hill <- rast("./w3_dems/1mdem_hillshade.tif")
  m1 <- rast("./w3_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./w3_dems/w3_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./AGU24posterAnalysis/vector_stream/vector_stream.shp")
  stream_crop <- crop(vect_stream, w3_outline)
  
} else if(watershed == "FB"){
  #background rasters
  hill <- rast("./fb_dems/1mdem_hillshade.tif")
  m1 <- rast("./fb_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./fb_dems/fb_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./carrieZigZag/FB_network.shp")
  stream_crop <- crop(vect_stream, outline)
  
} else if(watershed == "ZZ"){
  #background rasters
  hill <- rast("./zz_dems/1mdem_hillshade.tif")
  m1 <- rast("./zz_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./zz_dems/zz_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./carrieZigZag/zigzag_streams.shp")
  stream_crop <- crop(vect_stream, outline)
  
} else {
  stop("Not a study watershed!")
}
  
  #take the input dataset of props, and format for plotting
  
  locs <- data_24 %>%
    filter(wshed == watershed) %>%
    rename("ID" = number) %>% 
    select(ID, lat, long) %>%
    unique() %>%
    left_join(position_formatted, by = "ID") %>% 
    left_join(pk_formatted, by = "ID") %>% 
    left_join(dof_formatted, by = "ID") %>% 
    left_join(topo_formatted, by = "ID") #%>% drop_na()
  
  locs_shape <- vect(locs, geom = c("long", "lat"), crs = "+proj=longlat +datum=WGS84")
  #reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
  lcc <- terra::project(locs_shape, crs(m1))
  
  lcc <- as.data.frame(lcc, geom = "XY")



#specify plotting depending on hierarchy being tested

#final plot
named_plot <- ggplot()+
  geom_spatraster(data = hill, show.legend=FALSE)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = m1, alpha = 0.5, , show.legend=FALSE)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
    new_scale_fill() +
  geom_sf(data = outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = stream_crop, colour = "midnightblue", alpha = 0.3) +
    #geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 6) +
  #plot the squares, but offset from center point
  #geom_sf(data = lcc, aes(fill = "Position"),color = "dodgerblue3", pch = 22, size = 1, position = position_nudge(x = offset, y = offset)) +
  #geom_sf(data = lcc, aes(fill = "Pk"), color ="#FFA400",  pch = 22, size = 1, position = position_nudge(x = offset, y = -offset)) +
  geom_point(data = lcc, aes(x = x, y = y, fill = Pos),  
               pch = 22, size = size, position = position_nudge(x = offset, y = -offset)) + 
  scale_fill_gradient(low = "white",
                         high = "dodgerblue3",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+
      new_scale_fill() +
    geom_point(data = lcc, aes(x = x, y = y, fill = Pk),  
               pch = 22, size = size, position = position_nudge(x = offset, y = offset)) + 
scale_fill_gradient(low = "white",
                         high = "#FFA400",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+
  new_scale_fill() +
    geom_point(data = lcc, aes(x = x, y = y, fill = Dof),  
               pch = 22, size = size, position = position_nudge(x = -offset, y = -offset)) + 
  scale_fill_gradient(low = "white",
                         high = "#397367",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+
  new_scale_fill() +
    geom_point(data = lcc, aes(x = x, y = y, fill = Topo),  
               pch = 22, size = size, position = position_nudge(x = -offset, y = offset)) + 
  scale_fill_gradient(low = "white",
                         high = "#7E6B8F",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+


  theme(rect = element_rect(fill = "transparent", color = NA))+
  # scale_fill_gradient(low = "white",
  #                        high = hierarchy_color,
  #                        name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
  #                     breaks = c(0.25, 0.5, 0.75),
  #                     labels = c("25%", "50%", "75%"))+
  ggspatial::annotation_scale(location = 'br', pad_y = unit(1, "cm"), pad_x = unit(1, "cm"))
  # ggtitle(paste0(hierarchy_label,", ", watershed))
named_plot
  #ggsave(paste0("hierarchy_maps/",watershed,"_",hierarchy,".jpg"), named_plot)

}

#formatting that looks decent on big screen, in plot viewer pane
pigs_in_space(position_formatted,
                          pk_formatted, 
                          dof_formatted,
                          topo_formatted,
                          "W3", 8, size = 5)


```
```{r pigs-in-space-FB}

position_formatted <- format_outputs(fb_pairs) %>% rename("Pos" = prop)
pk_formatted <- format_outputs(fb_pairs_pk) %>% rename("Pk" = prop)
dof_formatted <- format_outputs(fb_pairs_dof) %>% rename("Dof" = prop)
topo_formatted <- format_outputs(fb_pairs_topo) %>% rename("Topo" = prop)

pigs_in_space(position_formatted,
                          pk_formatted, 
                          dof_formatted,
                          topo_formatted,
                          "FB", 8, size = 5)
```
```{r pigs-in-space-ZZ}

position_formatted <- format_outputs(zz_pairs) %>% rename("Pos" = prop)
pk_formatted <- format_outputs(zz_pairs_pk) %>% rename("Pk" = prop)
dof_formatted <- format_outputs(zz_pairs_dof) %>% rename("Dof" = prop)
topo_formatted <- format_outputs(zz_pairs_topo) %>% rename("Topo" = prop)

pigs_in_space(position_formatted,
                          pk_formatted, 
                          dof_formatted,
                          topo_formatted,
                          "ZZ", 9, size = 5)
```
#test with Botter & Durighetto data
Conclusion- the max number of observations of the whole network in the dataset they used for their analysis was 7-35 observations. I have 10,000 observations of my network. Doing this kind of analysis on their data will not work I think.
```{r show-number-of-obs}
#read in Botter et al 2021 data
bigdata <- read_csv("https://researchdata.cab.unipd.it/376/2/states.csv")
#show the number of observations of the whole network used in their analysis
bigdata %>% 
  group_by(Catchment, Stretch) %>% 
  summarise(num_obs = length(Date)) %>% 
  select(Catchment, num_obs) %>% unique()

#determine the number of observations of the whole network from my dataset
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary, wshed) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
#outputs the number of observations for each sensor in my study
data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary, wshed) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  group_by(ID, wshed) %>% 
  summarise(num_obs = length(DATETIME)) %>% 
  select(wshed, ID, num_obs) %>% unique() %>% 
  View()


```
```{r test-hierarchical-behavior}
bigdata
```

#New idea, make network maps where points are connected 
```{r relative-position-UNFINISHED}

```

```{r flow-permanenc-w3}
#map where points are connected by upstream, downstream relationships
#start with routes from local persistency
#left join to lat/longs
nad_coords <- as.data.frame(lcc, geom = "XY")

routes %>% rename("ID" = up) %>% 
  left_join(nad_coords, by = "ID") %>% 
  rename("up" = ID,
         "ID" = down) %>% 
  left_join(nad_coords, by = "ID") %>% 
  #slice(c(seq(1, 30, 2))) %>% 
  #slice(c(seq(2, 30, 2))) %>% 
  ggplot()+
  geom_spatraster(data = hill, show.legend=FALSE)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5, , show.legend=FALSE)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
    new_scale_fill() +
  geom_sf(data = w3_outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = w3_stream_crop, colour = "midnightblue", alpha = 0.5) +
  geom_point(aes(x = x.x, y = y.x))+
  geom_segment(aes(x = x.x, y = y.x, 
                   xend = x.y, yend = y.y, 
                   linewidth = prop.x, 
                   color = prop.x), 
               arrow=arrow(length=unit(0.3,"cm"))
               )+
scale_color_continuous(type = "viridis")+
                       theme(rect = element_rect(fill = "transparent", color = NA))+
  scale_fill_gradient(low = "black",
                         high = "white",
                         name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"))+
  ggspatial::annotation_scale(location = 'br', pad_y = unit(1, "cm"), pad_x = unit(1, "cm"))+
  ggtitle("Flow Permanence (Pk), W3")


#line connecting 
```

#Nick's idea: PLOT CHANGE VERSUS NO CHANGE IN TIME
```{r unfinished}
w3_pairs

input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

mod3 <- input %>% select(w3_pairs$up[1],w3_pairs$down[1], -datetime)
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))

```


Also test what happens if I only do hourly, daily, etc timestep for my earlier loops
#Determine all alternate scenarios
##testing to see if there will be differences
```{r hourly-W3}
#equivalent to hush 2 from earlier,
#make a new input for each watershed chunk
input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)
#now make loop to loop through all pairs of sensors
routes <- read_csv("w3_flowrouting.csv") %>% 
  rename("up" = drains_from, "down" = drains_to) %>% 
  select(sensor, down) %>% 
  filter(down != 100)

for(x in 1:length(routes$sensor)){
  up <- paste0("r_",routes$sensor[x])
  down <- paste0("r_",routes$down[x])
  
  out <- calc_support(up, down, input)
  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

}

alldat$prop <- alldat$points/alldat$total
w3_hourly <- alldat
#w3_pairs <- alldat
#this shows the proportion of time that pairs of upstream/downstream sensors behave according to the hypothesis of wetting up and drying down
hist(w3_new$prop)
hist(w3_pairs$prop)
hist(w3_hourly$prop)

plot(ecdf(w3_hourly$prop))
plot(ecdf(alldat$prop))
ggplot()+
  stat_ecdf(data = w3_pairs, aes(prop))+
  stat_ecdf(data = w3_new, aes(prop), color = "blue")

#plot them both on a boxplot just to put it in terms that I have been using
w3_30min_form <- data.frame("prop" = w3_pairs$prop, "type" = "pk", "shed" = "W3", timing = "30mins")
w3_hourly_form <- data.frame("prop" = w3_hourly$prop, "type" = "pk", "shed" = "W3", timing = "hourly")
w3_4hr_form <- data.frame("prop" = alldat$prop, "type" = "pk", "shed" = "W3", timing = "4hr")
w3_daily_form <- data.frame("prop" = alldat$prop, "type" = "pk", "shed" = "W3", timing = "daily")



sults_so_far <- rbind(w3_30min_form, w3_hourly_form, w3_4hr_form, w3_daily_form)

sults_so_far %>% 
  ggplot(aes(x = shed, y = prop, color = timing)) +
  geom_boxplot()+
  theme_classic()+
    geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Watershed",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(
                     values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     
                     name = "Organizing Scheme")+
  facet_wrap(~type)
```
```{r 4hrs-W3}
input <- rbind(bind23, bind24) %>%
  mutate(hour = hour(datetime)) %>% 
  filter(wshed == "W3", hour %in% c(0,4,8,12,16,20,24), mins %in% c(0)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)
#now make loop to loop through all pairs of sensors
routes <- read_csv("w3_flowrouting.csv") %>% 
  rename("up" = drains_from, "down" = drains_to) %>% 
  select(sensor, down) %>% 
  filter(down != 100)

for(x in 1:length(routes$sensor)){
  up <- paste0("r_",routes$sensor[x])
  down <- paste0("r_",routes$down[x])
  
  out <- calc_support(up, down, input)
  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

}

alldat$prop <- alldat$points/alldat$total
hist(alldat$prop)
```
```{r daily-W3}
#equivalent to hush 2 from earlier,
#make a new input for each watershed chunk
input <- rbind(bind23, bind24) %>%
  mutate(hour = hour(datetime)) %>% 
  filter(wshed == "W3", hour %in% c(12), mins %in% c(0)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)
#now make loop to loop through all pairs of sensors
routes <- read_csv("w3_flowrouting.csv") %>% 
  rename("up" = drains_from, "down" = drains_to) %>% 
  select(sensor, down) %>% 
  filter(down != 100)

for(x in 1:length(routes$sensor)){
  up <- paste0("r_",routes$sensor[x])
  down <- paste0("r_",routes$down[x])
  
  out <- calc_support(up, down, input)
  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

}

alldat$prop <- alldat$points/alldat$total
w3_daily <- alldat
#w3_pairs <- alldat
#this shows the proportion of time that pairs of upstream/downstream sensors behave according to the hypothesis of wetting up and drying down
hist(w3_new$prop)
hist(w3_pairs$prop)
hist(w3_hourly$prop)
hist(w3_daily$prop)



```
##now trying across all scenarios
```{r define-fucntion}
#write function to go through different timescales quickly. 
#specify routes as an input
run_scenario <- function(routes, hierarchy, watershed, timescale){

  #if statements to detect timescale, calculate appropriate inputs
  if(timescale == "30mins"){
    input <- rbind(bind23, bind24) %>%
      filter(wshed == watershed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
  } 
  else if(timescale == "hourly"){
    input <- rbind(bind23, bind24) %>%
      filter(wshed == watershed, mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
  } 
  else if(timescale == "4hr"){
    input <- rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == "W3", hour %in% c(0,4,8,12,16,20,24), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
  } 
  else if(timescale == "daily"){
    input <- rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == "W3", hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
  } 
  else {
    stop("Not a timescale anticipated!")
  }

#loop through sensors using calc_support function
for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  
  out <- calc_support(up, down, input)
  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

}

alldat$prop <- alldat$points/alldat$total
#format output for boxplot and cdf plotting
formatted_output <- data.frame("prop" = alldat$prop, 
                               "type" = hierarchy, 
                               "shed" = watershed, 
                               "timing" = timescale)

return(formatted_output)
}
#end of function
#define routes
routes_w3_position <- read_csv("w3_flowrouting.csv") %>% 
  rename("up" = drains_from, "down" = drains_to) %>% 
  select(sensor, down) %>% 
  filter(down != 100)

routes_w3_pk <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)
#test function
sults_so_far <- rbind(run_scenario(routes_w3_position, "position", "W3", "30mins"),
                      run_scenario(routes_w3_position, "position", "W3", "hourly"),
                      run_scenario(routes_w3_position, "position", "W3", "4hr"),
                      run_scenario(routes_w3_position, "position", "W3", "daily"),
                      
                      run_scenario(routes_w3_pk, "pk", "W3", "30mins"),
                      run_scenario(routes_w3_pk, "pk", "W3", "hourly"),
                      run_scenario(routes_w3_pk, "pk", "W3", "4hr"),
                      run_scenario(routes_w3_pk, "pk", "W3", "daily"))

sults_so_far %>% 
  mutate(timing = fct_relevel(timing,
                              c("30mins", "hourly", "4hr", "daily"))) %>% 
  ggplot(aes(x = timing, y = prop, color = timing)) +
  geom_boxplot()+
  theme_classic()+
    geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(
                     values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     
                     name = "Organizing Scheme")+
  facet_wrap(shed~type)
```

```{r}
input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)
#now make loop to loop through all pairs of sensors
routes <- read_csv("w3_flowrouting.csv") %>% 
  rename("up" = drains_from, "down" = drains_to) %>% 
  select(sensor, down) %>% 
  filter(down != 100)

for(x in 1:length(routes$sensor)){
  up <- paste0("r_",routes$sensor[x])
  down <- paste0("r_",routes$down[x])
  
  out <- calc_support(up, down, input)
  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

}

alldat$prop <- alldat$points/alldat$total
w3_hourly <- alldat
```



Separate chunk for discharge stuff
#Where on the hydrograph do transitions in states between two sensors occur?
```{r hydrograph-analysis}
#separate function to determine when on hydrograph transitions occur
determine_transitions <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 3
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])

no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
  drop_na()

colnames(no_dupes) <- c("up", "down", "datetime")
#determine if it is a wetting or drying transition by lagging up and down columns, then concatenating into a single number
coded_times <- no_dupes %>% 
  mutate(up_lag = lag(up),
         down_lag = lag(down)) %>% 
  mutate(coded = paste0(up_lag, down_lag, up, down)) %>% 
  mutate(direction = case_when(coded == "0001"| coded == "0111" ~ "wetting",
                               coded == "1101"| coded == "0100" ~ "drying")) %>% 
  filter(direction %in% c("wetting", "drying"))
return(coded_times)
}

#test functions
determine_transitions("r_23", "r_6", input)
all_transitions <- function(routes, input){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- determine_transitions(up, down, input)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  
  return(alldat)
}

all_trans <- all_transitions(routes, input)

all_trans$binary <- 1
all_trans$binary[all_trans$direction == "drying"] <- -1

testtt <- all_trans %>% group_by(datetime) %>% 
  filter(binary == -1) %>% 
  summarise(prop_wetting = sum(binary)) %>% 
  rename(DATETIME = datetime) %>% 
  right_join(q_23_f, by = "DATETIME")

ggplot(testtt, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  geom_point(data = drop_na(testtt), aes(x  = DATETIME, y = Q_mm_day, color = prop_wetting))+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()+
  scale_color_viridis_c(name = "Number of Drying")
```

```{r additional-quality-control}
#figure out what is going on for sensors 19 and 12 in watershed 3
#figure out a way to quickly diagnose/quality control all sensor pairs
rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == "W3", hour %in% c(12), mins %in% c(0),
             ID %in% c(19, 12)) %>%
      select(datetime, binary, ID) %>%
  ggplot()+
  geom_tile(aes(x = datetime, y = as.character(ID), fill = as.integer(binary)))
      
#for each pair, we are limited by the maximum extent of the record...
# to incorporate uncertainty, also provide the proportion of observations not used?
# also need to separate into separate chunks to account for gaps in record

rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == "W3", hour %in% c(12), mins %in% c(0),
             ID %in% c(19, 12)) %>%
      select(datetime, binary, ID) %>%
  filter(binary == 0)
```

#statistical test- one sample rank wilcoxon test
Way to ultimately summarise all of these scenarios- with the single test statistic from wilcoxon test
```{r}
# determine if the median of the sample (m) is greater than the theoretical value (mu)
wiltest <- sults_so_far %>% 
  filter(type == "position", shed == "W3")
median(wiltest$prop)

randomtest <- random_fb %>% 
  filter(iteration == 2)
median(randomtest$prop)
wilcox.test(wiltest$prop, mu = 0.5, alternative = "greater", exact = TRUE, conf.int = TRUE)
wilcox.test(randomtest$prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)

binom <- binom.test(sum(wiltest$prop > 0.5), length(wiltest$prop), p = 0.5, alternative = "greater")
binom.test(sum(randomtest$prop > 0.5), length(randomtest$prop), p = 0.5, alternative = "greater")

test_results <- wilcox.test(wiltest$prop, mu = 0.5, alternative = "greater", exact = TRUE, conf.int = TRUE)
pvalue <- test_results$p.value
upperb <- test_results$conf.int[1]
lowerb <- test_results$conf.int[2]

binom$p.value
binom$conf.int[1:2]

```

```{r run-test-on-everything}
#calculate the test statistics on results so far- without temporal degradation
sults_so_far %>% 
  group_by(type, shed) %>% 
  summarise(wcx_pvalue = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$p.value,
            wcx_upperb = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$conf.int[1],
            wcx_lowerb = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$conf.int[2])

sults_so_far %>% 
  group_by(type, shed) %>% 
  summarise(sign_p = binom.test(sum(prop > 0.5), length(prop), p = 0.5, alternative = "greater")$p.value,
            sign_upperb = binom.test(sum(prop > 0.5), length(prop), p = 0.5, alternative = "greater")$conf.int[1],
            sign_lowerb = binom.test(sum(prop > 0.5), length(prop), p = 0.5, alternative = "greater")$conf.int[2]
)
```

Test with and without replacement

Determine if there is something systematic that defines the sequence iteratively or randomly determined
#calculating discharge
```{r}
#source function I wrote in separate script
source("calcQ.R")
```

#plot hypsometric curves for each stream network
Extract elevation along the streams (might need ARC), then plot versus distance