---
title: "MapsForStoryboard1"
format: html
editor_options: 
  chunk_output_type: console
---
Not just maps for storyboard but also figures for storyboard

3/18/25
Trying to format for posterity

```{r setup}
#loading packages
library(pacman)
p_load(tidyverse, terra, tidyterra, whitebox, scales, wesanderson, caret, plotly,ggnewscale, sf, rgeoboundaries, elevatr, patchwork, ggspatial, zoo)
#whitebox::install_whitebox()

#reading in final format data for summer 23
data_23 <- read_csv("./DataForMary/HB_stic.csv")
#reading in final format data for summer 24
data_24 <- read_csv("./summer2024/STICS2024.csv")

data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

bind23 <- data_23 %>% 
  select(datetime, ID, wshed, binary, mins)
bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)
```
# Figure 1: maps and distributions of topo variables
## Maps
```{r NH-map-HB-location}
#make map of NH, pane to show where hubbard brook is
usa <- rgeoboundaries::geoboundaries("USA", "adm1")
NH <- usa[usa$shapeName == "New Hampshire",]
NH_outline <- vect(NH)
mdt <- get_elev_raster(locations = NH, z = 10, clip = "locations")
plot(mdt)
big <- rast(mdt) %>% 
  crop(NH_outline) %>% 
  mask(NH_outline)
writeRaster(big, "./w3_dems/NH.tif", overwrite = TRUE)

NH_path <- "./w3_dems/NH.tif"
hillshade_out <- "./w3_dems/NH_hillshade.tif"
wbt_hillshade(
  dem = NH_path,
  output = hillshade_out,
)
hill <- rast(hillshade_out)
hill2 <- crop(hill, NH_outline)
hill3 <- mask(hill2, NH_outline)
plot(hill)

#read in shapefil of watershed boundaries for HB
#path to file
HB_bounds <- "./HB/hbef_wsheds/hbef_wsheds.shp"
sheds <- vect(HB_bounds)
sheds <- terra::project(sheds, crs(hill))
plot(sheds)
#create boundaries of HB for showing location in NH
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
plot(m1)
HB_bounds <- vect(ext(m1, cells=NULL), crs = crs(m1))
HB_bounds <- terra::project(HB_bounds, crs(hill))

plot(HB_bounds)


#use bounds of sheds to crop NH dem
ybounds <- c(43.916,43.96219)
xbounds <- c(-71.80451, -71.69687)
plot(hill, xlim = xbounds, ylim = ybounds)
gauge <- c(43.939574, -71.703100)


NH_map <- ggplot()+
  geom_spatraster(data = hill3)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"), na.value = NA)+
    new_scale_fill() +
  geom_spatraster(data = big, alpha = 0.5)+
  
  #plot rectangle to show HB extent
      geom_sf(data = HB_bounds, color = "black", alpha = 0, lwd = 1.2) +

    #geom_point(aes(y=43.939574, x=-71.703100), colour="black", pch = 20, size = 8)+
   scale_fill_hypso_c(palette = "dem_screen")
NH_map

ggsave("NH.png", plot = NH_map, scale = 1, limitsize = FALSE, #height = 11,width = 4, units = "in"
       bg = NULL,)

ggplot()+
  geom_spatraster(data = hill3)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"), na.value = NA)+
    new_scale_fill() +
  geom_spatraster(data = big, alpha = 0.5)+
    geom_sf(data = sheds, fill = "darkslategray3", color = "black", alpha = 0.3) +
   scale_fill_hypso_c(palette = "dem_screen")+
  lims(x = xbounds,
       y = ybounds)
```
```{r whole-Valley}
#use HB dem from previous maps, but just don't crop it, include outline of the whole valley, and the outlines of each study shed
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
plot(m1)

#old hillshade
valley_hill <- rast("./HB/1m hydro enforced DEM/wall_hillshade.tif")
#old hillshade has a different angle, emphasizes the lineaments in the east better, but new one looks niver
#see if a newly calculated one looks better
hillshade_out <- "./HB/1m hydro enforced DEM/new_hillshade.tif"
wbt_hillshade(
  dem = dem,
  output = hillshade_out,
)
valley_hill2 <- rast(hillshade_out)
plot(valley_hill2)


#get the outlines for each watershed
#W3
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)
#FB
fb_shed <- "./fb_dems/fb_shed.tif"
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)
#ZZ
zz_shed <- "./zz_dems/zz_shed.tif"
zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)



valley_plot <- ggplot()+
  geom_spatraster(data = valley_hill2, maxcell = 5e+05)+#make max cells bigger on final version
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = m1, alpha = 0.5)+
  geom_sf(data = fb_outline, fill = NA, color = "black", alpha = 0.3, lwd = 1.5) +
  geom_sf(data = w3_outline, fill = NA, color = "black", alpha = 0.3, lwd = 1.5) +
  geom_sf(data = zz_outline, fill = NA, color = "black", alpha = 0.3, lwd = 1.5) +

  #geom_sf(data = fb_net, colour = "darkslategray3") +#, lwd = 3) +
    #geom_sf(data = lcc, colour = "midnightblue") + #, pch = 19, size = 6) +
  #geom_sf(data = fb_pour, colour = "black") + #, pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen")+#, limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0.5, "cm"), 
                              pad_y = unit(0.5, "cm")) #, line_width = 3, text_cex = 5, tick_height = 20)
valley_plot
#ggsave("valley.png", plot = valley_plot, limitsize = FALSE, units = "in", bg = NULL,)

#combine 3 watershed shapes into one layer, and either label or make distinct colors

```
```{r FB-topography-map}
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
plot(m1)

#reading in final format data for summer 23
data_23 <- read_csv("./DataForMary/HB_stic.csv")
#reading in final format data for summer 24
data_24 <- read_csv("./summer2024/STICS2024.csv")

#define the rectangular area that will be shown on final map
#determined for each watershed, from figures4poster script
ybounds <- c(4868850,4869650)
xbounds <- c(279350, 280450)
plot(m1, xlim = xbounds, ylim = ybounds)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
plot(crop1)
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./fb_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
fb_crop <- "./fb_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
fb_net <- vect("./carrieZigZag/FB_network.shp")
plot(fb_net)

###pour point to define where the watershed boundary is
#manually type coords from windows computer
fb_pour_coords <- data.frame("easting" = 280400,
                             "northing" = 4869120)
#convert to SpatVector object
fb_pour <- vect(fb_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
fb_pour_filename <- "./fb_dems/fb_pour.shp"
#writeVector(fb_pour, fb_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
######
#breach and fill I guess
fb_crop <- "./fb_dems/1mdem_crop.tif"

fb_breached <- "./fb_dems/1mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = fb_crop,
  output = fb_breached,
  dist = 1,
  fill = TRUE)

fb_filled <- "./fb_dems/1mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = fb_breached,
  output = fb_filled
)
#calculate flow accumulation and direction
fb_flowacc <- "./fb_dems/1mdem_fb_flowacc.tif"
wbt_d8_flow_accumulation(input = fb_filled,
                         output = fb_flowacc)
plot(rast(fb_flowacc))
fb_d8pt <- "./fb_dems/1mdem_fb_d8pt.tif"
wbt_d8_pointer(dem = fb_filled,
               output = fb_d8pt)
plot(rast(fb_d8pt))


#delineate streams
fb_streams <- "./fb_dems/fb_streams.tif"
wbt_extract_streams(flow_accum = fb_flowacc,
                    output = fb_streams,
                    threshold = 8000)
plot(rast(fb_streams))
plot(as.lines((as.polygons(rast(fb_streams)))),
     xlim = c(279800, 279900), ylim = c(4869100,4869200))
#results in weird lines, figure out how to simplify
topo_streams <- as.lines(as.polygons(rast(fb_streams)))

points(lcc)
#snap pour point to streams
fb_pour_snap <- "./fb_dems/fb_pour_snap.shp"
wbt_jenson_snap_pour_points(pour_pts = fb_pour_filename,
                            streams = fb_streams,
                            output = fb_pour_snap,
                            snap_dist = 10)
fb_pour_snap_read <- vect("./fb_dems/fb_pour_snap.shp")
plot(rast(fb_streams), 
     xlim = c(280200, 280410),
     ylim = c(4869300, 4869000))
points(fb_pour_snap_read, pch = 1)

fb_shed <- "./fb_dems/fb_shed.tif"
wbt_watershed(d8_pntr = fb_d8pt,
              pour_pts = fb_pour_snap,
              output = fb_shed)

plot(rast(fb_shed))
#convert raster of watershed area to vector for final mapping
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)
plot(fb_outline)

#get sensor locations from STIC data, format
locs <- data_23 %>% 
  filter(wshed == "FB") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))
plot(lcc)

#assign destination for hillshade calculation
hillshade_out <- "./fb_dems/1mdem_hillshade.tif"
wbt_hillshade(
  dem = fb_crop,
  output = hillshade_out,
)
hill <- rast(hillshade_out)
plot(hill)

fb_slope <- "./fb_dems/1mdem_slope.tif"
wbt_slope(dem = fb_filled,
          output = fb_slope,
          units = "degrees")

fb_twi <- "./fb_dems/1mdem_twi.tif"
wbt_wetness_index(sca = fb_flowacc, #flow accumulation
                  slope = fb_slope,
                  output = fb_twi)


#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.
fb_map <- ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = fb_outline, fill = NA, color = "black", alpha = 0.3) +#, lwd = 3) +
  geom_sf(data = fb_net, colour = "darkslategray3") +#, lwd = 3) +
    geom_sf(data = lcc, colour = "midnightblue") + #, pch = 19, size = 6) +
  geom_sf(data = fb_pour, colour = "black") + #, pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen", limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0.5, "cm"), 
                              pad_y = unit(0.5, "cm")) #, line_width = 3, text_cex = 5, tick_height = 20)

fb_map

#Map of TWI not included
# twi_output <- "./fb_dems/10mdem_twi.tif"
# 
# plot(rast(twi_output), xlim = xbounds, ylim = ybounds)
# ggplot()+
#   geom_spatraster(data = rast(twi_output))+
#   theme_void()+
#   lims(x = xbounds, y = ybounds)+
#   theme(legend.position = "")+
#     geom_sf(data = fb_outline, fill = NA, color = "black", alpha = 0.3) +#, lwd = 3) +
#     geom_sf(data = lcc, colour = "midnightblue", pch = 1) + #, pch = 19, size = 6) +
#    scale_fill_hypso_c(palette = "arctic")+
#   theme(rect = element_rect(fill = "transparent", color = NA))+
#   ggspatial::annotation_scale(location = 'tr', pad_x = unit(0.5, "cm"), 
#                               pad_y = unit(0.5, "cm"))


```
```{r try-NAIP-UNFINISHED}
#read in NAIP imagery for true color composite image
color <- sprc(c("./HB/Original zips/m_4307102_ne_19_030_20230823.jp2",
       "./HB/Original zips/m_4307102_nw_19_030_20230823.jp2",
       "./HB/Original zips/m_4307102_se_19_030_20230823.jp2",
       "./HB/Original zips/m_4307102_sw_19_030_20230823.jp2",
       "./HB/Original zips/m_4307103_ne_19_030_20230823.jp2",
       "./HB/Original zips/m_4307103_nw_19_030_20230823.jp2"))

stackT <- stack("./HB/Original zips/m_4307103_nw_19_030_20230823.jp2")


plot(rast("./HB/Original zips/m_4307103_nw_19_030_20230823.jp2"))
naip_csf_br <- brick(naip_csf_st)
inMemory(naip_csf_br)

plotRGB(naip_csf_br,
        r = 1, g = 2, b = 3,
        main = "RGB image \nColdsprings fire scar")
big_image <- merge(color)
```
##Fixed maps for department seminar
```{r}
HB_bounds <- "./HB/hbef_wsheds/hbef_wsheds.shp"
sheds <- vect(HB_bounds)
sheds <- terra::project(sheds, crs(hill))
plot(sheds)

hb_outline <- aggregate(sheds)


shed_colors <- c("#397367", "#FFD166", "#7E6B8F")

valley_plot <- ggplot()+
  geom_spatraster(data = valley_hill2, maxcell = 5e+05)+#make max cells bigger on final version
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = m1, alpha = 0.5)+
  geom_sf(data = hb_outline, fill = NA, color = "black", alpha = 0.6, lwd = 1, lty = 2) +
  geom_sf(data = fb_outline, fill = shed_colors[1], color = shed_colors[1], alpha = 0.6, lwd = 1) +
  geom_sf(data = w3_outline, fill = shed_colors[2], color = shed_colors[2], alpha = 0.6, lwd = 1) +
  geom_sf(data = zz_outline, fill = shed_colors[3], color = shed_colors[3], alpha = 0.6, lwd = 1) +

  #geom_sf(data = fb_net, colour = "darkslategray3") +#, lwd = 3) +
    #geom_sf(data = lcc, colour = "midnightblue") + #, pch = 19, size = 6) +
  #geom_sf(data = fb_pour, colour = "black") + #, pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen")+#, limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(0.5, "cm"))
valley_plot

ggsave("valley.png", plot = valley_plot, limitsize = FALSE, units = "in", bg = NULL,)

```
### Maps for individual watersheds
```{r depth-measurement-locations-map}
#map of watershed 3 with depth to bedrock
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
hill <- rast(hillshade_out)

#dem
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
#crop to rectangular area
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#writeRaster(crop1, "1mdemw3_cropped.tif")

#watershed boundary
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)

#w3 network- thing I need to change
#read in shapefile of stream converted in ARC
vect_stream_path <- "./AGU24posterAnalysis/vector_stream/vector_stream.shp"
#stream as a vector
vect_stream <- vect(vect_stream_path)
plot(vect_stream)
#crop to watershed boundary
w3_stream_crop <- crop(vect_stream, w3_outline)
plot(w3_stream_crop)
#or i could use old classification

#point locations- snapped points from above chunk
w3_stic_locs_snap <- "w3_stic_locs_snap.shp"

w3_stic_locs_r <- vect(w3_stic_locs_snap) %>% 
  left_join(pks_w3, by = "ID")



w3_stic_locs_r <- vect(w3_stic_locs_snap)
#writeVector(w3_stic_locs_r, "./seismic_map_exports/w3_stic_locs_snap.shp")


w3_net <- vect("./carrieZigZag/w3_network.shp")
#writeVector(w3_net, "./seismic_map_exports/network.shp")

plot(w3_net)


ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))

#test <- 
  ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "darkslategray3", lwd = 2) +
  geom_sf(data = w3_stic_locs_r, colour = "midnightblue", size = 2) +
  #geom_sf(data = dd, aes(color = (depth)), pch = 19, size = 3) +
  scale_color_gradient(low = "black", high = "white")+
  #geom_sf(data = w3_pour, colour = "black") +
   scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))


```
```{r fallsBrook-map}
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

#define the rectangular area that will be shown on final map
ybounds <- c(4868850,4869650)
xbounds <- c(279350, 280450)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
points(lcc)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./fb_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
fb_crop <- "./fb_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
fb_net <- vect("./carrieZigZag/FB_network.shp")

###pour point to define where the watershed boundary is
#manually type coords from windows computer
fb_pour_coords <- data.frame("easting" = 280400,
                             "northing" = 4869120)
#convert to SpatVector object
fb_pour <- vect(fb_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
fb_pour_filename <- "./fb_dems/fb_pour.shp"
#writeVector(fb_pour, fb_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
b_crop <- "./fb_dems/1mdem_crop.tif"

fb_breached <- "./fb_dems/1mdem_breach.tif"
# wbt_breach_depressions_least_cost(
#   dem = fb_crop,
#   output = fb_breached,
#   dist = 1,
#   fill = TRUE)

fb_filled <- "./fb_dems/1mdem_fill.tif"
# wbt_fill_depressions_wang_and_liu(
#   dem = fb_breached,
#   output = fb_filled
# )
#calculate flow accumulation and direction
fb_flowacc <- "./fb_dems/1mdem_fb_flowacc.tif"
# wbt_d8_flow_accumulation(input = fb_filled,
#                          output = fb_flowacc)
# plot(rast(fb_flowacc))
fb_d8pt <- "./fb_dems/1mdem_fb_d8pt.tif"
# wbt_d8_pointer(dem = fb_filled,
#                output = fb_d8pt)
# plot(rast(fb_d8pt))


#delineate streams
fb_streams <- "./fb_dems/fb_streams.tif"
# wbt_extract_streams(flow_accum = fb_flowacc,
#                     output = fb_streams,
#                     threshold = 8000)
# plot(rast(fb_streams))
# points(lcc)
#snap pour point to streams
fb_pour_snap <- "./fb_dems/fb_pour_snap.shp"
# wbt_jenson_snap_pour_points(pour_pts = fb_pour_filename,
#                             streams = fb_streams,
#                             output = fb_pour_snap,
#                             snap_dist = 10)
fb_pour_snap_read <- vect("./fb_dems/fb_pour_snap.shp")

fb_shed <- "./fb_dems/fb_shed.tif"
# wbt_watershed(d8_pntr = fb_d8pt,
#               pour_pts = fb_pour_snap,
#               output = fb_shed)

#convert raster of watershed area to vector for final mapping
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)

#get sensor locations from STIC data, format
locs <- data_23 %>% 
  filter(wshed == "FB") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))

#assign destination for hillshade calculation
hillshade_out <- "./fb_dems/1mdem_hillshade.tif"
# wbt_hillshade(
#   dem = fb_crop,
#   output = hillshade_out,
# )
hill <- rast(hillshade_out)

#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.
#fb_map <- 
  ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = fb_outline, fill = NA, color = "#397367", alpha = 0.3, lwd = 2) +
  geom_sf(data = fb_net, colour = "darkslategray3", lwd = 2) +
    geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 2) +
  #geom_sf(data = fb_pour, colour = "black", pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen", limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))

```
```{r zigZag-map}
#map for ZZ
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
#plot(m1)

#get sensor locations from STIC data, format
locs <- data_23 %>% 
  filter(wshed == "ZZ") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))
#plot(lcc)
#define the rectangular area that will be shown on final map
ybounds <- c(4866400,4867500)
xbounds <- c(277200, 277650)
#plot(m1, xlim = xbounds, ylim = ybounds)
#points(lcc)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#plot(crop1)
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./zz_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
zz_crop <- "./zz_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
zz_net <- vect("./carrieZigZag/zigzag_streams.shp")
#plot(zz_net)

###pour point to define where the watershed boundary is
#manually type coords from windows computer


zz_pour_coords <- data.frame("easting" = 277280.45,
                             "northing" = 4867436.45)
#convert to SpatVector object
zz_pour <- vect(zz_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
zz_pour_filename <- "./zz_dems/zz_pour.shp"
#writeVector(zz_pour, zz_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
zz_crop <- "./zz_dems/1mdem_crop.tif"

zz_breached <- "./zz_dems/1mdem_breach.tif"


zz_filled <- "./zz_dems/1mdem_fill.tif"
# wbt_fill_depressions_wang_and_liu(
#   dem = zz_breached,
#   output = zz_filled
# )
#calculate flow accumulation and direction
zz_flowacc <- "./zz_dems/1mdem_zz_flowacc.tif"
# wbt_d8_flow_accumulation(input = zz_filled,
#                          output = zz_flowacc)
#plot(rast(zz_flowacc))
zz_d8pt <- "./zz_dems/1mdem_zz_d8pt.tif"
# wbt_d8_pointer(dem = zz_filled,
#                output = zz_d8pt)
#plot(rast(zz_d8pt))


#delineate streams
zz_streams <- "./zz_dems/zz_streams.tif"
# wbt_extract_streams(flow_accum = zz_flowacc,
#                     output = zz_streams,
#                     threshold = 8000)
# plot(rast(zz_streams))
# points(lcc)
#snap pour point to streams
zz_pour_snap <- "./zz_dems/zz_pour_snap.shp"
# wbt_jenson_snap_pour_points(pour_pts = zz_pour_filename,
#                             streams = zz_streams,
#                             output = zz_pour_snap,
#                             snap_dist = 10)
zz_pour_snap_read <- vect("./zz_dems/zz_pour_snap.shp")
# plot(rast(zz_streams), 
#      xlim = c(280200, 280410),
#      ylim = c(4869300, 4869000))
# points(zz_pour_snap_read, pch = 1)

zz_shed <- "./zz_dems/zz_shed.tif"
# wbt_watershed(d8_pntr = zz_d8pt,
#               pour_pts = zz_pour_snap,
#               output = zz_shed)

#plot(rast(zz_shed))
#convert raster of watershed area to vector for final mapping
zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)
#plot(zz_outline)



#assign destination for hillshade calculation
hillshade_out <- "./zz_dems/1mdem_hillshade.tif"
# wbt_hillshade(
#   dem = zz_crop,
#   output = hillshade_out,
# )
hill <- rast(hillshade_out)
#plot(hill)
#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.
#zz_map <- 
  ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2) +
  geom_sf(data = zz_net, colour = "darkslategray3", lwd = 2) +
    geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 2) +
  #geom_sf(data = zz_pour, colour = "black", pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen", limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))


```

```{r combine-maps}
#combine and save all maps
all_map <- w3_map + fb_map + zz_map +
  theme(rect = element_rect(fill = "transparent", color = NA))

all_map
#24 in by 16
# ggsave("all.png", plot = all_map, scale = 2, limitsize = FALSE, height = 16,
#        width = 24, units = "in", bg = NULL,)
#no matter what I do, I cannot get these formatted how I want them

ggsave("w3.png", plot = w3_map, scale = 2, limitsize = FALSE, height = 11,
       width = 9, units = "in", bg = NULL,)

ggsave("fb.png", plot = fb_map, scale = 2, limitsize = FALSE, height = 11,
       width = 9, units = "in", bg = NULL,)

ggsave("zz.png", plot = zz_map, scale = 2, limitsize = FALSE, height = 11,
       width = 9, units = "in", bg = NULL,)

```

## Topo variables
```{r topo-distributions}
#get the areas of each watershed, and the whole valley
#delineate shed and stream using 3, 5, 10m dem, then plot distribution of elevation and TWI

dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 3)
#plot(m10)

#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./HB/1m hydro enforced DEM/dem3m.tif", overwrite = TRUE)
m3_path <- "./HB/1m hydro enforced DEM/dem3m.tif"

fb_elev <- m10 %>% 
  crop(fb_outline) %>% 
  mask(fb_outline)
zz_elev <- m10 %>% 
  crop(zz_outline) %>% 
  mask(zz_outline)
w3_elev <- m10 %>% 
  crop(w3_outline) %>% 
  mask(w3_outline)

plot(fb_elev)
summary(zz_elev$dem1m)
summary(fb_elev$dem1m)
summary(w3_elev$dem1m)

hist(zz_elev$dem1m)
hist(fb_elev$dem1m)
hist(w3_elev$dem1m)

zz_dist <- as_tibble(data.frame("elev" = zz_elev$dem1m, "site" = "zz"))
fb_dist <- as_tibble(data.frame("elev" = fb_elev$dem1m, "site" = "fb"))
w3_dist <- as_tibble(data.frame("elev" = w3_elev$dem1m, "site" = "w3"))

all_dist <- rbind(zz_dist, fb_dist, w3_dist)

ggplot(all_dist)+
  geom_boxplot(aes(x=site, y=dem1m))

#now calculate and display the distribution of TWI for each watershed as boxplot
#calculate TWI for the whole valley, then just extract like above
#fill and breach 3m dem
breach_output <- "./HB/1m hydro enforced DEM/dem3m_breach.tif"
wbt_breach_depressions_least_cost(
  dem = m3_path,
  output = breach_output,
  dist = 3,
  fill = TRUE)

fill_output <- "./HB/1m hydro enforced DEM/dem3m_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
slope_output <- "./HB/1m hydro enforced DEM/dem3m_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
twi_output <- "./HB/1m hydro enforced DEM/dem3m_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

#crop and mask TWI by each of my watersheds
twi_valley <- rast(twi_output)
fb_twi <- twi_valley %>% 
  crop(fb_outline) %>% 
  mask(fb_outline)
zz_twi <- twi_valley %>% 
  crop(zz_outline) %>% 
  mask(zz_outline)
w3_twi <- twi_valley %>% 
  crop(w3_outline) %>% 
  mask(w3_outline)

#convert raster values to tibble
zz_tib <- as_tibble(data.frame("twi" = zz_twi$dem3m_twi, "site" = "zz"))
fb_tib <- as_tibble(data.frame("twi" = fb_twi$dem3m_twi, "site" = "fb"))
w3_tib <- as_tibble(data.frame("twi" = w3_twi$dem3m_twi, "site" = "w3"))

all_twi <- rbind(zz_tib, fb_tib, w3_tib)

ggplot(all_twi)+
  geom_boxplot(aes(x=site, y=dem3m_twi))

ggplot(all_twi)+
  geom_histogram(aes(y=after_stat(density),
                     fill=site, 
                     x=(dem3m_twi)), 
                 alpha=0.6, position="identity")
ggplot(all_twi)+
  geom_histogram(aes(y=after_stat(density),
                     fill=site, 
                     x=log(dem3m_twi)))

#create plots of distribution
```

```{r calculate-curvature}
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)

w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)
#FB
fb_shed <- "./fb_dems/fb_shed.tif"
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)
#ZZ
zz_shed <- "./zz_dems/zz_shed.tif"
zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)

p_load(spatialEco)


fb_curve <- m1 %>% 
  crop(fb_outline) %>% 
  mask(fb_outline) %>% 
  spatialEco::curvature()
zz_curve <- m1 %>% 
  crop(zz_outline) %>% 
  mask(zz_outline)%>% 
  spatialEco::curvature()
w3_curve <- m1 %>% 
  crop(w3_outline) %>% 
  mask(w3_outline)%>% 
  spatialEco::curvature()
curve1 <- spatialEco::curvature(m1)
curve10 <- spatialEco::curvature(m10)

#extract point values of curvature
w3_locs <- read_csv("./STIC_uaa/w32.csv")%>% 
  select(ID, POINT_X, POINT_Y)
fb_locs <- read_csv("./STIC_uaa/fb2.csv") %>% 
  select(ID, POINT_X, POINT_Y)
zz_locs <- read_csv("./STIC_uaa/zz1.csv")%>% 
  select(ID, POINT_X, POINT_Y)

w3_curve <- extract(curve10, w3_locs[,2:3]) %>% select(-ID) %>% 
  bind_cols(w3_locs)
fb_curve <- extract(curve10, fb_locs[,2:3]) %>% select(-ID) %>% 
  bind_cols(fb_locs)
zz_curve <- extract(curve10, zz_locs[,2:3]) %>% select(-ID) %>% 
  bind_cols(zz_locs)
```
UNFINISHED
```{r accumulated-drainage-area}

#.csv files with extracted 1m drainage area values, but also contain snapped coords in UTM
w3_uaa_1m <- read_csv("./STIC_uaa/w32.csv") %>% 
  select(ID, RASTERVALU)
fb_uaa_1m <- read_csv("./STIC_uaa/fb2.csv")%>% 
  select(ID, RASTERVALU)
zz_uaa_1m <- read_csv("./STIC_uaa/zz2.csv")%>% 
  select(ID, RASTERVALU)
#set routes for all 3 watersheds
#routes_w3 <- 
downstream_ones <- 
  read_csv("w3_flowrouting.csv") %>% 
  filter(sensor %in% W3_IDs & drains_from %in% W3_IDs) %>% 
    select(sensor, drains_to) %>% 
    filter(drains_to != 100) %>% 
    rename("ID" = sensor) %>% 
    left_join(w3_uaa_1m, by = "ID") %>% 
    rename("upstream_uaa" = RASTERVALU,
           "upstream_ID" = ID,
           "ID" = drains_to) %>% 
    left_join(w3_uaa_1m, by = "ID") %>% 
    rename("downstream_ID" = ID,
           "downstream_uaa" = RASTERVALU) %>% 
    drop_na() %>% 
    mutate(diff = downstream_uaa - upstream_uaa)%>% 
  select(upstream_ID, diff) %>% 
  rename("ID" = upstream_ID,
  "acc_uaa" = diff)
  
channel_init <- read_csv("w3_flowrouting.csv") %>% 
      filter(sensor %in% W3_IDs) %>% 
    filter(drains_from == 0) %>% 
    rename("ID" = sensor) %>% 
    left_join(w3_uaa_1m, by = "ID") %>% 
  select(ID, RASTERVALU) %>% 
  rename("acc_uaa" = RASTERVALU)
  

routes_w3 <- 
  bind_rows(channel_init, downstream_ones) %>% 
  arrange(desc(acc_uaa)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

##FB
#downstream_ones <- 
  read_csv("fb_flowrouting.csv") %>% 
  filter(up %in% FB_IDs & down %in% FB_IDs) %>% 
    filter(up != 100) %>% 
    rename("ID" = up) %>% 
    left_join(fb_uaa_1m, by = "ID") %>% 
    rename("upstream_uaa" = RASTERVALU,
           "upstream_ID" = ID,
           "ID" = down) %>% 
    left_join(fb_uaa_1m, by = "ID") %>% 
    rename("downstream_ID" = ID,
           "downstream_uaa" = RASTERVALU) %>% 
    drop_na() %>% 
    mutate(diff = downstream_uaa - upstream_uaa)%>% 
  select(upstream_ID, diff) %>% 
  rename("ID" = upstream_ID,
  "acc_uaa" = diff)
  
#channel_init <- 
  read_csv("fb_flowrouting.csv") %>% View()
      filter(down %in% FB_IDs) %>% View()
    filter(up == 0) %>% 
    rename("ID" = down) %>% 
    left_join(fb_uaa_1m, by = "ID") %>% 
  select(ID, RASTERVALU) %>% 
  rename("acc_uaa" = RASTERVALU)
  

routes_fb <- 
  bind_rows(channel_init, downstream_ones) %>% 
  arrange(desc(acc_uaa)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)


##ZZ
downstream_ones <- 
  read_csv("zz_flowrouting.csv") %>% 
  filter(sensor %in% ZZ_IDs & drains_from %in% ZZ_IDs) %>% 
    select(sensor, drains_to) %>% 
    filter(drains_to != 100) %>% 
    rename("ID" = sensor) %>% 
    left_join(zz_uaa_1m, by = "ID") %>% 
    rename("upstream_uaa" = RASTERVALU,
           "upstream_ID" = ID,
           "ID" = drains_to) %>% 
    left_join(zz_uaa_1m, by = "ID") %>% 
    rename("downstream_ID" = ID,
           "downstream_uaa" = RASTERVALU) %>% 
    drop_na() %>% 
    mutate(diff = downstream_uaa - upstream_uaa)%>% 
  select(upstream_ID, diff) %>% 
  rename("ID" = upstream_ID,
  "acc_uaa" = diff)
  
channel_init <- read_csv("zz_flowrouting.csv") %>% 
      filter(sensor %in% ZZ_IDs) %>% 
    filter(drains_from == 0) %>% 
    rename("ID" = sensor) %>% 
    left_join(zz_uaa_1m, by = "ID") %>% 
  select(ID, RASTERVALU) %>% 
  rename("acc_uaa" = RASTERVALU)
  

routes_zz <- 
  bind_rows(channel_init, downstream_ones) %>% 
  arrange(desc(acc_uaa)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

```


# Figure 3: Distribution of flow permanence and duration of flow
## Flow Permanence
```{r prepare-discharge}
#from TestingFrameworks script

#read in discharge from W3-- input to Carrie's model, discharge in L/s
#q <- read_csv("https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-hbr.1.17&entityid=efc477b3ef1bb3dd8b9355c9115cd849")
#write.csv(q, "HB_5minQ.csv")
q <- read_csv("HB_5minQ.csv")

#input discharge needs to be in mm/day?
#reference to understand difference between daily mean and instantaneous streamflow:
#https://hydrofunctions.readthedocs.io/en/master/notebooks/DailyMean_vs_Instant.html

#creating minute column, used to filter out higher temporal resolution measurements for plotting
data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

#find the range of dates that I need discharge for
start <- min(data_23$datetime)
stop <- max(data_23$datetime)

#filtering discharge down to the range of dates
q_23 <- q %>% 
  filter(DATETIME > start & DATETIME < stop) %>% 
  #convert to mm/day.
  #converting instantaneous streamflow to mm/day by taking measurement, and scaling   it up as if that was the discharge for the whole day. It is not, it is just at that   moment, but should fix any units/order of magnitude issues
  mutate("Q_mm_day" = Discharge_ls * 0.001 * 86400 / 420000 * 1000) 
q_23$mins <- minute(q_23$DATETIME)

#removing times that are not coincident with STIC observations
q_23_f <- filter(q_23, mins %in% c(0, 30))

ggplot(q_23_f, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#also read in provisional 2024 data
q_24 <- read_csv("w3_discharge_24.csv")

#find the range of dates that I need discharge for
start24 <- ymd_hms("2024-05-15 00:00:00 UTC")
stop24 <- max(data_24$datetime)

#filtering discharge down to the range of dates
q_24 <- q_24 %>% 
  mutate(datetime = mdy_hm(datetime)) %>% 
  filter(datetime > start24 & datetime < stop24)  
q_24$mins <- minute(q_24$datetime)

#removing times that are not coincident with STIC observations
q_24_f <- filter(q_24, mins %in% c(0, 30))

ggplot(q_24_f, aes(x  = datetime, y = Q_mmperday))+
  geom_line()+
  labs(title = "Discharge from W3, May to July 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()
```
```{r flow-permanence-W3}
#just summer 2023
data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

pks_23 <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>% 
  #left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()
#just summer 2024
pks_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number) %>% 
  #left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()

both <- inner_join(pks_23, pks_24, by = "ID")
ggplot()+
  geom_point(data = both,aes(x = pk.x, y = pk.y))+
  labs(title = "Change in pk from '23 to '24, W3",
       x = "2023",
       y = "2024")+
  geom_abline(slope=1, intercept=0)+
  theme_classic()

#both summers
#just rbind summers 23 and 24
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_w3 <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "W3")
```
```{r flow-permanence-FB}
#both summers
#just rbind summers 23 and 24
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_fb <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "FB")
```
```{r flow-permanence-ZZ}
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_zz <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "ZZ")
```
```{r combine-boxplot}
rbind(pks_w3, pks_fb, pks_zz) %>% 
  ggplot(aes(x = wshed, y = pk))+
  geom_boxplot()+
  geom_jitter(width = 0.1, alpha = 0.5)+
  theme_classic()+
  labs(title = "Distributions of Proportion of Time Flowing",
       x = "Watershed",
       y = "Proportion of Time Flowing")

rbind(pks_w3, pks_fb, pks_zz) %>% 
  ggplot(aes(x = pk, fill = wshed)) +
geom_histogram(binwidth = 0.1)+
    theme_classic()+
  labs(title = "Distributions of Proportion of Time Flowing",
       x = "Proportion of Time Flowing",
       y = "Density")+
  scale_fill_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     
                     name = "Watershed")+
  facet_wrap(~wshed)

```


#Preparing inputs
```{r prepare-inputs}
#create input that only uses sensors that were deployed during both deployments
#need to make a list of sensors deployed in both campaigns for each watershed
#W3
w3_deployed24 <- unique(filter(data_24, wshed == "W3")$number)
w3_deployed23 <- unique(filter(data_23, wshed == "W3")$ID)
W3_IDs <- intersect(w3_deployed24, w3_deployed23)
#FB
fb_deployed24 <- unique(filter(data_24, wshed == "FB")$number)
fb_deployed23 <- unique(filter(data_23, wshed == "FB")$ID)
FB_IDs <- intersect(fb_deployed24, fb_deployed23)
#ZZ
zz_deployed24 <- unique(filter(data_24, wshed == "ZZ")$number)
zz_deployed23 <- unique(filter(data_23, wshed == "ZZ")$ID)
ZZ_IDs <- intersect(zz_deployed24, zz_deployed23)


#detach("package:fitdistrplus")
#detach("package:MASS")


bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)%>% 
  filter(mins %in% c(0, 30))


bind23 <- data_23 %>% 
  filter(ID %in% intersect(bind24$ID, data_23$ID)) %>% 
  select(datetime, ID, wshed, binary, mins) %>% 
  filter(mins %in% c(0, 30))

#filter by each watershed, then recombine at the end
input_w3 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3") %>% 
  filter(ID %in% W3_IDs)
input_fb <- rbind(bind23, bind24) %>%
  filter(wshed == "FB") %>% 
  filter(ID %in% FB_IDs)
input_zz <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ") %>% 
  filter(ID %in% ZZ_IDs)

input_all <- rbind(input_w3, input_fb, input_zz)
write_csv(input_w3, "calc_support_inputs_w3.csv")
write_csv(input_fb, "calc_support_inputs_fb.csv")
write_csv(input_zz, "calc_support_inputs_zz.csv")

      
```

## Average Duration of Flow
Problem with calculating duration of Flow- the maximum value is dependent on how long the sensor was continuously deployed
```{r dof-w3}
#successfully calculating the average duration of flow for an event
bind23 <- data_23 %>% 
  select(datetime, ID, wshed, binary, mins)
bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)

#old dof calculation
# dof_w3 <- rbind(bind23, bind24) %>%
#   filter(wshed == "W3", mins %in% c(0, 30)) %>%
#   select(datetime, ID, binary) %>%
#   mutate(group = data.table::rleid(binary)) %>%
#   group_by(group, ID) %>%
#   summarise(state = first(binary),
#             timeperiod = dplyr::last(datetime) - dplyr::first(datetime)) %>%
#   mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>%
#   group_by(ID) %>%
#   filter(state == 1) %>%
#   summarise(avg_days_flowing = mean(timeperiod))
# 
# #new dof calculation, where it is a proportion
durations <- bind24 %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, ID, binary) %>%
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>%
    summarise(duration = max(datetime) - min(datetime)) %>%
  mutate(duration = as.numeric(duration, units = "days"))


w3_dof_24 <- bind24 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
  group_by(group, ID) %>%
  summarise(state = first(binary), 
            timeperiod = dplyr::last(datetime) - dplyr::first(datetime)
            ) %>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
  group_by(ID) %>% 
  filter(state == 1) %>% 
    left_join(durations, by = "ID") %>% 
    mutate(proportion = timeperiod/duration) 

durations <- bind23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>% 
    summarise(duration = max(datetime) - min(datetime)) %>% 
  mutate(duration = as.numeric(duration, units = "days"))

w3_dof_23 <-  bind23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
  group_by(group, ID) %>%
  summarise(state = first(binary), 
            timeperiod = dplyr::last(datetime) - dplyr::first(datetime)
            ) %>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
  group_by(ID) %>% 
  filter(state == 1) %>% 
    left_join(durations, by = "ID") %>% 
    mutate(proportion = timeperiod/duration) 
#calculate for 24 and 23 separately, then combine for summarization purposes
dof_w3 <- rbind(w3_dof_23, w3_dof_24) %>% 
  mutate(oneyear = proportion * 365) %>% 
  summarise(avg_days_flowing = mean(oneyear),
            sd_days_flowing = sd(oneyear))%>% 
  mutate(wshed = "W3")

#new sequence, different than old one
# routes_w3 <- dof_w3 %>%
#     filter(ID %in% W3_IDs) %>% 
#   arrange(desc(avg_days_flowing)) %>% 
#   mutate(down = lag(ID)) %>% 
#   rename("up" = ID) %>% drop_na() %>% 
#   select(up, down)
#compare this sequence to the old one?
# routes_w3 <- dof_w3 %>%
#     filter(ID %in% W3_IDs) %>% 
#   arrange(desc(avg_days_flowing)) %>% 
#   mutate(down = lag(ID)) %>% 
#   rename("up" = ID) %>% drop_na() %>% 
#   select(up, down)
```
```{r dof-fb}
#old dof calculation
# dof_fb <- rbind(bind23, bind24) %>% 
#   filter(wshed == "FB", mins %in% c(0, 30)) %>% 
#   select(datetime, ID, binary) %>% 
#   mutate(group = data.table::rleid(binary)) %>%
#   group_by(group, ID) %>%
#   summarise(state = first(binary), 
#             timeperiod = dplyr::last(datetime) - dplyr::first(datetime)) %>% 
#   mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
#   group_by(ID) %>% 
#   filter(state == 1) %>% 
#   summarise(avg_days_flowing = mean(timeperiod)) %>% 
#   mutate(wshed = "FB") %>% 
#   #remove outlier left out over winter
#   filter(ID != 32)


durations <- bind24 %>%
  filter(wshed == "FB", mins %in% c(0, 30)) %>%
  select(datetime, ID, binary) %>%
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>%
    summarise(duration = max(datetime) - min(datetime)) %>%
  mutate(duration = as.numeric(duration, units = "days"))

fb_dof_24 <- bind24 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
  group_by(group, ID) %>%
  summarise(state = first(binary), 
            timeperiod = dplyr::last(datetime) - dplyr::first(datetime)
            ) %>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
  group_by(ID) %>% 
  filter(state == 1) %>% 
    left_join(durations, by = "ID") %>% 
    mutate(proportion = timeperiod/duration) 

durations <- bind23 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>% 
    summarise(duration = max(datetime) - min(datetime)) %>% 
  mutate(duration = as.numeric(duration, units = "days"))

fb_dof_23 <-  bind23 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
  group_by(group, ID) %>%
  summarise(state = first(binary), 
            timeperiod = dplyr::last(datetime) - dplyr::first(datetime)
            ) %>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
  group_by(ID) %>% 
  filter(state == 1) %>% 
    left_join(durations, by = "ID") %>% 
    mutate(proportion = timeperiod/duration) 
#calculate for 24 and 23 separately, then combine for summarization purposes
dof_fb <- rbind(fb_dof_23, fb_dof_24) %>% 
  mutate(oneyear = proportion * 365) %>% 
  summarise(avg_days_flowing = mean(oneyear),
            sd_days_flowing = sd(oneyear))%>%  
  mutate(wshed = "FB")
```
```{r dof-zz}
#old dof calculation
# dof_zz <- rbind(bind23, bind24) %>% 
#   filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
#   select(datetime, ID, binary) %>% 
#   mutate(group = data.table::rleid(binary)) %>%
#   group_by(group, ID) %>%
#   summarise(state = first(binary), 
#             timeperiod = dplyr::last(datetime) - dplyr::first(datetime)) %>% 
#   mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
#   group_by(ID) %>% 
#   filter(state == 1) %>% 
#   summarise(avg_days_flowing = mean(timeperiod)) %>% 
#   mutate(wshed = "ZZ")

durations <- bind24 %>%
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>%
  select(datetime, ID, binary) %>%
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>%
    summarise(duration = max(datetime) - min(datetime)) %>%
  mutate(duration = as.numeric(duration, units = "days"))

zz_dof_24 <- bind24 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
  group_by(group, ID) %>%
  summarise(state = first(binary), 
            timeperiod = dplyr::last(datetime) - dplyr::first(datetime)
            ) %>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
  group_by(ID) %>% 
  filter(state == 1) %>% 
    left_join(durations, by = "ID") %>% 
    mutate(proportion = timeperiod/duration) 

durations <- bind23 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>% 
    summarise(duration = max(datetime) - min(datetime)) %>% 
  mutate(duration = as.numeric(duration, units = "days"))

zz_dof_23 <-  bind23 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(group = data.table::rleid(binary)) %>%
  group_by(group, ID) %>%
  summarise(state = first(binary), 
            timeperiod = dplyr::last(datetime) - dplyr::first(datetime)
            ) %>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days")) %>% 
  group_by(ID) %>% 
  filter(state == 1) %>% 
    left_join(durations, by = "ID") %>% 
    mutate(proportion = timeperiod/duration) 
#calculate for 24 and 23 separately, then combine for summarization purposes
dof_zz <- rbind(zz_dof_23, zz_dof_24) %>% 
  mutate(oneyear = proportion * 365) %>% 
  summarise(avg_days_flowing = mean(oneyear),
            sd_days_flowing = sd(oneyear))%>% 
  mutate(wshed = "ZZ")
```
```{r combine-boxplot}
rbind(dof_w3, dof_fb, dof_zz) %>% 
  ggplot(aes(x = wshed, y = (avg_days_flowing)))+
  geom_boxplot()+
  geom_jitter(width = 0.1, alpha = 0.5)+
  theme_classic()+
  labs(title = "Distributions of Duration of Continuous Flow",
       x = "Watershed",
       y = "Duration of Continuous Flow (days)")

rbind(dof_w3, dof_fb, dof_zz) %>% 
  ggplot(aes(x = avg_days_flowing, y = sd_days_flowing, color = wshed))+
  geom_point()+
  theme_classic()+
  labs(title = "Distributions of Duration of Continuous Flow",
       x = "Mean days flowing",
       y = "sd days flowing")

rbind(dof_w3, dof_fb, dof_zz) %>% 
  ggplot(aes(x = avg_days_flowing, fill = wshed)) +
geom_histogram(binwidth = 25)+
    theme_classic()+
  labs(title = "Distributions of Duration of Continuous Flow",
       x = "Mean days of continuous flow",
       y = "Density")+
  scale_fill_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     
                     name = "Watershed")#+
  facet_wrap(~wshed)
```
I did this, but I think we have decided to remove it
## Instead of duration of flow- number of transitions
```{r figuring-out-nt}
bind23 <- data_23 %>% 
  select(datetime, ID, wshed, binary, mins)
bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)

nt_w3 <- rbind(bind23, bind24) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
      ungroup() %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  filter(days == 48) %>% 
  filter(transition %in% c(-1, 1)) %>% #filtered to when there is a drying transition on a complete day
  summarise(transitions = sum(abs(transition))) %>% #absolute value so that wetting and drying are the same
      ungroup() %>% 
      group_by(ID) %>%
  summarise(avg_t_day = mean(transitions))%>% 
  mutate(wshed = "W3")

nt_fb <- rbind(bind23, bind24) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
      ungroup() %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  filter(days == 48) %>% 
  filter(transition %in% c(-1, 1)) %>% #filtered to when there is a drying transition on a complete day
  summarise(transitions = sum(abs(transition))) %>% #absolute value so that wetting and drying are the same
      ungroup() %>% 
      group_by(ID) %>%
  summarise(avg_t_day = mean(transitions))%>% 
  mutate(wshed = "FB")





bind24 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30), ID == 1) %>% 
     filter(datetime > ymd_hms("2024-06-17 18:00:00") & datetime < ymd_hms("2024-06-20 18:00:00")) %>% 
   select(datetime, ID, binary) %>% 
     ggplot(aes(x = datetime, y = binary))+
     geom_tile()
```
```{r nt-w3}
duration23 <- bind23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  ungroup() %>% 
  filter(days == 48) %>% 
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>% 
    summarise(duration = max(datetime) - min(datetime)) %>% 
  mutate(duration = as.numeric(duration, units = "days"))

w3_nt_23 <- bind23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
      ungroup() %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  filter(days == 48) %>% 
  filter(transition %in% c(-1, 1)) %>% 
    #filter(ID == 22) %>% #filtered to when there is a drying transition on a complete day
  summarise(transitions = sum(abs(transition))) %>% 
        filter(transitions <= 4) %>% #%>% #absolute value so that wetting and drying are the same
      group_by(ID) %>%
    summarise(total = sum(transitions)) %>% 
    left_join(duration23, by = "ID") %>% 
        group_by(ID) %>%
    summarise(proportion = total/duration)

duration24 <- bind24 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  ungroup() %>% 
  filter(days == 48) %>% 
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>% 
    summarise(duration = max(datetime) - min(datetime)) %>% 
  mutate(duration = as.numeric(duration, units = "days"))

w3_nt_24 <- bind24 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
      ungroup() %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  filter(days == 48) %>% 
  filter(transition %in% c(-1, 1)) %>% 
    #filter(ID == 22) %>% #filtered to when there is a drying transition on a complete day
  summarise(transitions = sum(abs(transition))) %>% 
        filter(transitions <= 4) %>% #%>% #absolute value so that wetting and drying are the same
      group_by(ID) %>%
    summarise(total = sum(transitions)) %>% 
    left_join(duration24, by = "ID") %>% 
        group_by(ID) %>%
    summarise(proportion = total/duration)
#total number of transitions divided by duration in days of observation


#calculate for 24 and 23 separately, then combine
nt_w3 <- rbind(w3_nt_23, w3_nt_24) %>% 
  group_by(ID) %>% 
  summarise(avg_nt = mean(proportion))%>% 
  mutate(wshed = "W3")
```
```{r nt-fb}
duration23 <- bind23 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  ungroup() %>% 
  filter(days == 48) %>% 
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>% 
    summarise(duration = max(datetime) - min(datetime)) %>% 
  mutate(duration = as.numeric(duration, units = "days"))

fb_nt_23 <- bind23 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
      ungroup() %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  filter(days == 48) %>% 
  filter(transition %in% c(-1, 1)) %>% 
    #filter(ID == 22) %>% #filtered to when there is a drying transition on a complete day
  summarise(transitions = sum(abs(transition))) %>% 
        filter(transitions <= 4) %>% #%>% #absolute value so that wetting and drying are the same
      group_by(ID) %>%
    summarise(total = sum(transitions)) %>% 
    left_join(duration23, by = "ID") %>% 
        group_by(ID) %>%
    summarise(proportion = total/duration)

duration24 <- bind24 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  ungroup() %>% 
  filter(days == 48) %>% 
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>% 
    summarise(duration = max(datetime) - min(datetime)) %>% 
  mutate(duration = as.numeric(duration, units = "days"))

fb_nt_24 <- bind24 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
      ungroup() %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  filter(days == 48) %>% 
  filter(transition %in% c(-1, 1)) %>% 
    #filter(ID == 22) %>% #filtered to when there is a drying transition on a complete day
  summarise(transitions = sum(abs(transition))) %>% 
        filter(transitions <= 4) %>% #%>% #absolute value so that wetting and drying are the same
      group_by(ID) %>%
    summarise(total = sum(transitions)) %>% 
    left_join(duration24, by = "ID") %>% 
        group_by(ID) %>%
    summarise(proportion = total/duration)
#total number of transitions divided by duration in days of observation


#calculate for 24 and 23 separately, then combine
nt_fb <- rbind(fb_nt_23, fb_nt_24) %>% 
  group_by(ID) %>% 
  summarise(avg_nt = mean(proportion))%>% 
  mutate(wshed = "FB")
```
```{r nt-zz}
duration23 <- bind23 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  ungroup() %>% 
  filter(days == 48) %>% 
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>% 
    summarise(duration = max(datetime) - min(datetime)) %>% 
  mutate(duration = as.numeric(duration, units = "days"))

zz_nt_23 <- bind23 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
      ungroup() %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  filter(days == 48) %>% 
  filter(transition %in% c(-1, 1)) %>% 
    #filter(ID == 22) %>% #filtered to when there is a drying transition on a complete day
  summarise(transitions = sum(abs(transition))) %>% 
        filter(transitions <= 4) %>% #%>% #absolute value so that wetting and drying are the same
      group_by(ID) %>%
    summarise(total = sum(transitions)) %>% 
    left_join(duration23, by = "ID") %>% 
        group_by(ID) %>%
    summarise(proportion = total/duration)

duration24 <- bind24 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  ungroup() %>% 
  filter(days == 48) %>% 
  mutate(group = data.table::rleid(binary)) %>%
    group_by(ID) %>% 
    summarise(duration = max(datetime) - min(datetime)) %>% 
  mutate(duration = as.numeric(duration, units = "days"))

zz_nt_24 <- bind24 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  mutate(year = year(datetime),
         month = month(datetime),
         day = day(datetime)) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
      ungroup() %>% 
      group_by(ID, year, month, day) %>%
  mutate(days = n()) %>% 
  filter(days == 48) %>% 
  filter(transition %in% c(-1, 1)) %>% 
    #filter(ID == 22) %>% #filtered to when there is a drying transition on a complete day
  summarise(transitions = sum(abs(transition))) %>% 
        filter(transitions <= 4) %>% #%>% #absolute value so that wetting and drying are the same
      group_by(ID) %>%
    summarise(total = sum(transitions)) %>% 
    left_join(duration24, by = "ID") %>% 
        group_by(ID) %>%
    summarise(proportion = total/duration)
#total number of transitions divided by duration in days of observation


#calculate for 24 and 23 separately, then combine
nt_zz <- rbind(zz_nt_23, zz_nt_24) %>% 
  group_by(ID) %>% 
  summarise(avg_nt = mean(proportion))%>% 
  mutate(wshed = "ZZ")
```
```{r boxplot}
rbind(nt_w3, nt_fb, nt_zz) %>% 
  ggplot(aes(x = wshed, y = (avg_nt)))+
  geom_boxplot()+
  geom_jitter(width = 0.1, alpha = 0.5)+
  theme_classic()+
  labs(title = "Distributions of the Number of Transitions",
       x = "Watershed",
       y = "NT normalized by duration of observation")
```

Add tpi and twi
##tpi and twi
```{r locs-from-arc}
#get snapped sensor locations from extracted values from arc
w3_locs <- read_csv("./STIC_uaa/w32.csv")%>% 
  select(ID, POINT_X, POINT_Y)
fb_locs <- read_csv("./STIC_uaa/fb2.csv") %>% 
  select(ID, POINT_X, POINT_Y)
zz_locs <- read_csv("./STIC_uaa/zz1.csv")%>% 
  select(ID, POINT_X, POINT_Y)
```
```{r tpi-W3}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(w3_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

w3_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble()

w3_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble()

w3_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble()

```
```{r tpi-fb}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(fb_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

fb_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble()

fb_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble()

fb_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble()
```
```{r tpi-zz}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(zz_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

zz_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble()

zz_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble()

zz_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble()
```


#Figure 4: How often does the network wet up, dry down, etc?
##Figure out how to test this behavior
```{r}
test <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  filter(ID == 1)%>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"
```
First thing to try that I have tried before: determine all unique combinations of wet and dry sensors
```{r}
#find all unique combinations of wet and dry sensors
hush2 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  pivot_wider(names_from = ID, values_from = binary)

hush3 <- hush2[1:5,-1]
hush3
hush3[-1, ]

hush3[-5,] - hush3[-1, ]

# xy.list <- as.list(as.data.frame(t(hush2[,-1])))
# length(xy.list)
# length(unique(data$datetime))
# #unique combinations of flow
# length(unique(xy.list))

#perhaps subset it so that I am working with one pair of consecutive sensors at a time
hush3[-5,15:17] - hush3[-1, 15:17]
hush3[,15:17]

#how often does 17 activate before 16?
#filter to every time that 17 transitions to flowing- value of -1
l <- 20 #length of time
hush3 <- hush2[1:l,-1] #get the number of time steps equal to l, get rid of time id col

hush3[-l,16:17] - hush3[-1, 16:17] #find the change with lagged cols, output does not have last delta

#find every time that 17 is -1
hush4 <- hush3[-l,16:17] - hush3[-1, 16:17]
colnames(hush4) <- c("up", "down")

hush4

#how often does the upstream sensor go to -1 after the downstream sensor?
require(data.table)
data.table:::duplist(hush4[, 3:5]) 

hush4 %>% 
mutate(group = data.table::rleid(down)) %>%
  group_by(group) %>%
  summarise(state = first(down),
            rows = length(group)) #%>% 
  mutate(timeperiod = as.numeric(timeperiod, units = "days"))

#how can I calculate seqeunce of activation?
data <- data.frame(
  Var1 = c(1, 0, 1, 1, 0),
  Var2 = c(0, 1, 1, 0, 1),
  Var3 = c(1, 1, 1, 1, 0)
)

target_sequence <- c(1, 0, 1)

matches <- apply(data, 1, function(row) all(row == target_sequence))
count <- sum(matches)
proportion <- count / nrow(data)

cat("Number of matches:", count, "\n")
cat("Proportion of matches:", proportion, "\n")



#one idea- fit a logistic regression of every sensor to every other sensor. If a sensor is always flowing when another is, it would be a good predictor
mod <- hush2[,16:17]
colnames(mod) <- c("up", "down")

mod2 <- hush2
colnames(mod2) <- as.character(paste0("r_",colnames(hush2)))

model <- glm(r_1 ~.,family=binomial(link='logit'),data=mod2)
summary(model)
```
```{r}
#how often does x activate after y?

data <- data.frame(
  Var1 = c(1, 0, 1, 1, 0, 1, 1),
  Var2 = c(0, 1, 1, 0, 1, 0, 0),
  Var3 = c(1, 1, 1, 1, 0, 1, 1)
)
target_sequence <- matrix(
  c(1, 0, 1,
    0, 1, 1),
  nrow = 2, byrow = TRUE
)

# Flatten the target sequence for comparison
target_vector <- as.vector(t(target_sequence))

# Use rollapply to create sliding windows
windows <- rollapply(data, width = nrow(target_sequence), by.column = FALSE, FUN = function(x) as.vector(t(x)))

# Check for matches
matches <- apply(windows, 1, function(window) all(window == target_vector))

# Get indices of matches
match_indices <- which(matches)

cat("Matching sequence found at indices:", match_indices, "\n")


#now test on my data
mod3 <- select(mod2,r_15, r_16, r_17)
target_sequence <- matrix(
  c(
    0, 0, 1,
    0, 1, 1,
    1, 1, 1),
  nrow = 3, byrow = TRUE
)
target_vector <- as.vector(t(target_sequence))

# Use rollapply to create sliding windows
windows <- rollapply(mod3, width = nrow(target_sequence), by.column = FALSE, FUN = function(x) as.vector(t(x)))

# Check for matches
matches <- apply(windows, 1, function(window) all(window == target_vector))

# Get indices of matches
match_indices <- which(matches)
length(match_indices)

###another attempt

# Example data
mod3 <- select(mod2,r_15, r_16, r_17)


# Define window size
window_size <- 6

# Create sliding windows
windows <- rollapply(
  mod3,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Get most common sequence
most_common_sequence <- names(sorted_counts)[1]
cat("Most common sequence:", most_common_sequence, "\n")
cat("Frequency:", sorted_counts[1], "\n")

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
print(sequence_df)

```

```{r}
#find the most common sequence of activation during the rising limb, or times when slope of discharge is positive
mod3 <- drop_na(mod2[,-1])
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))
#make it so that there cannot be a sequence without change

# Define window size
window_size <- 4

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
View(sequence_df)
```

```{r}
#convert this result into a plot
#number of panes dictated by window size
window_size

number_in_list <- 4
#left join the sequence to the locations of the sensors and plot
head(sequence_df)

status <- data.frame("ID" = as.numeric(rep(substr(colnames(no_dupes), 3,4),window_size)),
                     "status" = unlist(strsplit(sequence_df$Sequence[number_in_list], split = "")),
                     "timestep" = c(rep(1, 31), rep(2, 31), rep(3, 31), rep(4, 31)))

#get sensor locations from STIC data, format
locs <- data_23 %>% 
  filter(wshed == "W3") %>% 
  select(ID, lat, long) %>% 
  unique() %>% 
  left_join(status, by = "ID")
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))

ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = w3_outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = w3_net, colour = "darkslategray3") +
    geom_sf(data = lcc, aes(colour = status), pch = 19) +
  facet_wrap(~timestep)+
   scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))
```

```{r w3-map}
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
plot(m1)

#get sensor locations from STIC data, format
locs <- data_23 %>% 
  filter(wshed == "W3") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))
plot(lcc)
#define the rectangular area that will be shown on final map
ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
plot(m1, xlim = xbounds, ylim = ybounds)
points(lcc)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
plot(crop1)
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
writeRaster(crop1, "./w3_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
w3_crop <- "./w3_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
w3_net <- vect("./carrieZigZag/w3_network.shp")
plot(w3_net)

###pour point to define where the watershed boundary is
#manually type coords from windows computer
 
 
w3_pour_coords <- data.frame("easting" = 281537.46,
                             "northing" = 4870424.50)
#convert to SpatVector object
w3_pour <- vect(w3_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
w3_pour_filename <- "./w3_dems/w3_pour.shp"
writeVector(w3_pour, w3_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
w3_crop <- "./w3_dems/1mdem_crop.tif"

w3_breached <- "./w3_dems/1mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = w3_crop,
  output = w3_breached,
  dist = 1,
  fill = TRUE)

w3_filled <- "./w3_dems/1mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = w3_breached,
  output = w3_filled
)
#calculate flow accumulation and direction
w3_flowacc <- "./w3_dems/1mdem_w3_flowacc.tif"
wbt_d8_flow_accumulation(input = w3_filled,
                         output = w3_flowacc)
plot(rast(w3_flowacc))
w3_d8pt <- "./w3_dems/1mdem_w3_d8pt.tif"
wbt_d8_pointer(dem = w3_filled,
               output = w3_d8pt)
plot(rast(w3_d8pt))


#delineate streams
w3_streams <- "./w3_dems/w3_streams.tif"
wbt_extract_streams(flow_accum = w3_flowacc,
                    output = w3_streams,
                    threshold = 8000)
plot(rast(w3_streams))
points(lcc)
#snap pour point to streams
w3_pour_snap <- "./w3_dems/w3_pour_snap.shp"
wbt_jenson_snap_pour_points(pour_pts = w3_pour_filename,
                            streams = w3_streams,
                            output = w3_pour_snap,
                            snap_dist = 10)
w3_pour_snap_read <- vect("./w3_dems/w3_pour_snap.shp")
plot(rast(w3_streams), 
     xlim = c(281400, 282000),
     ylim = c(4870400, 4870800))
points(w3_pour_snap_read, pch = 1)

w3_shed <- "./w3_dems/w3_shed.tif"
wbt_watershed(d8_pntr = w3_d8pt,
              pour_pts = w3_pour_snap,
              output = w3_shed)

plot(rast(w3_shed))
#convert raster of watershed area to vector for final mapping
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)
plot(w3_outline)



#assign destination for hillshade calculation
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
wbt_hillshade(
  dem = w3_crop,
  output = hillshade_out,
)
hill <- rast(hillshade_out)
plot(hill)

#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.





```

Whenever there is a change in state, does it follow the wetting up, drying down, or flow permanence hypothesis?

```{r}
#testing pairs of sensors
#filter to two upstream/downstream sensors
mod3 <- drop_na(mod2[,-1]) %>% select("r_22", "r_23")
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))
#make it so that there cannot be a sequence without change

# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001", "0011", "0111", "1110", "1100")

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations

```
Use above chunk to write a function to take an upstream and downstream sensor, and determine what proportion of the times it changes state follow the systematic behavior I am looking for.

```{r define-function}
#testing pairs of sensors
#filter to two upstream/downstream sensors
calc_support <- function(up, down, input){
#up <- "r_10"
#down <- "r_6"
incase <- data.frame(up = c(0,0), down = c(0,0))
colnames(incase) <- c(up, down)
mod3 <- input %>% select(up,down, -datetime)
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all)) 
#make it so that there cannot be a sequence without change

#error when both locations flowed the whole time; 
if(length(no_dupes[,1]) == 1) no_dupes <- rbind(no_dupes, no_dupes, incase)

# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
#View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001", "0011", "0111", "1110", "1100", "1101", "0100")
#subset from the 16 possibilities, except it is fewer because
#0000, 1111, 0101, 1010 are removed...

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down, points, total)
#error handling- in situation where both points flowed 100% of the time
if(length(no_dupes[,1]) == 1) output$points <- NA
if(length(no_dupes[,1]) == 1) output$total <- NA

return(output)
}

calc_support("r_10", "r_6", input)
```

```{r define-function-no-pandering}
#testing pairs of sensors
#filter to two upstream/downstream sensors
calc_support <- function(up, down, input){
#up <- "r_10"
#down <- "r_6"

mod3 <- input %>% select(up,down, -datetime)
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))
#make it so that there cannot be a sequence without change

#error when both locations flowed the whole time; 

# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
#View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")
#ones that I included before: c("0011", "1110", "1100")
#subset from the 16 possibilities, except it is fewer because
#0000, 1111, 0101, 1010 are removed...

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down, points, total)

return(output)
}

calc_support("r_10", "r_6", input)
```

```{r function-final-version}
calc_support <- function(up, down, input){
#up <- routes$up[i] #input
#down <- routes$down[i]
incase <- data.frame(up = c(0,0), down = c(0,0))
colnames(incase) <- c(up, down)
mod3 <- input %>% select(up,down, -datetime)
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))
#all flowing all the time?
check <- nrow(no_dupes)
#make it so that there cannot be a sequence without change

#error when both locations flowed the whole time; 
if(check == 1) no_dupes <- rbind(no_dupes, no_dupes, incase)

# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
#View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")
#subset from the 16 possibilities, except it is fewer because
#0000, 1111, 0101, 1010 are removed...

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down, points, total)
#error handling- in situation where both points flowed 100% of the time
if(check == 1) output$points <- NA
if(check == 1) output$total <- NA

return(output)
}

calc_support("r_28", "r_6", input)
```

#finalized versions of analysis functions
Finished on 2/21/25, fixed calc_support and the nest of for loops so that they work smoothly with any set of routes, inputs, and timesteps.

Structure of functions:
- fantastic_four (routes, shed("W3", "FB", "ZZ")) *remove sensors not in ZZ_IDs, etc. from routes
|- for loop, 4 different test timescales
  |- calculate hierarchy (routes, input, timestep (as duration))
    |- for loop, number of pairs dictated by routes
      |- iterate groups (up, down, input, timestep (as duration))
        |- for loop, number of groups of uninterrupted dates, dictated by timestep
          |- calc_support (up, down, input) 
        
```{r refined-functions}
#chunk to trouble shoot non-functioning instances to figure out why they are not working
#instance not working:
#run_scenario(routes_w3_pk, "pk", "W3", "daily")

routes <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

input <- rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
        filter(wshed == "W3", mins %in% c(0, 30)) %>%
      #filter(wshed == "W3", hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary) #%>% 
  #filter(datetime < stop24 & datetime > start24)


calc_support <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  output$total <- NA
  output$points <- NA
  return(output)
} 
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")

output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")


sub <- filter(sequence_df, Sequence %in% supports)
output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(output)}
}

#test function
#calc_support("r_23", "r_6", input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

#iterate_groups("r_23", "r_6", input, hours(1))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- iterate_groups(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down) %>%
    summarise(total = sum(total),
              points = sum(points)) %>% 
    mutate(prop = points/total)
  return(final_output)
}

#calculate_hierarchy(routes, input, minutes(30))
#calculate_hierarchy(routes, input, days(1))


#make a function to loop through the four possible timesteps, and combine the output just for ease of applying this many different variations
#fantastic 4 determines the input on its own
fantastic_four <- function(routes, shed){
  theFour <- c("30mins", "hourly", "4hr", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "hourly"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(1)
  } 
  else if(timescale == "4hr"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(0,4,8,12,16,20,24), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(4)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}



#final conclusion- do need to break it up into groups! gets rid of 1-4 erroneous pairs, which will matter more at lower temporal resolutions
#for each scenario create the routes, inputs, and specify the timestep
```


#Running analysis using refined functions
Iterating through each pair of sensors, using the upstream sensor as the id, and the downstream sensor as the secondary or dependent. Seeing how often pairs of sensors wet up or dry down relative to each other.
```{r relative-position}
#set routes for all 3 watersheds
routes_w3 <- read_csv("w3_flowrouting.csv") %>% 
  filter(sensor %in% W3_IDs, drains_from %in% W3_IDs) %>% 
  rename("up" = drains_from, "down" = sensor) %>% 
  select(up, down) %>% 
  filter(down != 100, up != 0)
  
routes_fb <- read_csv("fb_flowrouting.csv") %>% 
  filter(up %in% FB_IDs) %>% 
  filter(down %in% FB_IDs) %>% 
  drop_na()
routes_zz <- read_csv("zz_flowrouting.csv") %>% 
  filter(up %in% ZZ_IDs) %>% 
  filter(down %in% ZZ_IDs) %>% 
  drop_na()

#run calc_support for all sheds and timesteps for relative position
all_position <- rbind(fantastic_four(routes_w3, "W3"),
                      fantastic_four(routes_fb, "FB"),
                      fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Relative Position")
write_csv(all_position, "./hierarchy_analysis_results/relativePosition.csv")
#test <- read_csv("./hierarchy_analysis_results/relativePosition.csv")
```
```{r flow-permanence}
#make list that make pairs of sites based on local persistency
routes_w3 <- pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- pks_fb %>%
    filter(ID %in% FB_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- pks_zz %>%
    filter(ID %in% ZZ_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

all_pk <- rbind(fantastic_four(routes_w3, "W3"),
                fantastic_four(routes_fb, "FB"),
                fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Flow Permanence")

write_csv(all_pk, "./hierarchy_analysis_results/flowPermanence.csv")
```
```{r number-of-transitions}
#make list that make pairs of sites based on local persistency
routes_w3 <- nt_w3 %>%
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(avg_nt)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- nt_fb %>% 
    filter(ID %in% FB_IDs) %>% 
  arrange(desc(avg_nt)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- nt_zz %>% 
    filter(ID %in% ZZ_IDs) %>% 
  arrange(desc(avg_nt)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

all_nt <- rbind(fantastic_four(routes_w3, "W3"),
                 fantastic_four(routes_fb, "FB"),
                 fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Duration of Flow")

write_csv(all_dof, "./hierarchy_analysis_results/durationOfFlow.csv")
```
```{r drainage-area}
#I was not confident in my R determined values, so I extracted drainage area in Arc. Reading in results

#.csv files with extracted 1m drainage area values, but also contain snapped coords in UTM
fb_uaa_1m <- read_csv("./STIC_uaa/fb2.csv")
zz_uaa_1m <- read_csv("./STIC_uaa/zz2.csv")
w3_uaa_1m <- read_csv("./STIC_uaa/w32.csv")

routes_w3 <- w3_uaa_1m %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_uaa_1m %>%
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_zz <- zz_uaa_1m %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_uaa <- rbind(fantastic_four(routes_w3, "W3"),
                 fantastic_four(routes_fb, "FB"),
                 fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Drainage Area")

write_csv(all_uaa, "./hierarchy_analysis_results/drainageArea.csv")
```

```{r topographic-position-index}
# fb_uaa_1m <- read_csv("./STIC_uaa/fb2.csv")
# zz_uaa_1m <- read_csv("./STIC_uaa/zz2.csv")
# w3_uaa_1m <- read_csv("./STIC_uaa/w32.csv")

routes_w3 <- w3_tpi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_tpi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_tpi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_tpi <- rbind(fantastic_four(routes_w3, "W3"),
                 fantastic_four(routes_fb, "FB"),
                 fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Topographic Position Index")

write_csv(all_tpi, "./hierarchy_analysis_results/tpi.csv")
```
```{r topographic-wetness-index}
routes_w3 <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_twi <- rbind(fantastic_four(routes_w3, "W3"),
                 fantastic_four(routes_fb, "FB"),
                 fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Topographic Wetness Index")

write_csv(all_twi, "./hierarchy_analysis_results/twi.csv")
```
```{r slope}
routes_w3 <- w3_slope %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(slope) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_slope %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange((slope)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_slope %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange((slope)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_slope <- rbind(fantastic_four(routes_w3, "W3"),
                 fantastic_four(routes_fb, "FB"),
                 fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Slope")

write_csv(all_slope, "./hierarchy_analysis_results/slope.csv")
```

```{r curvature}
routes_w3 <- w3_curve %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(dem1m) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_curve %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange((dem1m)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_curve %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange((dem1m)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_curvature <- rbind(fantastic_four(routes_w3, "W3"),
                 fantastic_four(routes_fb, "FB"),
                 fantastic_four(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Curvature")
```


add accumulated drainage area
##Compile all of my results so far 2
Consolidate the gains!!
```{r consolidating-progress}
#prop stands for proportion or percentage of transitions that support the hypothesis
all_position <- read_csv("./hierarchy_analysis_results/relativePosition.csv")
all_pk <- read_csv("./hierarchy_analysis_results/flowPermanence.csv")
all_uaa <- read_csv("./hierarchy_analysis_results/drainageArea.csv")

sults_so_far_avgday <- sults_so_far
sults_so_far <- rbind(all_position,
                      all_pk,
                      #all_curvature,
                      #all_acc_uaa
                      #all_nt,
                      all_uaa
                      #all_slope,
                      #all_twi
                      ) #%>% 
  mutate(timescale = fct_relevel(timescale,
                              c("30mins", "hourly", "4hr", "daily")),
         hierarchy = as.factor(str_trim(as.character(hierarchy))),
         
         hierarchy = fct_relevel(hierarchy, 
                                 c(
                                "Flow Permanence",
                                "curvature")))
                                #"Relative Position",
                                #"Drainage Area",
                                #"Slope",
                                #"Topographic Wetness Index")))#,
                                #"Random")))
sults_so_far %>% 
  ggplot(aes(x = shed, y = prop, color = hierarchy)) +
  geom_boxplot()+
  theme_classic()+
    geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Watershed",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  facet_grid(~timescale)


#instead of boxplots, show the ecdf and one-sided violin plot
sults_so_far %>% 
  filter(timescale == "30mins", shed == "W3") %>% 
  ggplot(aes(x = shed, y = prop, color = hierarchy)) +
  geom_violin()+
  theme_classic()+
  geom_point(position = position_jitterdodge(jitter.width = 0.1), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Watershed",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F", "grey"),
                     name = "Organizing Scheme")+
  facet_wrap(~timescale)

#violin plot of just 30 min timescale
sults_so_far %>% 
  filter(timescale %in% c("30mins", "hourly")) %>% 
  ggplot(aes(x = timescale, y = prop, color = hierarchy)) +
  geom_violin()+
  theme_classic()+
  geom_point(position = position_jitterdodge(jitter.width = 0.1), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Timescale",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy (daily timescale?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     name = "Organizing Scheme")+
    facet_grid(rows = "shed")

#one sided violin plot
# p_load(devtools)
# devtools::install_github("psyteachr/introdataviz")
# library(introdataviz)

# sults_so_far %>% 
#   ggplot(aes(x = shed, y = prop, color = type)) +
#   geom_split_violin()+
#   theme_classic()+
#     #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
#   geom_hline(yintercept = 0.5, lty = 2)+
#   labs(x = "Watershed",
#        y = "Proportion",
#        title = "How often do adjacent sensors follow a hierarchy?")+
#   scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
#                      labels = c("Duration of Flow",
#                                 "Flow Permanence",
#                                 "Relative Position",
#                                 "Drainage Area"),
#                      name = "Organizing Scheme")+
#   facet_wrap(~type)

#plot ecdf instead of boxplots or violin plots
plot(ecdf(w3_hourly$prop))
plot(ecdf(alldat$prop))
ggplot()+
  stat_ecdf(data = w3_pairs, aes(prop))+
  stat_ecdf(data = w3_new, aes(prop), color = "blue")

sults_so_far %>% 
  #filter(timescale == "30mins") %>% 
  ggplot(aes(prop, color = hierarchy)) +
  stat_ecdf(geom = "line")+
  theme_classic()+
    #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
    geom_vline(xintercept = 0.5, lty = 2)+

  labs(x = "Prop",
       y = "Percentage of values less than or equal",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     name = "Organizing Scheme")+
  facet_wrap(timescale~shed)#+
  lims(x = c(0.5,1),
       y = c(0.5, 1))

#just plain distributions might look better?
sults_so_far %>% 
  filter(timescale == "30mins") %>% 
  ggplot(aes(prop, color = hierarchy)) +
geom_density(alpha = 0.5)+
    theme_classic()+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+

    #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  labs(x = "Prop",
       y = "Density",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     name = "Organizing Scheme")+
  facet_wrap(~shed)#+

#try plain histogram
sults_so_far %>% 
  filter(timescale == "30mins") %>% 
  ggplot(aes(prop, fill = hierarchy)) +
geom_histogram(binwidth = 0.1)+
    theme_classic()+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+

    #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  labs(x = "Prop",
       y = "Density",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_fill_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     
                     name = "Organizing Scheme")+
  facet_grid(hierarchy~shed)

sults_so_far %>% 
  filter(timescale %in% c("30mins", "hourly")) %>% 
  ggplot(aes(x = prop, fill = shed, y = after_stat(density))) +
geom_density(alpha = 0.5)+
      geom_vline(xintercept = 0.5, lty = 2)+
    theme_classic()+
  labs(title = "Distributions of Proportion of time Hiearchies Followed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = c("#397367", "#FFA400", "#93C2F1"),
                     
                     name = "Watershed")+
  facet_wrap(~hierarchy)

#FINAL DENSITY PLOTS

#######THIS ONE
sults_so_far %>% 
  filter(timescale != "4hr") %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = shed, color = shed), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 3))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Hiearchies Followed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = c("#397367", "#FFA400", "#93C2F1"),
                     
                     name = "Watershed")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1"),
                     
                     name = "Watershed")+
  facet_grid(timescale~hierarchy)

sults_so_far %>% 
  filter(timescale %in% c("30mins", "hourly")) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = shed, color = shed), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 3))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Hiearchies Followed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = c("#397367", "#FFA400", "#93C2F1"),
                     
                     name = "Watershed")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1"),
                     
                     name = "Watershed")+
  facet_grid(timescale~hierarchy)

shed_colors <- c("#397367", "#FFD166", "#7E6B8F")

#plots for committee meeting
sults_so_far %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = shed, color = shed), alpha = 0.5)+
    #geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 3))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Hiearchies Followed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = shed_colors,
                     
                     name = "Watershed")+
  scale_color_manual(values = shed_colors,
                     
                     name = "Watershed")+
  facet_grid(timescale~hierarchy)
```
##looping by drainage area UNFINISHED
Lost in a file hell
```{r create-3m-network}
streams <- "./HB/1m hydro enforced DEM/dem3m_streams.tif"
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

wbt_extract_streams(flow_accum = flowacc_output,
                    output = streams,
                    threshold = 500)
plot(rast(streams))
#w3 zoom
ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
plot(rast(streams), xlim = xbounds, ylim = ybounds)
#points(lcc)
#zz () zoom
ybounds <- c(4866400,4867500)
xbounds <- c(277200, 277650)

#use these networks, snap points to them before extracting values
plot(vect("./HB/hbstream/ZZ_subcatchment_flowlines.shp"))
plot(rast(streams), add = TRUE, alpha = 0.5)

plot(vect("./HB/hbstream/FB_subcatchment_flowlines.shp"))
plot(rast(streams), add = TRUE, alpha = 0.5)

plot(vect("./HB/hbstream/hb42_master_startend.shp"))
plot(rast(streams), add = TRUE, alpha = 0.5)


```

```{extract-drainage-area-FB}
#calculate for FB
locs <- data_24 %>% 
  filter(wshed == "FB") %>% 
  select(number, lat, long) %>% 
  rename("ID" = number) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
fb_lcc <- terra::project(locs_shape, crs(rast(flowacc_output)))

plot(vect("./HB/hbstream/FB_subcatchment_flowlines.shp"))
old_stream <- vect("./HB/hbstream/FB_subcatchment_flowlines.shp")
old_stream_rast <- terra::rasterize(old_stream, rast(flowacc_output))
plot(old_stream_rast)
writeRaster(old_stream_rast, "actualFB_shed.tif", overwrite = TRUE)


#snap points to streamlines, use 5m FB streams
#read in raster and re-save
test <- raster::raster("HB/hbstream/fb_5m.tif")
writeRaster(test, "actualFB_shed.tif", overwrite = TRUE)
#path to save fb stic locations
fb_lcc_path <- "./mapsForStoryboardFiles/fb_stics.shp"
writeVector(fb_lcc, fb_lcc_path, overwrite = TRUE)
#path for snapped stic locations
fb_lcc_path_snap <- "./mapsForStoryboardFiles/fb_stics_snapped.shp"
fb_streams <- "actualFB_shed.tif"
wbt_jenson_snap_pour_points(pour_pts = fb_lcc_path,
                            streams = fb_streams,
                            output = fb_lcc_path_snap,
                            snap_dist = 20)
#before extracting, I should snap points to stream network based on this resolution perhaps... ugh

#something about this doesn't seem right...
FB_uaa_ex <- extract(rast(flowacc_output), vect(fb_lcc_path_snap))
ybounds <- c(4868850,4869650)
xbounds <- c(279350, 280450)
plot(rast(fb_streams), xlim = xbounds, ylim = ybounds)

plot(vect(fb_lcc_path_snap), add = TRUE)

```
```{r extracting-drainage-ZZ}
#calculate for FB
locs <- data_24 %>% 
  filter(wshed == "ZZ") %>% 
  select(number, lat, long) %>% 
  rename("ID" = number) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
zz_lcc <- terra::project(locs_shape, crs(rast(flowacc_output)))

#snap points to streamlines, use 5m FB streams
#read in raster and re-save
test <- raster::raster("HB/hbstream/zz_5m.tif")
writeRaster(test, "actualZZ_shed.tif", overwrite = TRUE)
#path to save fb stic locations
zz_lcc_path <- "./mapsForStoryboardFiles/zz_stics.shp"
writeVector(zz_lcc, zz_lcc_path, overwrite = TRUE)
#path for snapped stic locations
zz_lcc_path_snap <- "./mapsForStoryboardFiles/zz_stics_snapped.shp"
zz_streams <- "actualZZ_shed.tif"
wbt_jenson_snap_pour_points(pour_pts = zz_lcc_path,
                            streams = zz_streams,
                            output = zz_lcc_path_snap,
                            snap_dist = 20)
#before extracting, I should snap points to stream network based on this resolution perhaps... ugh

ZZ_uaa_ex <- extract(rast(flowacc_output), vect(zz_lcc_path_snap))
plot(rast(zz_streams))

plot(vect(zz_lcc_path_snap), add = TRUE)
```




#run any number of random hierarchies, compare to the actual
Structure of functions:
- big_iteration(iterations, shed)
  |- for loop- loops through the 4 hierarchies
    |- random_iteration(iterations, shed, hierarchy)
     |- for loop- loops through number of iterations, finding the right combos, routes, and inputs for each watershed and hierarchy. Relies on the the already calculated results from the hierarchy analysis, the form of sults_so_far
```{r define-functions}

#function where you specify number of iterations, shed, and the hierarchy. Returns the p-value for each times step for each iteration compared to the actual hiearchy run. Relies on sults_so_far being in its current form.
random_iteration <- function(iterations, shed, hierarchy){
  if(shed == "W3") {
    combos <- W3_IDs
    
    sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
    sample <- as_tibble(sample_perms, .name_repair = "minimal")
    #colnames(sample) <- paste0("I_",seq(1, iterations, 1))
    colnames(sample) <- seq(1, iterations, 1)
  } else if(shed == "FB"){
    combos <- FB_IDs
    
    sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
    sample <- as_tibble(sample_perms, .name_repair = "minimal")
    #colnames(sample) <- paste0("I_",seq(1, iterations, 1))
    colnames(sample) <- seq(1, iterations, 1)
    
  } else if(shed == "ZZ"){
    combos <- ZZ_IDs
    
    sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
    sample <- as_tibble(sample_perms, .name_repair = "minimal")
    #colnames(sample) <- paste0("I_",seq(1, iterations, 1))
    colnames(sample) <- seq(1, iterations, 1)
  }
  
  test_against <- sults_so_far %>% 
    filter(shed == shed, hierarchy == hierarchy) %>% 
    group_by(timescale)
  
  for(f in 1:iterations){
    routes <- sample %>% select(paste0(f))
    colnames(routes) <- "up"
    routes <- routes %>% mutate(down = lag(up))%>% 
      drop_na()
  
    out <- fantastic_four(routes, shed)
    #speculative, added later to compare p-values
    out <- out %>% group_by(timescale) %>% 
      summarise(pval = wilcox.test(test_against$prop, prop, 
                                   alternative = "greater", 
                                   exact = FALSE, conf.int = TRUE)$p.value) %>% 
      ungroup() %>% 
    mutate(comparison = hierarchy,
           shed = shed,
           timescale = timescale)

    out$iteration <- f
  
    if(f == 1) alldat <- out
    if(f > 1) alldat <- rbind(alldat, out)
}
return(alldat)
}

#testing random_iteration function, seems to work as intended
#random_iteration(2, "W3", "Drainage Area")
#random_iteration(2, "FB", "Drainage Area")
random_iteration(2, "ZZ", "Relative Position")

#make a function to iterate through each hierarchy, make formatted output for plotting
big_iteration <- function(iterations, shed){
  hierarchies <- c("Relative Position", "Drainage Area", "Flow Permanence", "Duration of Flow")
  
  for(v in 1:4){
    hierarchy <- hierarchies[v]
    out <- random_iteration(iterations, shed, hierarchy)
    out$hierarchy <- hierarchies[v]
    
    if(v == 1) fanfar <- out
    if(v > 1) fanfar <- rbind(fanfar, out)
  }

  fanfar$shed <- shed
  return(fanfar)
}

#testing big_iteration
big_iteration(2, "ZZ")

#running big iteration on the data
#beware running again- takes forever
# random_w3 <- big_iteration(100, "W3")
# write_csv(random_w3, "./hierarchy_analysis_results/randomW3.csv")
# random_fb <- big_iteration(100, "FB")
# write_csv(random_fb, "./hierarchy_analysis_results/randomFB.csv")
# random_zz <- big_iteration(100, "ZZ")
# write_csv(random_zz, "./hierarchy_analysis_results/randomZZ.csv")
random_w3 <- read_csv("./hierarchy_analysis_results/randomW3.csv")
random_fb <- read_csv("./hierarchy_analysis_results/randomFB.csv")
random_zz <- read_csv("./hierarchy_analysis_results/randomZZ.csv")

```
```{r plot-random-results}
rbind(random_w3,
      random_fb,
      random_zz) %>% 
  mutate(timescale = fct_relevel(timescale,
                              c("30mins", "hourly", "4hr", "daily")),
         hierarchy = as.factor(str_trim(as.character(hierarchy))),
         hierarchy = fct_relevel(hierarchy, 
                                 c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"))) %>% 
  ggplot(aes(x = timescale, y = pval, color = hierarchy)) +
  geom_boxplot()+
  theme_classic()+
    geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.05, lty = 2)+
  labs(x = "Timescale",
       y = "P-value",
       title = "P-values from Wilcoxon test comparing median of random hierarchies to actual ones")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
    facet_grid(rows = "shed")



```
```{r testing-random-iteration}
set.seed(123)  # For reproducibility
iterations <- 1000

combos <- FB_uaa_ex %>% 
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up)

combos <- unique(combos$up)
sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
sample <- as_tibble(sample_perms, .name_repair = "minimal")
#colnames(sample) <- paste0("I_",seq(1, iterations, 1))
colnames(sample) <- seq(1, iterations, 1)
  
random_iteration(sample, 2, 123, "FB", "Drainage Area")
random_iteration(sample, 2, 123, "FB", "Relative Position")


```
```{r random-W3}
set.seed(124)  # For reproducibility
iterations <- 100

combos <- W3_uaa_ex %>% 
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) %>% 
  mutate(up = paste0("r_",up),
         down = paste0("r_",down)) %>% 
  select(up)

combos <- unique(combos$up)
sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
sample <- as_tibble(sample_perms, .name_repair = "minimal")
#colnames(sample) <- paste0("I_",seq(1, iterations, 1))
colnames(sample) <- seq(1, iterations, 1)


find_score <- function(routes, input, t){
  for(x in 1:length(routes$up)){
  
    out <- calc_support(routes$up[x], routes$down[x], input)
    if(x == 1) alldat <- out
    if(x > 1) alldat <- rbind(alldat, out)

  }
  alldat$prop <- alldat$points/alldat$total
  alldat$iteration <- t

  return(alldat)
}

for(t in 1:iterations){
  routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- routes %>% mutate(down = lag(up))%>% 
    drop_na()
    
  output <- find_score(routes, input, t)
  if(t == 1) alldat3 <- output
    if(t > 1) alldat3 <- rbind(alldat3, output)
}

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))
random_w3 <- alldat3
#alldat is seed 123
random_w3 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed), size = 3)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")
random_w3_2 <- alldat3

#alldat3 is seed 124
alldat3 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed))+
  theme_classic()

```

Try for a sequence of 3 times- also shows that this systematic wetting is very common
```{r window3}
calc_support <- function(up, down){
mod3 <- drop_na(mod2[,-1]) %>% select(all_of(up),all_of(down))
no_dupes <- mod3 %>%
  filter(row_number() == 1 | !apply(. == lag(.), 1, all))
#make it so that there cannot be a sequence without change

# Define window size
window_size <- 3

# Create sliding windows
windows <- rollapply(
  no_dupes,
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
colnames(sequence_df) <- c("Sequence", "Frequency")
#View(sequence_df)

total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:

supports <- c("000111", "110100", "011101", "110111", "000100")
#subset from the 16 possibilities, except it is fewer because
#0000, 1111, 0101, 1010 are removed...

sub <- filter(sequence_df, Sequence %in% supports)
points <- sum(sub$Frequency)

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down, points, total)
return(output)
}

calc_support("r_22", "r_23")
```



#Figure 6: relationship between flow permanence and topography, dof and topography
Conclusion- No clear relationship, this might not make the final cut.
```{r flow-permanence}
#drainage area
W3_topo_pk <- W3_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(pks_w3, by = "ID")
FB_topo_pk <- FB_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(pks_fb, by = "ID")
ZZ_topo_pk <- ZZ_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(pks_zz, by = "ID")

rbind(W3_topo_pk, FB_topo_pk, ZZ_topo_pk) %>% 
  ggplot(aes(x = (uaa), y = pk, color = wshed))+
  geom_point()+
    facet_wrap(~wshed, scales = "free")+
  theme_classic()+
  labs(title = "Flow permanence versus drainage area",
       x = "Drainage area",
       y = "Flow Permanence")
```
```{r dof}
rbind(dof_w3, dof_fb, dof_zz) %>% 
  ggplot(aes(x = wshed, y = (avg_days_flowing)))+
  geom_boxplot()+
  geom_jitter(width = 0.1, alpha = 0.5)+
  theme_classic()+
  labs(title = "Distributions of Average Duration of Flow",
       x = "Watershed",
       y = "Duration of Flow (days)")

#drainage area
W3_topo_dof <- W3_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(dof_w3, by = "ID")
FB_topo_dof <- FB_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(dof_fb, by = "ID")
ZZ_topo_dof <- ZZ_uaa_ex %>% 
  rename("uaa" = '10mdem_flowacc') %>%
  left_join(dof_zz, by = "ID")

rbind(W3_topo_dof, FB_topo_dof, ZZ_topo_dof) %>% 
  ggplot(aes(x = (uaa), y = avg_days_flowing, color = wshed))+
  facet_wrap(~wshed, scales = "free")+
  geom_point()+
  theme_classic()+
  labs(title = "dof versus drainage area",
       x = "Drainage area",
       y = "Duration of Flow (days)")
```

#test 1000 random hierarchies
Test every possible combination of pairs of sensors, see which one works the most hierarchically
```{r figuring-it-out}
#maybe best way to do this is make random lists of up and downstream sensors like 1000 times, then run through hierarchical testing algorithm
combos <- W3_uaa_ex %>% 
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) %>% 
  mutate(up = paste0("r_",up),
         down = paste0("r_",down)) %>% 
  select(up)

combos <- unique(combos$up)

install.packages("gtools")  # Install the package if not already installed
library(gtools)
set.seed(123)  # For reproducibility
iterations <- 10
sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
sample <- as_tibble(sample_perms, .name_repair = "minimal")
#colnames(sample) <- paste0("I_",seq(1, iterations, 1))
colnames(sample) <- seq(1, iterations, 1)


#write for loop to iterate through possibilities and determine which has the highest hierarchy score
# code from loop chunks
input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)
#now make loop to loop through all pairs of sensors
t = 2
routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- mini %>% mutate(down = lag(up))%>% 
    drop_na()
for(t in 1:iterations){
  routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- routes %>% mutate(down = lag(up))%>% 
    drop_na()
    
  output <- find_score(routes, input, t)
  if(t == 1) alldat2 <- output
    if(t > 1) alldat2 <- rbind(alldat2, output)
}

find_score <- function(routes, input, t){
  for(x in 1:length(routes$up)){
  
    out <- calc_support(routes$up[x], routes$down[x], input)
    if(x == 1) alldat <- out
    if(x > 1) alldat <- rbind(alldat, out)

  }
  alldat$prop <- alldat$points/alldat$total
  alldat$iteration <- t

  return(alldat)
}

alldat2 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point()+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed))

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))

```
Finding 1000 random combinations
```{r random-W3}
set.seed(124)  # For reproducibility
iterations <- 1000

combos <- W3_uaa_ex %>% 
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) %>% 
  mutate(up = paste0("r_",up),
         down = paste0("r_",down)) %>% 
  select(up)

combos <- unique(combos$up)
sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
sample <- as_tibble(sample_perms, .name_repair = "minimal")
#colnames(sample) <- paste0("I_",seq(1, iterations, 1))
colnames(sample) <- seq(1, iterations, 1)


find_score <- function(routes, input, t){
  for(x in 1:length(routes$up)){
  
    out <- calc_support(routes$up[x], routes$down[x], input)
    if(x == 1) alldat <- out
    if(x > 1) alldat <- rbind(alldat, out)

  }
  alldat$prop <- alldat$points/alldat$total
  alldat$iteration <- t

  return(alldat)
}

for(t in 1:iterations){
  routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- routes %>% mutate(down = lag(up))%>% 
    drop_na()
    
  output <- find_score(routes, input, t)
  if(t == 1) alldat3 <- output
    if(t > 1) alldat3 <- rbind(alldat3, output)
}

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))
random_w3 <- alldat3
#alldat is seed 123
random_w3 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed), size = 3)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")
random_w3_2 <- alldat3

#alldat3 is seed 124
alldat3 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed))+
  theme_classic()

```
```{r random-FB}
set.seed(123)  # For reproducibility
iterations <- 1000

combos <- FB_uaa_ex %>% 
  rename("up" = ID, "uaa" = 'dem3m_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) %>% 
  mutate(up = paste0("r_",up),
         down = paste0("r_",down)) %>% 
  select(up)

combos <- unique(combos$up)
sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
sample <- as_tibble(sample_perms, .name_repair = "minimal")
#colnames(sample) <- paste0("I_",seq(1, iterations, 1))
colnames(sample) <- seq(1, iterations, 1)


find_score <- function(routes, input, t){
  for(x in 1:length(routes$up)){
  
    out <- calc_support(routes$up[x], routes$down[x], input)
    if(x == 1) alldat <- out
    if(x > 1) alldat <- rbind(alldat, out)

  }
  alldat$prop <- alldat$points/alldat$total
  alldat$iteration <- t

  return(alldat)
}

for(t in 1:iterations){
  routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- routes %>% mutate(down = lag(up))%>% 
    drop_na()
    
  output <- find_score(routes, input, t)
  if(t == 1) alldat3 <- output
    if(t > 1) alldat3 <- rbind(alldat3, output)
}

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))
random_fb <- alldat3

alldat3 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed), size = 3)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")

```
```{r random-ZZ}
set.seed(123)  # For reproducibility
iterations <- 1000

combos <- ZZ_uaa_ex %>% 
  rename("up" = ID, "uaa" = '10mdem_flowacc') %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) %>% 
  mutate(up = paste0("r_",up),
         down = paste0("r_",down)) %>% 
  select(up)

combos <- unique(combos$up)
sample_perms <- replicate(iterations, sample(combos))  # Generate 10 random permutations
sample <- as_tibble(sample_perms, .name_repair = "minimal")
#colnames(sample) <- paste0("I_",seq(1, iterations, 1))
colnames(sample) <- seq(1, iterations, 1)


find_score <- function(routes, input, t){
  for(x in 1:length(routes$up)){
  
    out <- calc_support(routes$up[x], routes$down[x], input)
    if(x == 1) alldat <- out
    if(x > 1) alldat <- rbind(alldat, out)

  }
  alldat$prop <- alldat$points/alldat$total
  alldat$iteration <- t

  return(alldat)
}

for(t in 1:iterations){
  routes <- sample %>% select(paste0(t))
  colnames(routes) <- "up"
  routes <- routes %>% mutate(down = lag(up))%>% 
    drop_na()
    
  output <- find_score(routes, input, t)
  if(t == 1) alldat3 <- output
    if(t > 1) alldat3 <- rbind(alldat3, output)
}

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))
random_zz <- alldat3

alldat3 %>% group_by(iteration) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed), size = 3)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")

```
```{r combine-and-plot}
random_w3$shed = "W3"
random_fb$shed = "FB"
random_zz$shed = "ZZ"

reference <- sults_so_far %>% group_by(type, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop))

rbind(random_w3, random_fb, random_zz) %>% group_by(iteration, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd, shape = shed))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(color = type), size = 3)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  labs(title = "1000 random organizations",
       y = "Proportion sd",
       x = "Proportion mean")

rbind(random_w3, random_fb, random_zz) %>% group_by(iteration, shed) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd, shape = shed))+
  geom_point(alpha = 0.1)+
  geom_point(data = reference, aes(color = type), size = 3)+
  theme_classic()+
  facet_wrap(~shed)+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  labs(title = "Random organizations by watershed",
       y = "Proportion sd",
       x = "Proportion mean")
```

#iterate to find the best hierarchy
```{r iterating-smart-W3}
#pick a sensor randomly, then test every other sensor to see which has the best hierarchical behavior with that one. Proceed down the list without replacement
# combos <- W3_uaa_ex %>% 
#   rename("up" = ID, "uaa" = '10mdem_flowacc') %>%
#   arrange(desc(uaa)) %>% 
#   mutate(down = lag(up)) %>% 
#    drop_na() %>% 
#   select(up, down) %>% 
#   mutate(up = paste0("r_",up),
#          down = paste0("r_",down)) %>% 
#   select(up)

#combos <- unique(combos$up)

input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- unique(colnames(input[-1]))
#find the starting point

for(t in 1:length(combos)){
 #t <- 1 
begin <- combos[t]
options_initial <- combos[combos != begin]
options <- combos[combos != begin]

routes <- data.frame("up" = rep(begin, length(options_initial)),
                     "down" = options_initial)
for(x in 1:length(options_initial)){
#x <- 1
#calc_support("r_23", "r_6", input)

  for(i in 1:length(options)){
        out <- calc_support(routes$up[i], routes$down[i], input)
        if(i == 1) inner_out <- out
        if(i > 1) inner_out <- rbind(inner_out, out)
  }
  
  inner_out$prop <- inner_out$points/inner_out$total
  
        kep <- inner_out[inner_out$prop == max(inner_out$prop),]
        kep <- kep[1,]
  options <- options[options != kep$down[1]]
  routes <- data.frame("up" = rep(kep$down[1], length(options)),
                     "down" = options)
print(paste0("x = ",x))

  if(x == 1) big_keep <- kep
  if(x > 1) big_keep <- rbind(big_keep, kep)
}
big_keep$start <- begin
if(t == 1) biggest_keep <- big_keep
  if(t > 1) biggest_keep <- rbind(biggest_keep, big_keep)
}
#started at 1:55 pm, finished 1:59
smart_iterate_w3 <- biggest_keep
smart_iterate_w3 %>% group_by(start) #%>% 
  ggplot()+
  geom_density(data = smart_iterate_w3, 
               aes(x = prop, y = after_stat(density), color = start))+
  geom_density(data = filter(all_pk, timescale == "30mins" | shed == "W3"))+
  aes(x = prop, y = after_stat(density))#+

```
```{r iterating-smart-FB}
#pick a sensor randomly, then test every other sensor to see which has the best hierarchical behavior with that one. Proceed down the list without replacement
input <- rbind(bind23, bind24) %>%
  filter(wshed == "FB", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)
#find the starting point
combos <- unique(colnames(input[-1]))

#troubleshoot
#t <- 1
for(t in 1:length(combos)){
  
begin <- combos[t]
options_initial <- combos[combos != begin]
options <- combos[combos != begin]

routes <- data.frame("up" = rep(begin, length(options_initial)),
                     "down" = options_initial)
for(x in 1:length(options_initial)){
#x <- 5

  for(i in 1:length(options)){
#i <- 1
        out <- calc_support(routes$up[i], routes$down[i], input)
        print(paste0("i = ",i))

        if(i == 1) inner_out <- out
        if(i > 1) inner_out <- rbind(inner_out, out)
  }
  
  inner_out$prop <- inner_out$points/inner_out$total
  
        kep <- inner_out[inner_out$prop == max(inner_out$prop),]
        kep <- kep[1,]
  options <- options[options != kep$down[1]]
  routes <- data.frame("up" = rep(kep$down[1], length(options)),
                     "down" = options)
print(paste0("x = ",x))

  if(x == 1) big_keep <- kep
  if(x > 1) big_keep <- rbind(big_keep, kep)
}
big_keep$start <- begin
print(paste0("finished with",begin))
if(t == 1) biggest_keep <- big_keep
  if(t > 1) biggest_keep <- rbind(biggest_keep, big_keep)
}

smart_iterate_fb <- biggest_keep

biggest_keep %>% group_by(start) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  ggplot(aes(x = mean, y = sd))+
  geom_point(shape = 17)+
  geom_point(data = reference, aes(x = mean, y = sd, color = type, shape = shed), size = 2)+
  theme_classic()+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  labs(title = "Iteratively determined best hierarchy",
       y = "Proportion sd",
       x = "Proportion mean")

```

#maps of hierarchical behavior
```{r prepare-sensor-locs}
sults_so_far

pk_prop <- w3_pairs_pk %>% select(up, prop) %>% 
  mutate(ID = as.numeric(substr(up, 3, 4))) %>% 
  select(ID, prop)

locs <- data_23 %>% 
  filter(wshed == "W3") %>% 
  select(ID, lat, long) %>% 
  unique() %>% 
  left_join(pk_prop, by = "ID")


locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))

```
```{r map-template}
#map of watershed 3 with depth to bedrock
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
hill <- rast(hillshade_out)

#dem
#dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
#m1 <- rast(dem)
w3_crop <- "./w3_dems/1mdem_crop.tif"
m1 <- rast(w3_crop)
#watershed boundary
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)

#w3 network- thing I need to change
#read in shapefile of stream converted in ARC
vect_stream_path <- "./AGU24posterAnalysis/vector_stream/vector_stream.shp"
#stream as a vector
vect_stream <- vect(vect_stream_path)
#plot(vect_stream)
#crop to watershed boundary
w3_stream_crop <- crop(vect_stream, w3_outline)
plot(w3_stream_crop)
#or i could use old classification


ggplot()+
  geom_spatraster(data = hill, show.legend=FALSE)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5, , show.legend=FALSE)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
    new_scale_fill() +
  geom_sf(data = w3_outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = w3_stream_crop, colour = "midnightblue") +
    #geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 6) +
  geom_sf(data = lcc, aes(fill = prop), pch = 21, size = 3) +
  theme(rect = element_rect(fill = "transparent", color = NA))+
  scale_fill_gradient(low = "black",
                         high = "white",
                         name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"))+
  ggspatial::annotation_scale(location = 'br', pad_y = unit(1, "cm"), pad_x = unit(1, "cm"))+
  ggtitle("Flow Permanence (Pk), W3")
```
```{r define-plotting-function}
#use above code to write a function to plot the prop for each hierarchy and each watershed

#first, specify input dataset
#wshed inputs = c("W3", "FB", "ZZ")
#hierarchy = c("position", "pk", "dof", "uaa")
plot_in_space <- function(dataset, watershed, hierarchy){
  
  ##get correct geospatial files depending on input watershed
if(watershed == "W3") {
  #background rasters
  hill <- rast("./w3_dems/1mdem_hillshade.tif")
  m1 <- rast("./w3_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./w3_dems/w3_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./AGU24posterAnalysis/vector_stream/vector_stream.shp")
  stream_crop <- crop(vect_stream, w3_outline)
  
} else if(watershed == "FB"){
  #background rasters
  hill <- rast("./fb_dems/1mdem_hillshade.tif")
  m1 <- rast("./fb_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./fb_dems/fb_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./carrieZigZag/FB_network.shp")
  stream_crop <- crop(vect_stream, outline)
  
} else if(watershed == "ZZ"){
  #background rasters
  hill <- rast("./zz_dems/1mdem_hillshade.tif")
  m1 <- rast("./zz_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./zz_dems/zz_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./carrieZigZag/zigzag_streams.shp")
  stream_crop <- crop(vect_stream, outline)
  
} else {
  stop("Not a study watershed!")
}
  
  #take the input dataset of props, and format for plotting
  prop <- dataset %>% select(up, prop) %>%
    mutate(ID = as.numeric(substr(up, 3, 4))) %>%
    select(ID, prop)
  
  locs <- data_24 %>%
    filter(wshed == watershed) %>%
    rename("ID" = number) %>% 
    select(ID, lat, long) %>%
    unique() %>%
    left_join(prop, by = "ID") %>% 
    drop_na()
  
  locs_shape <- vect(locs, geom = c("long", "lat"), crs = "+proj=longlat +datum=WGS84")
  #reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
  lcc <- terra::project(locs_shape, crs(m1))



#specify plotting depending on hierarchy being tested

if(hierarchy == "position") {
  hierarchy_color <- "dodgerblue3"
  hierarchy_label <- "Relative Position"
  
} else if(hierarchy == "pk"){
  hierarchy_color <- "#FFA400"
  hierarchy_label <- "Flow Permanence"
  
} else if(hierarchy == "dof"){
  hierarchy_color <- "#397367"
  hierarchy_label <- "Duration of Flow"
  
} else if(hierarchy == "uaa"){
  hierarchy_color <- "#7E6B8F"
  hierarchy_label <- "Drainage Area"
  
} else {
  stop("Not a hierarchy anticipated!")
}

#final plot
named_plot <- ggplot()+
  geom_spatraster(data = hill, show.legend=FALSE)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = m1, alpha = 0.5, , show.legend=FALSE)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
    new_scale_fill() +
  geom_sf(data = outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = stream_crop, colour = "midnightblue", alpha = 0.3) +
    #geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 6) +
  geom_sf(data = lcc, aes(fill = prop), pch = 21, size = 3) +
  theme(rect = element_rect(fill = "transparent", color = NA))+
  scale_fill_gradient(low = "white",
                         high = hierarchy_color,
                         name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"))+
  ggspatial::annotation_scale(location = 'br', pad_y = unit(1, "cm"), pad_x = unit(1, "cm"))+
  ggtitle(paste0(hierarchy_label,", ", watershed))
named_plot
  #ggsave(paste0("hierarchy_maps/",watershed,"_",hierarchy,".jpg"), named_plot)

}
```
```{r run-function-for-scenarios}
#position
plot_in_space(w3_pairs, "W3", "position")
plot_in_space(fb_pairs, "FB", "position")
plot_in_space(zz_pairs, "ZZ", "position")

#pk
plot_in_space(w3_pairs_pk, "W3", "pk")
plot_in_space(fb_pairs_pk, "FB", "pk")
plot_in_space(zz_pairs_pk, "ZZ", "pk")

#dof
plot_in_space(w3_pairs_dof, "W3", "dof")
plot_in_space(fb_pairs_dof, "FB", "dof")
plot_in_space(zz_pairs_dof, "ZZ", "dof")

#topo
plot_in_space(w3_pairs_topo, "W3", "uaa")
plot_in_space(fb_pairs_topo, "FB", "uaa")
plot_in_space(zz_pairs_topo, "ZZ", "uaa")





```

After meeting on 2/6/25, I am going to plot the proportion of time each point follows the hierarchy using a square divided into 4 sections, one for each hierarchy
##showing all hierarchies on one plot- square plot (pigs in space)
```{r define-function}
#change to have 4 inputs, the 4 hierarchies... actually better to format hierarchy before plotting
format_outputs <- function(dataset, name){
  dataset %>% select(up, prop) %>%
    mutate(ID = as.numeric(substr(up, 3, 4))) %>%
    select(ID, prop)
}

position_formatted <- format_outputs(w3_pairs) %>% rename("Pos" = prop)
pk_formatted <- format_outputs(w3_pairs_pk) %>% rename("Pk" = prop)
dof_formatted <- format_outputs(w3_pairs_dof) %>% rename("Dof" = prop)
topo_formatted <- format_outputs(w3_pairs_topo) %>% rename("Topo" = prop)

#reformat to take 4 formatted datasets as input
pigs_in_space <- function(results,
                          watershed, offset, size){
  
  ##get correct geospatial files depending on input watershed
if(watershed == "W3") {
  #background rasters
  hill <- rast("./w3_dems/1mdem_hillshade.tif")
  m1 <- rast("./w3_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./w3_dems/w3_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./AGU24posterAnalysis/vector_stream/vector_stream.shp")
  stream_crop <- crop(vect_stream, w3_outline)
  
} else if(watershed == "FB"){
  #background rasters
  hill <- rast("./fb_dems/1mdem_hillshade.tif")
  m1 <- rast("./fb_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./fb_dems/fb_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./carrieZigZag/FB_network.shp")
  stream_crop <- crop(vect_stream, outline)
  
} else if(watershed == "ZZ"){
  #background rasters
  hill <- rast("./zz_dems/1mdem_hillshade.tif")
  m1 <- rast("./zz_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./zz_dems/zz_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./carrieZigZag/zigzag_streams.shp")
  stream_crop <- crop(vect_stream, outline)
  
} else {
  stop("Not a study watershed!")
}
  
  #take the input dataset of props, and format for plotting
  
  locs <- data_24 %>%
    filter(wshed == watershed) %>%
    rename("ID" = number) %>% 
    select(ID, lat, long) %>%
    unique() %>%
    left_join(results, by = "ID")
  
  locs_shape <- vect(locs, geom = c("long", "lat"), crs = "+proj=longlat +datum=WGS84")
  #reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
  lcc <- terra::project(locs_shape, crs(m1))
  
  lcc <- as.data.frame(lcc, geom = "XY")



#specify plotting depending on hierarchy being tested

#final plot
named_plot <- ggplot()+
  geom_spatraster(data = hill, show.legend=FALSE)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = m1, alpha = 0.5, , show.legend=FALSE)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
    new_scale_fill() +
  geom_sf(data = outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = stream_crop, colour = "midnightblue", alpha = 0.3) +
    #geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 6) +
  #plot the squares, but offset from center point
  #geom_sf(data = lcc, aes(fill = "Position"),color = "dodgerblue3", pch = 22, size = 1, position = position_nudge(x = offset, y = offset)) +
  #geom_sf(data = lcc, aes(fill = "Pk"), color ="#FFA400",  pch = 22, size = 1, position = position_nudge(x = offset, y = -offset)) +
  geom_point(data = lcc, aes(x = x, y = y, fill = pos),  
               pch = 22, size = size, position = position_nudge(x = offset, y = -offset)) + 
  scale_fill_gradient(low = "white",
                         high = "dodgerblue3",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+
      new_scale_fill() +
    geom_point(data = lcc, aes(x = x, y = y, fill = pk),  
               pch = 22, size = size, position = position_nudge(x = offset, y = offset)) + 
scale_fill_gradient(low = "white",
                         high = "#FFA400",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+
  new_scale_fill() +
    geom_point(data = lcc, aes(x = x, y = y, fill = dof),  
               pch = 22, size = size, position = position_nudge(x = -offset, y = -offset)) + 
  scale_fill_gradient(low = "white",
                         high = "#397367",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+
  new_scale_fill() +
    geom_point(data = lcc, aes(x = x, y = y, fill = uaa),  
               pch = 22, size = size, position = position_nudge(x = -offset, y = offset)) + 
  scale_fill_gradient(low = "white",
                         high = "#7E6B8F",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+


  theme(rect = element_rect(fill = "transparent", color = NA))+
  # scale_fill_gradient(low = "white",
  #                        high = hierarchy_color,
  #                        name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
  #                     breaks = c(0.25, 0.5, 0.75),
  #                     labels = c("25%", "50%", "75%"))+
  ggspatial::annotation_scale(location = 'br', pad_y = unit(1, "cm"), pad_x = unit(1, "cm"))
  # ggtitle(paste0(hierarchy_label,", ", watershed))
named_plot
  #ggsave(paste0("hierarchy_maps/",watershed,"_",hierarchy,".jpg"), named_plot)

}

#prepare sults_so_far
formatted_input <- sults_so_far %>% filter(shed == "W3", timescale == "30mins") %>% 
  mutate(up = as.numeric(substr(up, 3, 4))) %>% 
  rename("ID" = up) %>% 
  select(ID, prop, hierarchy) %>% 
  pivot_wider(names_from = hierarchy, values_from = prop) %>% 
  rename("pos" = 'Relative Position',
         "pk" = 'Flow Permanence',
         "dof" = 'Duration of Flow',
         "uaa" = 'Drainage Area')
#formatting that looks decent on big screen, in plot viewer pane
pigs_in_space(formatted_input,
                          "W3", 8, size = 4)

goth_pigs_in_space(formatted_input,
                          "W3", 8, size = 4)


```
```{r pigs-in-space-FB}

#prepare sults_so_far
formatted_input <- sults_so_far %>% filter(shed == "FB", timescale == "30mins") %>% 
  mutate(up = as.numeric(substr(up, 3, 4))) %>% 
  rename("ID" = up) %>% 
  select(ID, prop, hierarchy) %>% 
  pivot_wider(names_from = hierarchy, values_from = prop) %>% 
  rename("pos" = 'Relative Position',
         "pk" = 'Flow Permanence',
         "dof" = 'Duration of Flow',
         "uaa" = 'Drainage Area')
#formatting that looks decent on big screen, in plot viewer pane
pigs_in_space(formatted_input,
                          "FB", 7, size = 4)

print(goth_pigs_in_space(formatted_input,
                          "FB", 6, size = 3), vp=viewport(angle=-90))
```
```{r pigs-in-space-ZZ}

#prepare sults_so_far
formatted_input <- sults_so_far %>% filter(shed == "ZZ", timescale == "30mins") %>% 
  mutate(up = as.numeric(substr(up, 3, 4))) %>% 
  rename("ID" = up) %>% 
  select(ID, prop, hierarchy) %>% 
  pivot_wider(names_from = hierarchy, values_from = prop) %>% 
  rename("pos" = 'Relative Position',
         "pk" = 'Flow Permanence',
         "dof" = 'Duration of Flow',
         "uaa" = 'Drainage Area')
#formatting that looks decent on big screen, in plot viewer pane
pigs_in_space(formatted_input,
                          "ZZ", 9, size = 4)
goth_pigs_in_space(formatted_input,
                          "ZZ", 9, size = 4)
p_load(grid)
print(goth_pigs_in_space(formatted_input,
                          "ZZ", 9, size = 4), vp=viewport(angle=180))
```

##Black and white pigs in space
```{r}
goth_pigs_in_space <- function(results,
                          watershed, offset, size){
  
  ##get correct geospatial files depending on input watershed
if(watershed == "W3") {
  #background rasters
  hill <- rast("./w3_dems/1mdem_hillshade.tif")
  m1 <- rast("./w3_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./w3_dems/w3_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./AGU24posterAnalysis/vector_stream/vector_stream.shp")
  stream_crop <- crop(vect_stream, w3_outline)
  
} else if(watershed == "FB"){
  #background rasters
  hill <- rast("./fb_dems/1mdem_hillshade.tif")
  m1 <- rast("./fb_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./fb_dems/fb_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./carrieZigZag/FB_network.shp")
  stream_crop <- crop(vect_stream, outline)
  
} else if(watershed == "ZZ"){
  #background rasters
  hill <- rast("./zz_dems/1mdem_hillshade.tif")
  m1 <- rast("./zz_dems/1mdem_crop.tif")
  #watershed boundary
  outline <- as.polygons(rast("./zz_dems/zz_shed.tif"), extent=FALSE)
  #stream network
  vect_stream <- vect("./carrieZigZag/zigzag_streams.shp")
  stream_crop <- crop(vect_stream, outline)
  
} else {
  stop("Not a study watershed!")
}
  
  #take the input dataset of props, and format for plotting
  
  locs <- data_24 %>%
    filter(wshed == watershed) %>%
    rename("ID" = number) %>% 
    select(ID, lat, long) %>%
    unique() %>%
    left_join(results, by = "ID")
  
  locs_shape <- vect(locs, geom = c("long", "lat"), crs = "+proj=longlat +datum=WGS84")
  #reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
  lcc <- terra::project(locs_shape, crs(m1))
  
  lcc <- as.data.frame(lcc, geom = "XY")



#specify plotting depending on hierarchy being tested

#final plot
named_plot <- ggplot()+
  geom_spatraster(data = hill, show.legend=FALSE)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = m1, alpha = 0.5, , show.legend=FALSE)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
    new_scale_fill() +
  geom_sf(data = outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = stream_crop, colour = "midnightblue", alpha = 0.3) +
    #geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 6) +
  #plot the squares, but offset from center point
  #geom_sf(data = lcc, aes(fill = "Position"),color = "dodgerblue3", pch = 22, size = 1, position = position_nudge(x = offset, y = offset)) +
  #geom_sf(data = lcc, aes(fill = "Pk"), color ="#FFA400",  pch = 22, size = 1, position = position_nudge(x = offset, y = -offset)) +
  geom_point(data = lcc, aes(x = x, y = y, fill = pos),  
               pch = 22, size = size, position = position_nudge(x = offset, y = -offset)) + 
  scale_fill_gradient(low = "white",
                         high = "black",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+
      new_scale_fill() +
    geom_point(data = lcc, aes(x = x, y = y, fill = pk),  
               pch = 22, size = size, position = position_nudge(x = offset, y = offset)) + 
scale_fill_gradient(low = "white",
                         high = "black",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+
  new_scale_fill() +
    geom_point(data = lcc, aes(x = x, y = y, fill = dof),  
               pch = 22, size = size, position = position_nudge(x = -offset, y = -offset)) + 
  scale_fill_gradient(low = "white",
                         high = "black",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+
  new_scale_fill() +
    geom_point(data = lcc, aes(x = x, y = y, fill = uaa),  
               pch = 22, size = size, position = position_nudge(x = -offset, y = offset)) + 
  scale_fill_gradient(low = "white",
                         high = "black",
                         #name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"),
                      na.value = NA)+


  theme(rect = element_rect(fill = "transparent", color = NA))+
  # scale_fill_gradient(low = "white",
  #                        high = hierarchy_color,
  #                        name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
  #                     breaks = c(0.25, 0.5, 0.75),
  #                     labels = c("25%", "50%", "75%"))+
  ggspatial::annotation_scale(location = 'br', pad_y = unit(1, "cm"), pad_x = unit(1, "cm"))
  # ggtitle(paste0(hierarchy_label,", ", watershed))
named_plot
  #ggsave(paste0("hierarchy_maps/",watershed,"_",hierarchy,".jpg"), named_plot)

}
```

#test with Botter & Durighetto data
Conclusion- the max number of observations of the whole network in the dataset they used for their analysis was 7-35 observations. I have 10,000 observations of my network. Doing this kind of analysis on their data will not work I think.
```{r show-number-of-obs}
#read in Botter et al 2021 data
bigdata <- read_csv("https://researchdata.cab.unipd.it/376/2/states.csv")
#show the number of observations of the whole network used in their analysis
bigdata %>% 
  group_by(Catchment, Stretch) %>% 
  summarise(num_obs = length(Date)) %>% 
  select(Catchment, num_obs) %>% unique()

#determine the number of observations of the whole network from my dataset
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary, wshed) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
#outputs the number of observations for each sensor in my study
data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary, wshed) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  group_by(ID, wshed) %>% 
  summarise(num_obs = length(DATETIME)) %>% 
  select(wshed, ID, num_obs) %>% unique() %>% 
  View()


```
```{r test-hierarchical-behavior}
bigdata
```

#NOT DONE make network maps where points are connected 
```{r relative-position-UNFINISHED}

```
```{r flow-permanenc-w3}
#map where points are connected by upstream, downstream relationships
#start with routes from local persistency
#left join to lat/longs
nad_coords <- as.data.frame(lcc, geom = "XY")

routes %>% rename("ID" = up) %>% 
  left_join(nad_coords, by = "ID") %>% 
  rename("up" = ID,
         "ID" = down) %>% 
  left_join(nad_coords, by = "ID") %>% 
  #slice(c(seq(1, 30, 2))) %>% 
  #slice(c(seq(2, 30, 2))) %>% 
  ggplot()+
  geom_spatraster(data = hill, show.legend=FALSE)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5, , show.legend=FALSE)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
    new_scale_fill() +
  geom_sf(data = w3_outline, fill = NA, color = "black", alpha = 0.3)+
  geom_sf(data = w3_stream_crop, colour = "midnightblue", alpha = 0.5) +
  geom_point(aes(x = x.x, y = y.x))+
  geom_segment(aes(x = x.x, y = y.x, 
                   xend = x.y, yend = y.y, 
                   linewidth = prop.x, 
                   color = prop.x), 
               arrow=arrow(length=unit(0.3,"cm"))
               )+
scale_color_continuous(type = "viridis")+
                       theme(rect = element_rect(fill = "transparent", color = NA))+
  scale_fill_gradient(low = "black",
                         high = "white",
                         name = str_wrap("Proportion of time following hierarchy (%)", width = 15),
                      breaks = c(0.25, 0.5, 0.75),
                      labels = c("25%", "50%", "75%"))+
  ggspatial::annotation_scale(location = 'br', pad_y = unit(1, "cm"), pad_x = unit(1, "cm"))+
  ggtitle("Flow Permanence (Pk), W3")


#line connecting 
```




#Where on the hydrograph do transitions in states between two sensors occur?
```{r hydrograph-analysis}
#separate function to determine when on hydrograph transitions occur
determine_transitions <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 3
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])

no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
  drop_na()

colnames(no_dupes) <- c("up", "down", "datetime")
#determine if it is a wetting or drying transition by lagging up and down columns, then concatenating into a single number
coded_times <- no_dupes %>% 
  mutate(up_lag = lag(up),
         down_lag = lag(down)) %>% 
  mutate(coded = paste0(up_lag, down_lag, up, down)) %>% 
  mutate(direction = case_when(coded == "0001"| coded == "0111" ~ "wetting",
                               coded == "1101"| coded == "0100" ~ "drying")) %>% 
  filter(direction %in% c("wetting", "drying"))
return(coded_times)
}

#test functions
determine_transitions("r_23", "r_6", input)
all_transitions <- function(routes, input){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- determine_transitions(up, down, input)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  
  return(alldat)
}

all_trans <- all_transitions(routes, input)

all_trans$binary <- 1
all_trans$binary[all_trans$direction == "drying"] <- -1

testtt <- all_trans %>% group_by(datetime) %>% 
  filter(binary == -1) %>% 
  summarise(prop_wetting = sum(binary)) %>% 
  rename(DATETIME = datetime) %>% 
  right_join(q_23_f, by = "DATETIME")

ggplot(testtt, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  geom_point(data = drop_na(testtt), aes(x  = DATETIME, y = Q_mm_day, color = prop_wetting))+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()+
  scale_color_viridis_c(name = "Number of Drying")
```
```{r additional-quality-control}
#figure out what is going on for sensors 19 and 12 in watershed 3
#figure out a way to quickly diagnose/quality control all sensor pairs
rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == "W3", hour %in% c(12), mins %in% c(0),
             ID %in% c(19, 12)) %>%
      select(datetime, binary, ID) %>%
  ggplot()+
  geom_tile(aes(x = datetime, y = as.character(ID), fill = as.integer(binary)))
      
#for each pair, we are limited by the maximum extent of the record...
# to incorporate uncertainty, also provide the proportion of observations not used?
# also need to separate into separate chunks to account for gaps in record

rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == "W3", hour %in% c(12), mins %in% c(0),
             ID %in% c(19, 12)) %>%
      select(datetime, binary, ID) %>%
  filter(binary == 0)
```

#statistical test- one sample rank wilcoxon test
Way to ultimately summarise all of these scenarios- with the single test statistic from wilcoxon test
```{r}
# determine if the median of the sample (m) is greater than the theoretical value (mu)
wiltest <- sults_so_far %>% 
  filter(type == "position", shed == "W3")
median(wiltest$prop)

randomtest <- random_fb %>% 
  filter(iteration == 2)
median(randomtest$prop)
wilcox.test(wiltest$prop, mu = 0.5, alternative = "greater", exact = TRUE, conf.int = TRUE)
wilcox.test(randomtest$prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)

binom <- binom.test(sum(wiltest$prop > 0.5), length(wiltest$prop), p = 0.5, alternative = "greater")
binom.test(sum(randomtest$prop > 0.5), length(randomtest$prop), p = 0.5, alternative = "greater")

test_results <- wilcox.test(wiltest$prop, mu = 0.5, alternative = "greater", exact = TRUE, conf.int = TRUE)
pvalue <- test_results$p.value
upperb <- test_results$conf.int[1]
lowerb <- test_results$conf.int[2]

binom$p.value
binom$conf.int[1:2]

```

```{r run-test-on-everything}
#calculate the test statistics on results so far- without temporal degradation
sults_so_far %>% 
  group_by(shed, hierarchy, timescale) %>% 
  summarise(wcx_pvalue = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$p.value,
            wcx_upperb = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$conf.int[1],
            wcx_lowerb = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$conf.int[2]) %>% 
  filter(wcx_pvalue <= 0.1) %>% 
  knitr::kable()

intermediate <- sults_so_far %>% 
  filter(shed == "W3", timescale == "30mins", hierarchy == "Drainage Area") 

#testing binom test
test <- intermediate$prop
binom.test(sum(test > 0.5), length(test), p = 0.5, alternative = "greater")$p.value
length(test > 0.5)
Mode(test)

sults_so_far %>% 
  group_by(shed, hierarchy, timescale) %>% 
  summarise(sign_p = binom.test(sum(prop > 0.5), length(prop), p = 0.5, alternative = "greater")$p.value,
            sign_upperb = binom.test(sum(prop > 0.5), length(prop), p = 0.5, alternative = "greater")$conf.int[1],
            sign_lowerb = binom.test(sum(prop > 0.5), length(prop), p = 0.5, alternative = "greater")$conf.int[2]) %>% 
  filter(sign_p <= 0.1) %>% 
  knitr::kable()

p_load(DescTools)

sults_so_far %>% 
  filter(shed == "ZZ") %>% 
    group_by(shed, hierarchy, timescale) %>% 
  summarise(median = median(prop),
            mode1 = Mode(prop)[1],
            mode2 = Mode(prop)[2]) %>% 
  knitr::kable()
```
```{r calculate-wilcox-0.5}
rand_test <- random_w3 %>% filter(timescale == "30mins")

wilcox.test(test, rand_test$prop, alternative = "greater", exact = FALSE, conf.int = TRUE)$p.value
#wilcox test against randomly generated
sults_so_far %>% 
  group_by(shed, hierarchy, timescale) %>% 
  summarise(wcx_pvalue = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$p.value,
            wcx_upperb = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$conf.int[1],
            wcx_lowerb = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$conf.int[2]) %>% 
  filter(wcx_pvalue <= 0.1) %>% 
  knitr::kable()
```

#NOT DONE calculating discharge
```{r}
#source function I wrote in separate script
source("calcQ.R")
```



#Try to predict the flow permanence based on topographic metrics
Use sensor locations snapped on ARC computer to extract different topographic metrics; metrics can be calculated here in R using whitebox tools.
##multilinear regression
```{r sensor-locations}
#get snapped sensor locations from extracted values from arc
w3_locs <- read_csv("./STIC_uaa/w32.csv")%>% 
  select(ID, POINT_X, POINT_Y)
fb_locs <- read_csv("./STIC_uaa/fb2.csv") %>% 
  select(ID, POINT_X, POINT_Y)
zz_locs <- read_csv("./STIC_uaa/zz1.csv")%>% 
  select(ID, POINT_X, POINT_Y)
```
```{r extract-topography-W3}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(w3_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

# To make the raster work: read in using raster package, then save as a new file
test <- raster::raster("HB/hbstream/w3_5m.tif")
writeRaster(test, "actualW3_shed.tif", overwrite = TRUE)
plot(rast("actualW3_shed.tif"), col = 'purple')
plot(locs_shape, add = TRUE)

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./fb_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)


#drainage area from ARC

#extract values
vals_df_w3 <- bind_cols(as_tibble(extract(rast(flowacc_output), 
                                       locs_shape, ID = FALSE, bind = TRUE)),
            as_tibble(extract(rast(slope_output), 
                              locs_shape, ID = FALSE, bind = TRUE))[,2],
            as_tibble(extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE))[,2],
            as_tibble(extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE))[,2]) %>% 
  rename("uaa" = '10mdem_flowacc', "slope" = `10mdem_slope`, "twi" = `10mdem_twi`, "tpi" = `10mdem_tpi`) %>% 
  left_join(pks_w3, by = "ID") %>% 
  select(-wshed, -ID)


```
```{r figuring-out-model}
#use reformatted df with sensor flow permanence and topo metrics in a multilinear regression
model <- lm(pk ~ uaa + slope + twi + tpi, data = vals_df)
summary(model)
```
##Fixing multilinear regression
```{r regression-fb}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(fb_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)


vals_df_fb <- bind_cols(as_tibble(extract(rast(flowacc_output), 
                                       locs_shape, ID = FALSE, bind = TRUE)),
            as_tibble(extract(rast(slope_output), 
                              locs_shape, ID = FALSE, bind = TRUE))[,2],
            as_tibble(extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE))[,2],
            as_tibble(extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE))[,2]) %>% 
  rename("uaa" = '10mdem_flowacc', "slope" = `10mdem_slope`, "twi" = `10mdem_twi`, "tpi" = `10mdem_tpi`) %>% 
  left_join(pks_fb, by = "ID") %>% 
  select(-wshed, -ID)

model_fb <- lm(pk ~ uaa + slope + twi + tpi, data = vals_df_fb)
summary(model_fb)

```
```{r regression-zz}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(zz_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

vals_df_zz <- bind_cols(as_tibble(extract(rast(flowacc_output), 
                                       locs_shape, ID = FALSE, bind = TRUE)),
            as_tibble(extract(rast(slope_output), 
                              locs_shape, ID = FALSE, bind = TRUE))[,2],
            as_tibble(extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE))[,2],
            as_tibble(extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE))[,2]) %>% 
  rename("uaa" = '10mdem_flowacc', "slope" = `10mdem_slope`, "twi" = `10mdem_twi`, "tpi" = `10mdem_tpi`) %>% 
  left_join(pks_zz, by = "ID") %>% 
  select(-wshed, -ID)

model_zz <- lm(pk ~ uaa + slope + twi + tpi, data = vals_df_zz)
summary(model_zz)
```
```{r combine-and-model}
vals_all <- rbind(vals_df_w3, vals_df_fb, vals_df_zz)

model_all <- lm(pk ~ uaa + slope + twi + tpi, data = vals_all)
summary(model_all)
```

#Running analysis again, but comparing wetting versus drying... how well do they follow hierarchy when distinguised
3-31-25, attempting to differentiate between wetting and drying events
```{r differentiated-states-functions}
#chunk to trouble shoot non-functioning instances to figure out why they are not working
#instance not working:
#run_scenario(routes_w3_pk, "pk", "W3", "daily")

routes <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

input <- rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
        filter(wshed == "W3", mins %in% c(0, 30)) %>%
      #filter(wshed == "W3", hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary) #%>% 
  #filter(datetime < stop24 & datetime > start24)


all_transitions <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  output$wetting <- NA
  output$drying <- NA
  return(output)
}
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
#if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")
sequence_df$t1_up <- as.numeric(substr(sequence_df$windows, 1, 1))
sequence_df$t1_down <- as.numeric(substr(sequence_df$windows, 2, 2))
sequence_df$t2_up <- as.numeric(substr(sequence_df$windows, 3, 3))
sequence_df$t2_down <- as.numeric(substr(sequence_df$windows, 4, 4))

sequence_df$sum_t1 <- sequence_df$t1_up + sequence_df$t1_down
sequence_df$sum_t2 <- sequence_df$t2_up + sequence_df$t2_down
sequence_df$direction <- "drying"
sequence_df$direction[sequence_df$sum_t1 < sequence_df$sum_t2] <- "wetting"
#new fixed code, should not drop non-hierarchical values
total_state_changes <- sequence_df %>% group_by(direction) %>% summarise(totals = sum(Freq))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- sequence_df %>% 
  filter(windows %in% supports) %>% group_by(direction) %>% summarise(hierarchical = sum(Freq)) 

sub <- total_state_changes %>% left_join(hierarchical_changes, by = c("direction")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))

#old code

# total_state_changes <- sequence_df %>% group_by(direction) %>% summarise(totals = sum(Freq))
# 
# supports <- c("0001","0111","1101", "0100")
# sub <- filter(sequence_df, windows %in% supports) %>%
#   group_by(direction) %>% summarise(hierarchicals = sum(Freq)) %>% 
#   left_join(total_state_changes, by = "direction") %>% 
#   mutate(prop = hierarchicals/totals)
#output$points <- sum(sub$Frequency)
output$drying <- sub$prop[1]
output$wetting <- sub$prop[2]
#write some way to score the sequence_df
#award one point for one of these configs:


#sub <- filter(sequence_df, Sequence %in% supports)

#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(output)}
}

#test function
all_transitions("r_23", "r_6", input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_wd <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- all_transitions(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

#iterate_groups_wd("r_23", "r_6", input, hours(1))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_wd <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- iterate_groups_wd(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    # group_by(up, down) %>%
    # summarise(total = sum(total),
    #           points = sum(points)) %>% 
    # mutate(prop = points/total)
  return(final_output)
}

#calculate_hierarchy_wd(routes, input, minutes(30))
#calculate_hierarchy(routes, input, days(1))


#make a function to loop through the four possible timesteps, and combine the output just for ease of applying this many different variations
#fantastic 4 determines the input on its own
fantastic_four_wd <- function(routes, shed){
  theFour <- c("30mins", "hourly", "4hr", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "hourly"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(1)
  } 
  else if(timescale == "4hr"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(0,4,8,12,16,20,24), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(4)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_wd(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}

#for each scenario create the routes, inputs, and specify the timestep
```
4-1-25, success! I can say what proportion of wetting and drying events follow a hierarchy

##finding flaw in wetting and drying analysis
Determined that original version resulted in excluding all sensor pairs with no hierarchical behavior, so distributions seemed a lot better than earlier analyses.
```{r finding-problem}
routes_fb <- read_csv("fb_flowrouting.csv") %>% 
  filter(up %in% FB_IDs) %>% 
  filter(down %in% FB_IDs) %>% 
  drop_na()

input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == "FB", hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)


fb_test <- calculate_hierarchy_combos(routes_fb, input, days(1))
fb_test_orig <- fb_test

fb_test$t1_up <- as.numeric(substr(fb_test$Sequence, 1, 1))
fb_test$t1_down <- as.numeric(substr(fb_test$Sequence, 2, 2))
fb_test$t2_up <- as.numeric(substr(fb_test$Sequence, 3, 3))
fb_test$t2_down <- as.numeric(substr(fb_test$Sequence, 4, 4))

fb_test$sum_t1 <- fb_test$t1_up + fb_test$t1_down
fb_test$sum_t2 <- fb_test$t2_up + fb_test$t2_down
fb_test$direction <- "drying"
fb_test$direction[fb_test$sum_t1 < fb_test$sum_t2] <- "wetting"

total_state_changes <- fb_test %>% group_by(up, down, direction) %>% summarise(totals = sum(Frequency))
#up down up down
supports <- c("0001","0111","1101", "0100")

#figuring out how to split wetting and drying, and non-hierarchical and hierarchical
hierarchical_changes <- fb_test %>% 
  filter(Sequence %in% supports) %>% group_by(up, down, direction) %>% summarise(hierarchical = sum(Frequency)) 

total_state_changes %>% left_join(hierarchical_changes, by = c("direction", "up", "down")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0)) %>% View()

#without wetting and drying
total_state_changes <- fb_test %>% group_by(up, down) %>% summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- fb_test %>% 
  filter(Sequence %in% supports) %>% group_by(up, down) %>% summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% left_join(hierarchical_changes, by = c("up", "down")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
un_split
```

##Running analysis on wetting versus drying
```{r relative-position-wetANDdry}
#set routes for all 3 watersheds
routes_w3 <- read_csv("w3_flowrouting.csv") %>% 
  filter(sensor %in% W3_IDs, drains_from %in% W3_IDs) %>% 
  rename("up" = drains_from, "down" = sensor) %>% 
  select(up, down) %>% 
  filter(down != 100, up != 0)
  
routes_fb <- read_csv("fb_flowrouting.csv") %>% 
  filter(up %in% FB_IDs) %>% 
  filter(down %in% FB_IDs) %>% 
  drop_na()
routes_zz <- read_csv("zz_flowrouting.csv") %>% 
  filter(up %in% ZZ_IDs) %>% 
  filter(down %in% ZZ_IDs) %>% 
  drop_na()

#run calc_support for all sheds and timesteps for relative position
all_position_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                      fantastic_four_wd(routes_fb, "FB"),
                      fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Relative Position")

summarized_position_wd <- all_position_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying))
```
```{r flow-permanence}
#make list that make pairs of sites based on local persistency
routes_w3 <- pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- pks_fb %>%
    filter(ID %in% FB_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- pks_zz %>%
    filter(ID %in% ZZ_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

all_pk_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                fantastic_four_wd(routes_fb, "FB"),
                fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Flow Permanence")

summarized_pk_wd <- all_pk_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying))
```
```{r duration-of-flow}
#change duration of flow to the number of times a sensor changes state
routes_w3 <- dof_w3 %>%
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(avg_days_flowing)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

#make list that make pairs of sites based on local persistency
routes_w3 <- dof_w3 %>%
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(avg_days_flowing)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- dof_fb %>% 
    filter(ID %in% FB_IDs) %>% 

  arrange(desc(avg_days_flowing)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- dof_zz %>% 
    filter(ID %in% ZZ_IDs) %>% 

  arrange(desc(avg_days_flowing)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

all_dof_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                 fantastic_four_wd(routes_fb, "FB"),
                 fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Duration of Flow")

summarized_dof_wd <- all_dof_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying))
```
```{r drainage-area}
#I was not confident in my R determined values, so I extracted drainage area in Arc. Reading in results

#.csv files with extracted 1m drainage area values, but also contain snapped coords in UTM
fb_uaa_1m <- read_csv("./STIC_uaa/fb2.csv")
zz_uaa_1m <- read_csv("./STIC_uaa/zz2.csv")
w3_uaa_1m <- read_csv("./STIC_uaa/w32.csv")

routes_w3 <- w3_uaa_1m %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_uaa_1m %>%
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_zz <- zz_uaa_1m %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_uaa_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                 fantastic_four_wd(routes_fb, "FB"),
                 fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Drainage Area")

summarized_uaa_wd <- all_uaa_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying))
```
```{r topographic-position-index}
# fb_uaa_1m <- read_csv("./STIC_uaa/fb2.csv")
# zz_uaa_1m <- read_csv("./STIC_uaa/zz2.csv")
# w3_uaa_1m <- read_csv("./STIC_uaa/w32.csv")

routes_w3 <- w3_tpi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_tpi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_tpi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_tpi_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                 fantastic_four_wd(routes_fb, "FB"),
                 fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Topographic Position Index")

summarized_tpi_wd <- all_tpi_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying))

```
```{r topographic-wetness-index}
routes_w3 <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_twi_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                 fantastic_four_wd(routes_fb, "FB"),
                 fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Topographic Wetness Index")

summarized_twi_wd <- all_twi_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying))
```
```{r slope}
routes_w3 <- w3_slope %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(slope) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_slope %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange((slope)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_slope %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange((slope)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_slope_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                 fantastic_four_wd(routes_fb, "FB"),
                 fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Slope")

summarized_slope_wd <- all_slope_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying))

```
##Plots based on wetting and drying broken up
Consolidate the gains!!
```{r plots-for-wetting-and-drying}


sults_so_far <- rbind(summarized_position_wd,
                      summarized_pk_wd,
                      #summarized_dof_wd,
                      #summarized_uaa_wd,
                      #summarized_tpi_wd,
                      summarized_twi_wd
                      #summarized_slope_wd
                      ) %>% ungroup() %>% 
  mutate(timescale = fct_relevel(timescale,
                              c("30mins", "hourly", "4hr", "daily")),
         hierarchy = as.factor(str_trim(as.character(hierarchy))),
         
         hierarchy = fct_relevel(hierarchy, 
                                 c(#"Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                #"Drainage Area",
                                #"Topographic Position Index",
                                "Topographic Wetness Index"#,
                                #"Slope"
  ))) %>% 
  pivot_longer(cols = starts_with("avg"),
               names_to = "direction",
               values_to = "prop") 

sults_so_far$direction[sults_so_far$direction == "avg_wet_prop"] <- "wetting"
sults_so_far$direction[sults_so_far$direction == "avg_dry_prop"] <- "drying"


sults_so_far <- sults_so_far%>% 
  mutate(direction = fct_relevel(direction,
                                 c("drying", "wetting")))


sults_so_far %>% 
  group_by(shed, hierarchy, timescale, direction) %>% 
  summarise(wcx_pvalue = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$p.value,
            wcx_upperb = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$conf.int[1],
            wcx_lowerb = wilcox.test(prop, mu = 0.5, alternative = "greater", exact = FALSE, conf.int = TRUE)$conf.int[2]) %>% 
  filter(wcx_pvalue <= 0.1) %>% 
  knitr::kable()

sults_so_far %>% 
  group_by(shed, hierarchy, timescale, direction) %>% 
  summarise(wcx_pvalue = wilcox.test(prop, mu = 0.5, alternative = "two.sided", exact = FALSE, conf.int = TRUE, conf.level = 0.95)$p.value,
            wcx_lowerb = wilcox.test(prop, mu = 0.5, alternative = "two.sided", exact = FALSE, conf.int = TRUE, conf.level = 0.95)$conf.int[1],
            wcx_upperb = wilcox.test(prop, mu = 0.5, alternative = "two.sided", exact = FALSE, conf.int = TRUE, conf.level = 0.95)$conf.int[2]) %>% 
  filter(wcx_pvalue <= 0.05) %>% 
  knitr::kable()

sults_so_far %>% 
  ggplot(aes(x = shed, y = prop, color = hierarchy)) +
  geom_boxplot()+
  theme_classic()+
    geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Watershed",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     labels = c("Duration of Flow",
                                "Flow Permanence",
                                "Relative Position",
                                "Drainage Area"),
                     name = "Organizing Scheme")+
  facet_grid(direction~timescale)

possible_times <- c("30mins", "hourly", "4hr", "daily")

t <- 4
sults_so_far %>% 
  filter(timescale %in% c(possible_times[t])) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = direction, color = direction), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  labs(title = possible_times[t],
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = c("#FFA400", "#93C2F1"),name = "Direction")+
  scale_color_manual(values = c("#FFA400", "#93C2F1"), name = "Direction")+
  ylim(c(0, 4))+
  xlim(c(0,1))+
  facet_grid(shed~hierarchy)

#instead of boxplots, show the ecdf and one-sided violin plot
sults_so_far %>% 
  filter(timescale == "30mins", shed == "W3") %>% 
  ggplot(aes(x = shed, y = prop, color = hierarchy)) +
  geom_violin()+
  theme_classic()+
  geom_point(position = position_jitterdodge(jitter.width = 0.1), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Watershed",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F", "grey"),
                     name = "Organizing Scheme")+
  facet_wrap(~timescale)

#violin plot of just 30 min timescale
sults_so_far %>% 
  filter(timescale %in% c("30mins", "hourly")) %>% 
  ggplot(aes(x = timescale, y = prop, color = hierarchy)) +
  geom_violin()+
  theme_classic()+
  geom_point(position = position_jitterdodge(jitter.width = 0.1), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
  labs(x = "Timescale",
       y = "Proportion",
       title = "How often do adjacent sensors follow a hierarchy (daily timescale?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     name = "Organizing Scheme")+
    facet_grid(rows = "shed")




#plot ecdf instead of boxplots or violin plots
plot(ecdf(w3_hourly$prop))
plot(ecdf(alldat$prop))
ggplot()+
  stat_ecdf(data = w3_pairs, aes(prop))+
  stat_ecdf(data = w3_new, aes(prop), color = "blue")

sults_so_far %>% 
  filter(timescale == "30mins") %>% 
  ggplot(aes(prop, color = hierarchy)) +
  stat_ecdf(geom = "line")+
  theme_classic()+
    #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  geom_hline(yintercept = 0.5, lty = 2)+
    geom_vline(xintercept = 0.5, lty = 2)+

  labs(x = "Prop",
       y = "Percentage of values less than or equal",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     name = "Organizing Scheme")+
  facet_wrap(~shed)#+
  lims(x = c(0.5,1),
       y = c(0.5, 1))

#just plain distributions might look better?
sults_so_far %>% 
  filter(timescale == "30mins") %>% 
  ggplot(aes(prop, color = hierarchy)) +
geom_density(alpha = 0.5)+
    theme_classic()+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+

    #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  labs(x = "Prop",
       y = "Density",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     name = "Organizing Scheme")+
  facet_wrap(~shed)#+

#try plain histogram
sults_so_far %>% 
  filter(timescale == "30mins") %>% 
  ggplot(aes(prop, fill = hierarchy)) +
geom_histogram(binwidth = 0.1)+
    theme_classic()+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+

    #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  labs(x = "Prop",
       y = "Density",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_fill_manual(values = c("#397367", "#FFA400", "#93C2F1", "#7E6B8F"),
                     
                     name = "Organizing Scheme")+
  facet_grid(hierarchy~shed)

sults_so_far %>% 
  filter(timescale %in% c("30mins", "hourly")) %>% 
  ggplot(aes(x = prop, fill = shed, y = after_stat(density))) +
geom_density(alpha = 0.5)+
      geom_vline(xintercept = 0.5, lty = 2)+
    theme_classic()+
  labs(title = "Distributions of Proportion of time Hiearchies Followed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = c("#397367", "#FFA400", "#93C2F1"),
                     
                     name = "Watershed")+
  facet_wrap(~hierarchy)

sults_so_far %>% 
  filter(timescale %in% c("30mins", "hourly")) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
  
geom_density(aes(fill = shed, color = shed), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  labs(title = "Distributions of Proportion of time Hiearchies Followed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = c("#397367", "#FFA400", "#93C2F1"),
                     
                     name = "Watershed")+
  scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1"),
                     
                     name = "Watershed")+
  facet_grid(timescale~hierarchy)


sults_so_far %>% 
  filter(timescale %in% c("30mins", "hourly")) %>% 
  ggplot() +
geom_density(alpha = 0.5)+
      geom_vline(xintercept = 0.5, lty = 2)+
    theme_classic()+
  labs(title = "Distributions of Proportion of time Hiearchies Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_wrap(~hierarchy)
#maybe it would look better to add the randomly generated hierarchies for a comparison
```

```{r}
rbind(bind23, bind24) %>% 
  filter(wshed == "W3", ID == 2)
```

#NOT DONE Run daily analysis again
Alter code to test two scenarios and compare to the original one
- test every hour for daily observation
- average state
- most common state in a day

#new analysis after committee meeting
##remove 0011, see how this affects distributions
###developing functions to return all combinations
Output of these functions will return all combinations of 4 observed and their counts, instead of filtering to combinations of interest like earlier attempts
Included a new function, calc_props that removes 0011 and 1100, but a function for different scenarios could be developed here instead
```{r all-combos-functions}
#removed all supports that I am filtering out
calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  sequence_df <- data.frame("Sequence" = NA, 
                            "Frequency" = NA,
                            "up" = up,
                            "down" = down)
  return(sequence_df)
} 
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
#supports <- c("0001","0111","1101", "0100")


sub <- sequence_df#filter(sequence_df, Sequence %in% supports)
output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)}
}

#test function
#calc_support_combos("r_13", "r_19", input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_combos <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support_combos(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

#iterate_groups("r_13", "r_19", input, min(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_combos <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- iterate_groups_combos(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down, Sequence) %>%
    summarise(Frequency = sum(Frequency))
  return(final_output)
}

#calculate_hierarchy_combos(routes_w32, input, min(30))

fantastic_four_combos <- function(routes, shed){
  theFour <- c("30mins", "hourly", "4hr", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "hourly"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(1)
  } 
  else if(timescale == "4hr"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(0,4,8,12,16,20,24), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(4)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_combos(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}

routes_w3 <- read_csv("w3_flowrouting.csv") %>% 
  filter(sensor %in% W3_IDs, drains_from %in% W3_IDs) %>% 
  rename("up" = drains_from, "down" = sensor) %>% 
  select(up, down) %>% 
  filter(down != 100, up != 0)

fantastic_four_combos(routes_w3, "W3") %>% View()


#must calculate prop values after and outside of fantastic 4
calc_props <- function(routes, shed){
  full_combos <- fantastic_four_combos(routes, shed)
  total_state_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    group_by(up, down, timescale, shed) %>% 
    summarise(totals = sum(Frequency))
  supports <- c("0001","0111","1101", "0100")

  hierarchical_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down, timescale, shed) %>%  
    summarise(hierarchical = sum(Frequency)) 

  un_split <- total_state_changes %>% 
    left_join(hierarchical_changes, by = c("up", "down", "shed", "timescale")) %>% 
    mutate(prop = hierarchical/totals) %>% 
    mutate_all(~replace(., is.na(.), 0))
  
return(un_split)
}


#code used to develop calc_props!
calc_props(routes_w3, "W3")
total_state_changes <- test %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    group_by(up, down, timescale, shed) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- test %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down, timescale, shed) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down", "shed", "timescale")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
un_split

#use this chunk to re-run everything
```
###re-run original analysis without 0011 and 1100 being included in prop value calculation
```{r relative-position}
#set routes for all 3 watersheds
routes_w3 <- read_csv("w3_flowrouting.csv") %>% 
  filter(sensor %in% W3_IDs, drains_from %in% W3_IDs) %>% 
  rename("up" = drains_from, "down" = sensor) %>% 
  select(up, down) %>% 
  filter(down != 100, up != 0)
  
routes_fb <- read_csv("fb_flowrouting.csv") %>% 
  filter(up %in% FB_IDs) %>% 
  filter(down %in% FB_IDs) %>% 
  drop_na()
routes_zz <- read_csv("zz_flowrouting.csv") %>% 
  filter(up %in% ZZ_IDs) %>% 
  filter(down %in% ZZ_IDs) %>% 
  drop_na()

#run calc_support for all sheds and timesteps for relative position
all_position_combo <- rbind(calc_props(routes_w3, "W3"),
                      calc_props(routes_fb, "FB"),
                      calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Relative Position")
#write_csv(all_position, "./hierarchy_analysis_results/relativePosition.csv")
#test <- read_csv("./hierarchy_analysis_results/relativePosition.csv")
```
```{r flow-permanence}
#make list that make pairs of sites based on local persistency
routes_w3 <- pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)
write_csv(routes_w3, "routes_w3_pk.csv")
routes_fb <- pks_fb %>%
    filter(ID %in% FB_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)
write_csv(routes_fb, "routes_fb_pk.csv")

routes_zz <- pks_zz %>%
    filter(ID %in% ZZ_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)
write_csv(routes_zz, "routes_zz_pk.csv")

all_pk_combo <- rbind(calc_props(routes_w3, "W3"),
                calc_props(routes_fb, "FB"),
                calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Flow Permanence")

#write_csv(all_pk, "./hierarchy_analysis_results/flowPermanence.csv")
```
```{r number-of-transitions}
#make list that make pairs of sites based on local persistency
routes_w3 <- nt_w3 %>%
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(avg_nt)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- nt_fb %>% 
    filter(ID %in% FB_IDs) %>% 
  arrange(desc(avg_nt)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- nt_zz %>% 
    filter(ID %in% ZZ_IDs) %>% 
  arrange(desc(avg_nt)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

all_nt_combo <- rbind(calc_props(routes_w3, "W3"),
                 calc_props(routes_fb, "FB"),
                 calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Duration of Flow")

#write_csv(all_dof, "./hierarchy_analysis_results/durationOfFlow.csv")
```
```{r drainage-area}
#I was not confident in my R determined values, so I extracted drainage area in Arc. Reading in results

#.csv files with extracted 1m drainage area values, but also contain snapped coords in UTM
fb_uaa_1m <- read_csv("./STIC_uaa/fb2.csv")
zz_uaa_1m <- read_csv("./STIC_uaa/zz2.csv")
w3_uaa_1m <- read_csv("./STIC_uaa/w32.csv")

routes_w3 <- w3_uaa_1m %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_uaa_1m %>%
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_zz <- zz_uaa_1m %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_uaa_combo <- rbind(calc_props(routes_w3, "W3"),
                 calc_props(routes_fb, "FB"),
                 calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Drainage Area")

#write_csv(all_uaa, "./hierarchy_analysis_results/drainageArea.csv")
```
```{r topographic-position-index}
# fb_uaa_1m <- read_csv("./STIC_uaa/fb2.csv")
# zz_uaa_1m <- read_csv("./STIC_uaa/zz2.csv")
# w3_uaa_1m <- read_csv("./STIC_uaa/w32.csv")

routes_w3 <- w3_tpi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_tpi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_tpi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_tpi_combo <- rbind(calc_props(routes_w3, "W3"),
                 calc_props(routes_fb, "FB"),
                 calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Topographic Position Index")

#write_csv(all_tpi, "./hierarchy_analysis_results/tpi.csv")
```
```{r topographic-wetness-index}
routes_w3 <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_twi_combo <- rbind(calc_props(routes_w3, "W3"),
                 calc_props(routes_fb, "FB"),
                 calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Topographic Wetness Index")

#write_csv(all_twi, "./hierarchy_analysis_results/twi.csv")
```
```{r slope}
routes_w3 <- w3_slope %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(slope) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_slope %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange((slope)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_slope %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange((slope)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_slope_combo <- rbind(calc_props(routes_w3, "W3"),
                 calc_props(routes_fb, "FB"),
                 calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Slope")

#write_csv(all_slope, "./hierarchy_analysis_results/slope.csv")
```
###plot results
```{r plot-results}
sults_so_far <- rbind(all_position_combo,
                      all_pk_combo
                      #all_nt_combo,
                     # all_uaa_combo,
                      #all_tpi_combo,
                     # all_slope_combo,
                      #all_twi_combo
                      ) %>% 
  mutate("real_or_random" = "real")

ggmst$label
  mutate(timescale = fct_relevel(timescale,
                              c("30mins", "hourly", "4hr", "daily")),
         hierarchy = as.factor(str_trim(as.character(hierarchy))),
         
         hierarchy = fct_relevel(hierarchy, 
                                 c(
                                "Flow Permanence",
                                #"Number of Transitions",
                                "Relative Position",
                                "Drainage Area",
                                "Slope",
                                "Topographic Wetness Index",
                                #"Topographic Position Index"
                                )))#,
                                #"Random")))

shed_colors <- c("#397367", "#FFD166", "#7E6B8F")

#plots for committee meeting
sults_so_far %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = shed, color = shed), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = shed_colors,
                     
                     name = "Watershed")+
  scale_color_manual(values = shed_colors,
                     
                     name = "Watershed")+
  facet_grid(timescale~hierarchy)


```



##Try all combinations
```{r iterating-smart-W3}
#pick a sensor randomly, then test every other sensor to see which has the best hierarchical behavior with that one. Proceed down the list without replacement
# combos <- W3_uaa_ex %>% 
#   rename("up" = ID, "uaa" = '10mdem_flowacc') %>%
#   arrange(desc(uaa)) %>% 
#   mutate(down = lag(up)) %>% 
#    drop_na() %>% 
#   select(up, down) %>% 
#   mutate(up = paste0("r_",up),
#          down = paste0("r_",down)) %>% 
#   select(up)

#combos <- unique(combos$up)

input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- unique(colnames(input[-1]))
#find the starting point

for(t in 1:length(combos)){
 #t <- 1 
begin <- combos[t]
options_initial <- combos[combos != begin]
options <- combos[combos != begin]

routes <- data.frame("up" = rep(begin, length(options_initial)),
                     "down" = options_initial)
for(x in 1:length(options_initial)){
#x <- 1
#calc_support("r_23", "r_6", input)

  for(i in 1:length(options)){
        out <- calc_support_combos(routes$up[i], routes$down[i], input)
        if(i == 1) inner_out <- out
        if(i > 1) inner_out <- rbind(inner_out, out)
  }
  
  routes <- data.frame("up" = combos[x],
                     "down" = combos)
print(paste0("x = ",x))

  if(x == 1) big_keep <- kep
  if(x > 1) big_keep <- rbind(big_keep, kep)
}
big_keep$start <- begin
if(t == 1) biggest_keep <- big_keep
  if(t > 1) biggest_keep <- rbind(biggest_keep, big_keep)
}
#started at 1:55 pm, finished 1:59
smart_iterate_w3 <- biggest_keep
smart_iterate_w3 %>% group_by(start) #%>% 
  ggplot()+
  geom_density(data = smart_iterate_w3, 
               aes(x = prop, y = after_stat(density), color = start))+
  geom_density(data = filter(all_pk, timescale == "30mins" | shed == "W3"))+
  aes(x = prop, y = after_stat(density))#+

```

```{r define-combos-function}
#function that will take two sensors, and determine which one is dominant, and what percentage of time it follows that hierarchy
calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  sequence_df <- data.frame("Sequence" = NA, 
                            "Frequency" = NA,
                            "up" = up,
                            "down" = down)
  return(sequence_df)
} 
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")


sub <- filter(sequence_df, Sequence %in% supports)
output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)}
}

#calc_support_combos("r_13", "r_19", input)

calc_props <- function(routes, shed){
  full_combos <- fantastic_four_combos(routes, shed)
total_state_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    group_by(up, down, timescale, shed) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down, timescale, shed) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down", "shed", "timescale")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
return(un_split)
}
```
```{r}
#set routes for all 3 watersheds
routes_w3 <- pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- unique(c(routes_w3$up, routes_w3$down))
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
test <- calc_props(all_combos_routes, "W3")
determined_directions <- test
```

```{r making-my-first-graph!}
#figure out how to display results of testing all possible combinations
#output the highest downstream for each upstream

p_load(igraph)

test %>% 
  group_by(up, timescale) %>%
  filter(timescale == "daily") %>% 
  reframe(max_prop = max(prop),
    down1 = down[which(prop == max(prop))]) %>% 
  filter(max_prop != 0) %>% View()#, !down1 %in% c("r_12", "r_19", "r_20"))

test %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>%
  group_by(up) 

edges <- test %>% 
  group_by(up, timescale) %>%
  filter(timescale == "30mins") %>% 
  reframe(max_prop = max(prop),
    down1 = down[which(prop == max(prop))]) %>% 
  filter(max_prop != 0) %>% #, !down1 %in% c("r_12", "r_19", "r_20")) %>% 
  select(down1, up)

  # create the network object
network <- graph_from_data_frame(d=edges, directed=T) 

# plot it
plot(network,layout = layout_nicely(network), edge.arrow.size = 0.25,
     vertex.size = 15,
     vertex.label.cex = 1)

#greedy algorithm
mst <- mst(network)

    # Visualize the MST (optional)
    plot(mst, layout = layout_nicely(mst), vertex.label = V(mst)$name,
         edge.arrow.size = 0.25,
     vertex.size = 20,
     vertex.label.cex = 1)

components(network, mode = c("weak"))
components(mst, mode = c("strong"))


  
test %>% 
  group_by(up, timescale) %>%
  reframe(max_prop = max(prop),
    down1 = down[which(prop == max(prop))]) %>% 
  filter(max_prop != 0, !down1 %in% c("r_12", "r_19", "r_20")) %>% 
  group_by(down1, timescale) %>%
  summarise(count = length(down1)) %>% 
  pivot_wider(id_cols = down1, names_from = timescale, values_from = count)
```
What if I make a graph using every connection, and weighted by the prop value, then put it through the greedy algorithm?
```{r full-graph-W3}
#create full set of edges
edges2 <- test %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(weight = (1 - labels) + 0.01,
         labels = round(labels, 2)) %>%  #weight will be opposite prop value
  select(up, down, weight, labels)

write.csv(edges2, "edges2.csv")
edges2 <- read_csv("edges2.csv")

network <- graph_from_adjacency_matrix(edges_W3, mode = "directed", weighted = TRUE) 
E(network)$weight
# plot it
plot(network,layout = layout_nicely(network), edge.arrow.size = 0.25,
     vertex.size = 15,
     vertex.label.cex = 1)
is_weighted(network)
#greedy algorithm
mst <- mst(network, weights = E(network)$weight)

    # Visualize the MST (optional)
same.shape <- layout_nicely(mst)
V(mst)$name <- key2$sensor_ID
plot(
  mst,
  layout = layout_nicely(mst),
  #layout = layout_as_tree(mst),
  vertex.label = V(mst)$name,
  edge.arrow.size = 0.25,
  vertex.size = 5,
  vertex.label.cex = 0.75,
  edge.weight=2,
  #edge.width=(E(mst)$weight*500),
  #edge.color = E(mst)$color
  edge.label = E(mst)$labels,
  asp = 0.5
  #margin = c(rep(0.01,4))
)

p_load(SEMgraph, graph, Rgraphviz, RBGL)
als.npn <- transformData(alsData$exprs)$data

# graph-based trees
graph1 <- alsData$graph
seed <- V(graph1)$name[sample(1:vcount(graph1), 10)]
tree1 <- SEMtree(network, data = as_data_frame(network), seed=seed, type="CLE", verbose=TRUE)
tree1 <- SEMtree(graph1, data = als.npn, seed=seed, type="CLE", verbose=TRUE)

V <- colnames(als.npn)[colnames(als.npn) %in% V(graph1)$name]
tree3 <- SEMtree(NULL, als.npn, seed=V, type="CAT", verbose=TRUE)
tree4 <- SEMtree(NULL, als.npn, seed=V, type="CPDAG", alpha=0.05, verbose=TRUE)

#try implementing on my data
edges3 <- edges2 %>% 
  select(!labels) %>% 
  # mutate(up = substr(up, 3,4),
  #        down = substr(up, 3,4)) #%>% 
  pivot_wider(names_from = up, values_from = weight)

V <- V(network)$label
tree3 <- SEMtree(network, E(network)$weight, seed = seq(1, 26, 1), type="MST", verbose=TRUE)

tree3 <- SEMtree(network$graph, edges3, seed=V, type="CAT", verbose=TRUE)
tree3 <- SEMtree(network$graph, edges3, seed=V, type="MST", verbose=TRUE)

plot(tree3, layout = layout_as_tree(tree3))

#explore and understand
test %>% 
  filter(timescale == "30mins", up == "r_10") %>% View()
```

```{r figuring-out-ggplot-for-graphs}
p_load(ggnetwork, intergraph)

ggmst <- fortify(mst, layout = igraph::layout_nicely(mst))
ggplot(ggnetwork(edges, layout = "target", cell.jitter = 0.75),
       aes(x = x, y = y, xend = xend, yend = yend))+
  geom_edges()+
  geom_nodes()

ggplot(fortify(mst, layout = igraph::nicely()),
       aes(x = x, y = y, xend = xend, yend = yend))+
  geom_edges()+
  geom_nodes()

ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 8) +
  geom_nodetext(aes(label = name),
                fontface = "bold", color = "white", size = 3) +
  theme_blank()

as_adjacency_matrix(mst)


```

Try JP's idea, weight by difference in pk
```{r weighted-by-differences}
up_pks <- pks_w3 %>% 
  select(ID, pk) %>% 
  mutate(ID = paste0("r_",ID)) %>% 
  rename(up = ID,
         up_pk = pk)
down_pks <- pks_w3 %>% 
  select(ID, pk) %>% 
  mutate(ID = paste0("r_",ID)) %>% 
  rename(down = ID,
         down_pk = pk)

edges4 <- 
  test %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  left_join(up_pks, by = "up") %>% 
  left_join(down_pks, by = "down") %>% 
  mutate(diff = abs(down_pk - up_pk)) %>% 
  rename(weight = diff,
         label = prop) %>% 
  select(up, down, weight, label)

network <- graph_from_data_frame(d=edges4, directed=T) 
E(network)$weight
# plot it
plot(network,layout = layout_nicely(network), edge.arrow.size = 0.25,
     vertex.size = 15,
     vertex.label.cex = 1)
is_weighted(network)
#greedy algorithm
mst <- mst(network, weights = edges4$weight)

places <- canonical_permutation(mst)$labeling
sample(places)
permute(mst, sample(places))

tt <- names(V(mst))
tt[-which(tt == "r_22")]
which(tt == "r_13")
rearranged <- c(tt[1:which(tt == "r_13")-1], "r_22", c(tt[-which(tt == "r_22")][which(tt == "r_13"):25]))

new_arrange <- c(12,19,6,23,10,11,20,18,7,17,27,14,21,5,2,1,24,13,3,9,8,16,22,15,28,29)
new_arrange <- paste0("r_",new_arrange)
s <- sort(names(V(mst)))
mst2 <- permute(mst, match(V(mst)$name, new_arrange))

plot(
  mst2,
  #layout = layout_nicely(mst),
  layout = layout_as_tree(mst2),
  vertex.label = V(mst2)$name,
  edge.arrow.size = 0.25,
  vertex.size = 10,
  vertex.label.cex = 0.75,
  edge.weight=2
  #margin = c(rep(0.01,4))
)

ggmst <- fortify(mst2, layout = igraph::layout_as_tree(mst2))
  

ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 8) +
  geom_nodetext(aes(label = name),
                fontface = "bold", color = "white", size = 3) +
   geom_edgetext(aes(label = round(label,2)), color = "black") +

  theme_blank()

as_adjacency_matrix(mst2)

hist(ggmst$label)

```

```{r optrees}
#try optrees
install.packages("https://cran.r-project.org/src/contrib/Archive/optrees/optrees_1.0.tar.gz")
library(optrees)

V(network)$name
as_adjacency_matrix(network)


nodes <- 1:4
arcs <- matrix(c(1,2,2, 1,3,3, 1,4,4, 2,3,3, 2,4,4, 3,2,3,
                 3,4,1, 4,2,1, 4,3,2),byrow = TRUE, ncol = 3)

edges2 <- test %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(weight = (1 - labels) + 0.01,
         labels = round(labels, 2)) %>%  #weight will be opposite prop value
  select(up, down, weight, labels)

arcs <- 
  test %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  rename(weight = prop) %>% 
  mutate(up = as.numeric(substr(up, 3,4)),
         down = as.numeric(substr(down, 3,4)),
         weight = rescale(weight, to = c(1,5))) %>% 
  rename(old_nodes = up) %>% 
  left_join(key, by = "old_nodes") %>% 
  rename(new_nodes_up = new_nodes,
         up = old_nodes,
         old_nodes = down) %>% 
  left_join(key, by = "old_nodes") %>% 
  rename(new_nodes_down = new_nodes,
         down = old_nodes) %>% 
  select(new_nodes_up, new_nodes_down, weight) %>% 
  as.matrix() %>% 
  unname()

nodes <- as.numeric(substr(V(network)$name,3,4))
length(nodes)
nodes <- seq(1,26,1)

#need to re-label nodes so that they are a numeric sequence
key <- data.frame("new_nodes" = seq(1,26,1),
                  "old_nodes" = as.numeric(substr(V(network)$name,3,4)))



output <- getMinimumArborescence(nodes, arcs, source.node = 4)
msArborEdmonds(nodes, arcs)
output_frame <- as.data.frame(output[2])
colnames(output_frame) <- c("parent", "child", "weight")



treez <- graph_from_data_frame(d=output_frame, directed=T) 
ggtreez <- fortify(treez, layout = igraph::layout_as_tree(treez))
  

ggplot(ggtreez, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 8) +
  geom_nodetext(aes(label = name),
                fontface = "bold", color = "white", size = 3) +
  theme_blank()

as_adjacency_matrix(treez)

```

Iterate through this tree method, removing the most connected sensors
```{r unfinished-pruning-based-on-optrees}

nodes <- as.numeric(substr(V(network)$name,3,4))
which(nodes == 12)
old_nodes <- nodes[-which(nodes == 12)]
length(nodes)
nodes <- seq(1,25,1)

#need to re-label nodes so that they are a numeric sequence
key <- data.frame("new_nodes" = seq(1,25,1),
                  "old_nodes" = old_nodes)
arcs <- 
  edges2 %>% 
  filter(up != "r_12", down != "r_12") %>% 
  filter()
  mutate(up = as.numeric(substr(up, 3,4)),
         down = as.numeric(substr(down, 3,4)),
         weight = rescale(weight, to = c(1,5))) %>% 
  rename(old_nodes = up) %>% 
  left_join(key, by = "old_nodes") %>% 
  rename(new_nodes_up = new_nodes,
         up = old_nodes,
         old_nodes = down) %>% 
  left_join(key, by = "old_nodes") %>% 
  rename(new_nodes_down = new_nodes,
         down = old_nodes) %>% 
  select(new_nodes_up, new_nodes_down, weight) %>% 
  as.matrix() %>% 
  unname()

output2 <- getMinimumArborescence(nodes, arcs)
output_frame2 <- as.data.frame(output2[2])
colnames(output_frame2) <- c("parent", "child", "weight")



treez2 <- graph_from_data_frame(d=output_frame2, directed=T) 
ggtreez2 <- fortify(treez2, layout = igraph::layout_as_tree(treez2))
  

ggplot(ggtreez2, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 8) +
  geom_nodetext(aes(label = name),
                fontface = "bold", color = "white", size = 3) +
  theme_blank()

as_adjacency_matrix(treez2)

test %>% 
  filter(timescale == "30mins", up == "r_28") %>% View()
n <- 26
2^(n*(n-1))
```

5/22/25
After big meeting with JP and Kevin, determine the best chain, and see how that does in analysis
- simple path- does not visit the same node or vertex twice
```{r construction-the-perfect-chain}
#calculate the shortest, simple path
all_simple_paths
shortest_paths
all_shortest_paths()

shortest_paths(network, 1, 26, weights = network$weight, mode = "out",
                   algorithm = "dijkstra", inbound.edges = TRUE)

igraph::dfs(network, 2, mode = "total")
plot(subgraph(network, c(1:10)))

#igraph_widest_path_widths_dijkstra

#igraph_minimum_spanning_tree_prim 

sample_tree <- sample_spanning_tree(network)

plot(subgraph_from_edges(network, sample_tree))

distances(network)

install.packages("TSP")
library(TSP)

keyz <- 
  test %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))



edgesT <- 
  test %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()

# Convert to TSP object
set.seed(1)
atsp <- as.ATSP(edgesT)

# Insert a dummy node to get a path instead of a cycle
atsp <- insert_dummy(atsp, label = "dummy")
tsp1 <- reformulate_ATSP_as_TSP(atsp)

#how to view the distance matrix
as.dist(tsp1)
# Solve using nearest_insertion (heuristic that works with ATSPs)
tour <- solve_TSP(tsp1, method = "nn")
tour <- filter_ATSP_as_TSP_dummies(tour, atsp)
as.integer(tour)
#cheapest insertion gives the same answer every time I think, 
#nearest insertion is non-deterministic

# Extract path, removing dummy node
path <- as.integer(tour)
path <- path[labels(tour)[path] != "dummy"]

# Create a chain graph from the path
edges <- cbind(path[-length(path)], path[-1])
chain_graph <- graph_from_edgelist(edges, directed = TRUE)
plot(chain_graph)

#parent -> child

#convert my chain back into the sensor ids, run through scoring algorithm, OR just get histogram of the prop values from those combinations
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

  mutate(across(as.data.frame(edges)), key) 

as.data.frame(edges) |>
  left_join(key, by = c("new_nodes" = "V1")) |>
  mutate(V1 = coalesce(new_nodes, V1)) |>
  select(-new_nodes)
rew <- as.data.frame(edges)
ix <- match(rew$V1, key$new_nodes)
rew$updated_V1 <- ifelse(!is.na(ix), key$old_nodes[ix], rew$V1)

ix <- match(rew$V2, key$new_nodes)
rew$updated_V2 <- ifelse(!is.na(ix), key$old_nodes[ix], rew$V2)

```

```{r}
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

all_graph_combo <- calc_props(routes_graph, "W3") %>% 
    mutate("hierarchy" = "Graph chain")

all_graph_combo %>% 
  filter(timescale == "30mins") %>% 
  group_by(timescale) %>% 
  summarise(sum(prop))

all_pk_combo %>% 
  filter(timescale == "30mins", shed == "W3") %>% 
  group_by(timescale) %>% 
  summarise(sum(prop))

# all_pk_combo <- rbind(calc_props(routes_w3, "W3"),
#                 calc_props(routes_fb, "FB"),
#                 calc_props(routes_zz, "ZZ")) %>% 
#   mutate("hierarchy" = "Flow Permanence")

sults_so_far <- rbind(all_position_combo,
                      all_pk_combo) %>% 
  mutate("real_or_random" = "real") %>% 
  filter(shed == "W3") %>% 
  rbind(all_graph_combo)

  
shed_colors <- c("#397367", "#FFD166", "#7E6B8F")

#plots for committee meeting
sults_so_far %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = shed, color = shed), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = shed_colors,
                     
                     name = "Watershed")+
  scale_color_manual(values = shed_colors,
                     
                     name = "Watershed")+
  facet_grid(timescale~hierarchy)


all_graph_combo %>% filter(timescale == "30mins") %>% View()
```

Iterate through all methods to calculate a chain
```{r all-methods-from-docs}
methods <- c("identity", "random", "nearest_insertion",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn", "two_opt")

tours <- lapply(methods, FUN = function(m) filter_ATSP_as_TSP_dummies(solve_TSP(tsp1, method = m), atsp))
tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

names(tours) <- methods
```

5/25/25: Figured out that the key was wrong! My chains are actually being optimized correctly; make a chunk to test every method, and plot the results so I can use the best
```{r finding-best-method}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

methods <- c("random",
  "cheapest_insertion")

#set up edges and distance matrix, convert to TSP object
edgesT <- 
  test %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()

# Convert to TSP object
atsp <- as.ATSP(edgesT)

# reformat from atsp to tsp
tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  tour <- solve_TSP(atsp, method = methods[i])
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "W3") %>% 
    mutate("method" = methods[i])

    if(i == 1) all_chain_outputs <- chain_output
    if(i > 1) all_chain_outputs <- rbind(all_chain_outputs, chain_output)
}

#PLOT THE OUTPUT OF LOOP
shed_colors <- c("#397367", "#FFD166", "#7E6B8F")

#plots for committee meeting
all_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

```

```{r}
reference <- rbind(all_position_combo,all_pk_combo) %>% 
  filter(shed == "W3", timescale %in% c("30mins", "daily")) %>% 
  rename("method" = hierarchy)


all_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")) %>%
  rbind(reference) %>% 
  filter(method != "Relative Position") %>%  
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    #geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

#plot ecdf 
#Kevin thought that ecdf might look good!
all_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")) %>%
  rbind(reference) %>% 
  filter(method != "Relative Position") %>% 
  #filter(timescale == "30mins") %>% 
  ggplot(aes(prop, color = method)) +
  geom_hline(yintercept = 0.5, alpha = 0.5, color = "grey", lwd = 1)+
    geom_vline(xintercept = 0.5, alpha = 0.5, color = "grey", lwd = 1)+
  stat_ecdf(geom = "step")+
  theme_minimal()+
  labs(x = "Prop",
       y = "Percentage of values less than or equal",
       title = "How often do adjacent sensors follow a hierarchy?")+
  # scale_color_manual(values = real_colors,
  #                    name = "Organizing Scheme")+
  facet_grid(~timescale)#+
  lims(x = c(0.5,1),
       y = c(0.5, 1))
```


Try calculating transfer energy
```{r experimenting-transfer-energy}
p_load(RTransferEntropy)

shannon_te <- transfer_entropy(test2[,1], test2[,2])
shannon_te

calc_te(test2[,1], test2[,2])
calc_te(test2[,2], test2[,1])

```

###last task- generate random numbers, see if they are hierarchical
create an input that is randomly generated, and then compare it to the actual
```{r prepare-random inputs}
#create input that only uses sensors that were deployed during both deployments
#need to make a list of sensors deployed in both campaigns for each watershed
#W3
w3_deployed24 <- unique(filter(data_24, wshed == "W3")$number)
w3_deployed23 <- unique(filter(data_23, wshed == "W3")$ID)
W3_IDs <- intersect(w3_deployed24, w3_deployed23)
#FB
fb_deployed24 <- unique(filter(data_24, wshed == "FB")$number)
fb_deployed23 <- unique(filter(data_23, wshed == "FB")$ID)
FB_IDs <- intersect(fb_deployed24, fb_deployed23)
#ZZ
zz_deployed24 <- unique(filter(data_24, wshed == "ZZ")$number)
zz_deployed23 <- unique(filter(data_23, wshed == "ZZ")$ID)
ZZ_IDs <- intersect(zz_deployed24, zz_deployed23)


bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)


bind23 <- data_23 %>% 
  filter(ID %in% intersect(bind24$ID, data_23$ID)) %>% 
  select(datetime, ID, wshed, binary, mins)

#filter by each watershed, then recombine at the end
input_w3 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3") %>% 
  filter(ID %in% W3_IDs)
input_fb <- rbind(bind23, bind24) %>%
  filter(wshed == "FB") %>% 
  filter(ID %in% FB_IDs)
input_zz <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ") %>% 
  filter(ID %in% ZZ_IDs)

rbind(input_w3, input_fb, input_zz)

#create function that will generate a random series of 1 and 0, with the success probability equal to the proportion of time flowing
X <- rbinom(20, 1, 0.5)

#create new input_w3, and later specify that this is the randomly generated one
input_w3 <- input_w3 %>% 
  group_by(ID) %>% 
  summarise(duration = length(datetime)) %>% 
  left_join(input_w3, by = "ID") %>% 
  left_join(pks_w3, by = "ID") %>% 
  group_by(ID) %>% 
  mutate(random_state = rbinom(duration, 1, pk)) %>% 
  select(-duration, -wshed.y, -binary, -pk) %>% 
  rename("binary" = random_state, "wshed" = wshed.x)

input_fb <- input_fb %>% 
  group_by(ID) %>% 
  summarise(duration = length(datetime)) %>% 
  left_join(input_fb, by = "ID") %>% 
  left_join(pks_fb, by = "ID") %>% 
  group_by(ID) %>% 
  mutate(random_state = rbinom(duration, 1, pk)) %>% 
  select(-duration, -wshed.y, -binary, -pk) %>% 
  rename("binary" = random_state, "wshed" = wshed.x)

input_zz <- input_zz %>% 
  group_by(ID) %>% 
  summarise(duration = length(datetime)) %>% 
  left_join(input_zz, by = "ID") %>% 
  left_join(pks_zz, by = "ID") %>% 
  group_by(ID) %>% 
  mutate(random_state = rbinom(duration, 1, pk)) %>% 
  select(-duration, -wshed.y, -binary, -pk) %>% 
  rename("binary" = random_state, "wshed" = wshed.x)

      
```
```{r is-it-really-random?}
#durations that sensors were deployed
input_w3 %>% 
  group_by(ID) %>% 
  summarise(duration = length(datetime))

input_w3 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3") %>% 
  filter(ID %in% W3_IDs)

testing <- input_w3 %>% 
  group_by(ID) %>% 
  summarise(duration = length(datetime)) %>% 
  left_join(input_w3, by = "ID") %>% 
  left_join(pks_w3, by = "ID") %>% 
  group_by(ID) %>% 
  mutate(random_state = rbinom(duration, 1, pk)) %>% 
    filter(mins %in% c(0, 30)) %>%
  select(ID, datetime, binary, random_state) %>% 
  pivot_longer(cols = c(binary, random_state))

#figuring out if the randomly generated flows are actually different
testing %>% 
  #filter(DATETIME > start & DATETIME < stop) %>% 
  #filter(threshold == "agree_or_no_90") %>% 
  ggplot()+
  geom_tile(aes(x = datetime, y = ID, fill = as.character(value)))+
  facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("No flow", "flowing"),
                    name = ""
                    )+
  labs(title = "Streamflow permanence in W3",
       x = "")+
  theme_classic()+
    theme(legend.position="bottom")#+
  scale_x_break(c(("2023-11-10 10:00:00"), ("2024-5-10 10:00:00")))
```
####running random flow states
```{r relative-position-random}
#set routes for all 3 watersheds
routes_w3 <- read_csv("w3_flowrouting.csv") %>% 
  filter(sensor %in% W3_IDs, drains_from %in% W3_IDs) %>% 
  rename("up" = drains_from, "down" = sensor) %>% 
  select(up, down) %>% 
  filter(down != 100, up != 0)
  
routes_fb <- read_csv("fb_flowrouting.csv") %>% 
  filter(up %in% FB_IDs) %>% 
  filter(down %in% FB_IDs) %>% 
  drop_na()
routes_zz <- read_csv("zz_flowrouting.csv") %>% 
  filter(up %in% ZZ_IDs) %>% 
  filter(down %in% ZZ_IDs) %>% 
  drop_na()

#run calc_support for all sheds and timesteps for relative position
all_position_random <- rbind(calc_props(routes_w3, "W3"),
                      calc_props(routes_fb, "FB"),
                      calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Relative Position")
#write_csv(all_position, "./hierarchy_analysis_results/relativePosition.csv")
#test <- read_csv("./hierarchy_analysis_results/relativePosition.csv")
```
```{r flow-permanence-random}
#randomly genrated inputs
input_w3 <- input_w3 %>% 
  group_by(ID) %>% 
  summarise(duration = length(datetime)) %>% 
  left_join(input_w3, by = "ID") %>% 
  left_join(pks_w3, by = "ID") %>% 
  group_by(ID) %>% 
  mutate(random_state = rbinom(duration, 1, pk)) %>% 
  select(-duration, -wshed.y, -binary, -pk) %>% 
  rename("binary" = random_state, "wshed" = wshed.x)

input_fb <- input_fb %>% 
  group_by(ID) %>% 
  summarise(duration = length(datetime)) %>% 
  left_join(input_fb, by = "ID") %>% 
  left_join(pks_fb, by = "ID") %>% 
  group_by(ID) %>% 
  mutate(random_state = rbinom(duration, 1, pk)) %>% 
  select(-duration, -wshed.y, -binary, -pk) %>% 
  rename("binary" = random_state, "wshed" = wshed.x)

input_zz <- input_zz %>% 
  group_by(ID) %>% 
  summarise(duration = length(datetime)) %>% 
  left_join(input_zz, by = "ID") %>% 
  left_join(pks_zz, by = "ID") %>% 
  group_by(ID) %>% 
  mutate(random_state = rbinom(duration, 1, pk)) %>% 
  select(-duration, -wshed.y, -binary, -pk) %>% 
  rename("binary" = random_state, "wshed" = wshed.x)
#make list that make pairs of sites based on local persistency
routes_w3 <- pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- pks_fb %>%
    filter(ID %in% FB_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- pks_zz %>%
    filter(ID %in% ZZ_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

all_pk_random <- rbind(calc_props(routes_w3, "W3"),
                calc_props(routes_fb, "FB"),
                calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Flow Permanence")

#write_csv(all_pk, "./hierarchy_analysis_results/flowPermanence.csv")
```
```{r drainage-area-random}
#I was not confident in my R determined values, so I extracted drainage area in Arc. Reading in results

#.csv files with extracted 1m drainage area values, but also contain snapped coords in UTM
fb_uaa_1m <- read_csv("./STIC_uaa/fb2.csv")
zz_uaa_1m <- read_csv("./STIC_uaa/zz2.csv")
w3_uaa_1m <- read_csv("./STIC_uaa/w32.csv")

routes_w3 <- w3_uaa_1m %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_uaa_1m %>%
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_zz <- zz_uaa_1m %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>%
  arrange(desc(uaa)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_uaa_random <- rbind(calc_props(routes_w3, "W3"),
                 calc_props(routes_fb, "FB"),
                 calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Drainage Area")

#write_csv(all_uaa, "./hierarchy_analysis_results/drainageArea.csv")
```
```{r topographic-wetness-index-random}
routes_w3 <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_twi_random <- rbind(calc_props(routes_w3, "W3"),
                 calc_props(routes_fb, "FB"),
                 calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Topographic Wetness Index")

#write_csv(all_twi, "./hierarchy_analysis_results/twi.csv")
```
```{r slope-random}
routes_w3 <- w3_slope %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(slope) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_slope %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange((slope)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_slope %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange((slope)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_slope_random <- rbind(calc_props(routes_w3, "W3"),
                 calc_props(routes_fb, "FB"),
                 calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Slope")

#write_csv(all_slope, "./hierarchy_analysis_results/slope.csv")
```

```{r plot-random-results}
sults_so_far_random <- rbind(all_position_random,
                      all_pk_random,
                      #all_nt_combo,
                      all_uaa_random
                      #all_tpi_combo,
                      #all_slope_random,
                      #all_twi_random
                      ) %>% 
  mutate("real_or_random" = "random")#%>% 
  mutate(timescale = fct_relevel(timescale,
                              c("30mins", "hourly", "4hr", "daily")),
         hierarchy = as.factor(str_trim(as.character(hierarchy))),
         
         hierarchy = fct_relevel(hierarchy, 
                                 c(
                                "Flow Permanence",
                                #"Number of Transitions",
                                "Relative Position",
                                "Drainage Area"
                                #"Slope",
                                #"Topographic Wetness Index",
                                #"Topographic Position Index"
                                )))#,
                                #"Random")))

shed_colors <- c("#397367", "#FFD166", "#7E6B8F")

#plots for committee meeting
sults_so_far_random %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = shed, color = shed), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "Randomly generated flow values",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = shed_colors,
                     
                     name = "Watershed")+
  scale_color_manual(values = shed_colors,
                     
                     name = "Watershed")+
  facet_grid(timescale~hierarchy)


sults_so_far_random %>% 
  filter(timescale == "30mins") %>% 
  ggplot(aes(prop, color = shed)) +
  stat_ecdf(geom = "line")+
  theme_minimal()+
    #geom_point(position = position_jitterdodge(jitter.width = 0.2), alpha = 0.5)+
  #geom_hline(yintercept = 0.5, lty = 2)+
    #geom_vline(xintercept = 0.5, lty = 2)+

  labs(x = "Prop",
       y = "Percentage of values less than or equal",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = shed_colors,
                     name = "Organizing Scheme")+
  facet_grid(timescale~hierarchy)#+
  lims(x = c(0.5,1),
       y = c(0.5, 1))

```

```{r ks-test}
#testing whether randomly generated prop values distribution is different than the actual one
sults_so_far_random %>% filter(timescale == "30mins", shed == "W3", hierarchy == "Flow Permanence")%>% select(prop)

real <- sults_so_far %>% filter(timescale == "30mins", shed == "W3", hierarchy == "Relative Position")
random <- sults_so_far_random %>% filter(timescale == "30mins", shed == "W3", hierarchy == "Relative Position")
ks.test(real$prop, 
        random$prop,
        alternative = c("two.sided"),
        exact = TRUE, simulate.p.value = TRUE, B = 2000)[2]

left_join(sults_so_far, sults_so_far_random, by = c("up", "down", "timescale", "shed", "hierarchy")) %>% 
  group_by(shed, hierarchy, timescale) %>% 
  summarise(
    # ks.test(prop.x, 
    #     prop.y,
    #     alternative = c("two.sided"),
    #     exact = TRUE, simulate.p.value = TRUE, B = 2000)[1],
       p_value= ks.test(prop.x, 
        prop.y,
        alternative = c("two.sided"),
        exact = TRUE, simulate.p.value = TRUE, B = 2000)[2]) %>% 
  filter(p_value <= 0.1) %>% 
  knitr::kable()
```

####More randomness, flow permanence was randomly generated
```{r figuring-it-out}
p_load(fitdistrplus)
rbind(pks_w3, pks_fb, pks_zz) %>% 
  ggplot(aes(x = wshed, y = pk))+
  geom_boxplot()+
  geom_jitter(width = 0.1, alpha = 0.5)+
  theme_classic()+
  labs(title = "Distributions of Proportion of Time Flowing",
       x = "Watershed",
       y = "Proportion of Time Flowing")

runif(30, min = min(pks_w3$pk), max = max(pks_w3$pk))

#Botter and Durighetto used a normal and beta distribution to randomly sample pk values for a simulation with 1000 nodes
#use beta distribution for all of mine
hist(rbeta(10000,5,2))
hist(rbeta(10000,2,5))
hist(rbeta(10000,5,5))
p_load(fitdistrplus)
fitdistr(pks_w3$pk, "beta",
         start = list(shape1 = 2, shape2 = 2))
hist(rbeta(1000,2,2))
hist(rbeta(1000,1.1831220,0.9849231))

fitdistr(pks_fb$pk, 
         start = list(shape1 = 1, shape2 = 1), densfun="beta")

#determining which distribution to use
plotdist(pks_w3$pk, histo = TRUE, demp = TRUE)
descdist(pks_w3$pk, boot = 1000)

fu <- fitdist(pks_w3$pk, "uniform")
fn <- fitdist(groundbeef$serving, "normal")
fb <- fitdist(pks_w3$pk, "beta")
plot(fitdist(pks_fb$pk, "beta"))
plot(fitdist(pks_zz$pk, "beta"))

plot(fitdist(pks_w3$pk, "unif"))
plot(fitdist(pks_w3$pk, "norm"))

plot(fitdist(pks_fb$pk, "weibull"))
plot(fitdist(pks_w3$pk, "weibull"))
plot(fitdist(pks_zz$pk, "weibull"))

un <- fitdist(pks_w3$pk, "unif")
nl <- fitdist(pks_w3$pk, "norm")
wb <- fitdist(pks_w3$pk, "weibull")
fb <- fitdist(pks_w3$pk, "beta")
plot.legend <- c("uniform", "normal", "weibull", "beta")
denscomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
qqcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
cdfcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
ppcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()

fitdistrplus::plotdist(pks_w3$pk, "beta", list(1.1831220, 0.9849231))
fitdistrplus::plotdist(pks_fb$pk, "beta", list(5, 1))

plotdist('beta', params=list( 3, 10), kind='density')
plotdist('beta', params=list( 3, 10), kind='cdf')

input_w3 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3") %>% 
  filter(ID %in% W3_IDs)

testing <- input_w3 %>% 
  group_by(ID) %>% 
  summarise(duration = length(datetime)) %>% 
  left_join(input_w3, by = "ID") %>% 
  left_join(pks_w3, by = "ID") %>% 
  group_by(ID) %>% 
  mutate(random_state = rbinom(duration, 1, pk)) %>% 
    filter(mins %in% c(0, 30)) %>%
  select(ID, datetime, binary, random_state) %>% 
  pivot_longer(cols = c(binary, random_state))
```

```{r w3-fit-distribution}
p_load(patchwork, fitdistrplus)
dataset <- pks_w3$pk
plotdist(dataset, histo = TRUE, demp = TRUE)

un <- fitdist(dataset, "unif")
plot(un)
nl <- fitdist(dataset, "norm")
wb <- fitdist(dataset, "weibull")
fb <- fitdist(dataset, "beta")
plot.legend <- c("uniform", "normal", "weibull", "beta")
dens <- denscomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
qq <- qqcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
cdf <- cdfcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
pp <- ppcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()

(dens + qq) / (cdf + pp)
w3_distribution <- fitdist(dataset, "unif")
```
```{r fb-fit-distribution}
dataset <- pks_fb$pk
fb <- fitdist(dataset, "beta")
fb_distribution <- fitdist(dataset, "beta")

plotdist(dataset, histo = TRUE, demp = TRUE)

plot(fb)

plot.legend <- c("beta")
denscomp(list(fb), legendtext = plot.legend, plotstyle = "graphic")
qqcomp(list(nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
cdfcomp(list(nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
ppcomp(list(nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
```
```{r zz-fit-distribution}
dataset <- pks_zz$pk
plotdist(dataset, histo = TRUE, demp = TRUE)

un <- fitdist(dataset, "unif")
plot(fb)
nl <- fitdist(dataset, "norm")
wb <- fitdist(dataset, "weibull")
fb <- fitdist(dataset, "beta")
plot.legend <- c("uniform", "normal", "weibull", "beta")
denscomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
qqcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
cdfcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
ppcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()

dens <- denscomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
qq <- qqcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
cdf <- cdfcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()
pp <- ppcomp(list(un, nl, wb, fb), legendtext = plot.legend, plotstyle = "ggplot")+
  theme_classic()

(dens + qq) / (cdf + pp)

zz_distribution <- fitdist(dataset, "beta")

```

```{r randomly-sample-pk-distributions}
#randomly sample pk distributions

runif(30, min = w3_distribution$estimate[1], max = w3_distribution$estimate[2])
rbeta(30, shape1 = fb_distribution$estimate[1], shape2 = fb_distribution$estimate[2])
rbeta(30, shape1 = zz_distribution$estimate[1], shape2 = zz_distribution$estimate[2])

#original/actual input datasets
input_w3 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3") %>% 
  filter(ID %in% W3_IDs)
input_fb <- rbind(bind23, bind24) %>%
  filter(wshed == "FB") %>% 
  filter(ID %in% FB_IDs)
input_zz <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ") %>% 
  filter(ID %in% ZZ_IDs)

timeframe <- input_w3 %>% 
  filter(ID == 2, mins %in% c(0, 30))

length(timeframe$datetime)
#W3
pks_w3_random <- runif(30, min = w3_distribution$estimate[1], max = w3_distribution$estimate[2])

for(z in 1:length(pks_w3_random)){
  flows <- rbinom(length(timeframe$datetime), 1, pks_w3_random[z])
  sheep_w3 <- tibble("datetime" = timeframe$datetime) %>% 
  mutate("ID" = z,
         "binary" = flows)
    if(z == 1) big_sheep <- sheep_w3
    if(z > 1) big_sheep <- rbind(big_sheep, sheep_w3)
}

input_w3 <- big_sheep %>% 
  mutate(wshed = "W3",
         mins = minute(datetime))

pks_fb_random <- rbeta(30, shape1 = fb_distribution$estimate[1], shape2 = fb_distribution$estimate[2])

for(z in 1:length(pks_fb_random)){
  flows <- rbinom(length(timeframe$datetime), 1, pks_fb_random[z])
  sheep_fb <- tibble("datetime" = timeframe$datetime) %>% 
  mutate("ID" = z,
         "binary" = flows)
    if(z == 1) big_sheep <- sheep_fb
    if(z > 1) big_sheep <- rbind(big_sheep, sheep_fb)
}

input_fb <- big_sheep %>% 
  mutate(wshed = "FB",
         mins = minute(datetime))

pks_zz_random <- rbeta(30, shape1 = zz_distribution$estimate[1], shape2 = zz_distribution$estimate[2])

for(z in 1:length(pks_zz_random)){
  flows <- rbinom(length(timeframe$datetime), 1, pks_zz_random[z])
  sheep_fb <- tibble("datetime" = timeframe$datetime) %>% 
  mutate("ID" = z,
         "binary" = flows)
    if(z == 1) big_sheep <- sheep_fb
    if(z > 1) big_sheep <- rbind(big_sheep, sheep_fb)
}

input_zz <- big_sheep %>% 
  mutate(wshed = "ZZ",
         mins = minute(datetime))
```

```{r flow-permanence-random}
#make list that make pairs of sites based on local persistency
#library(MASS, exclude = "select")
detach("package:fitdistrplus")
detach("package:MASS")
routes_w3 <- data.frame("ID" = seq(1, 30, 1),
                    "pk" = pks_w3_random )%>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- data.frame("ID" = seq(1, 30, 1),
                    "pk" = pks_fb_random )%>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  dplyr::select(up, down)

routes_zz <- data.frame("ID" = seq(1, 30, 1),
                    "pk" = pks_zz_random )%>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  dplyr::select(up, down)

all_pk_random2 <- rbind(calc_props(routes_w3, "W3"),
                calc_props(routes_fb, "FB"),
                calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Flow Permanence",
         "real_or_random" = "random")

#write_csv(all_pk, "./hierarchy_analysis_results/flowPermanence.csv")
```
Chunk to plot totally random pk versus the actual, see how it does
```{r plot-results}
#" 
real_colors <- c("#397367","#FFD166")#, "#7E6B8F")


just2 <- sults_so_far %>% 
  filter(hierarchy == "Flow Permanence") %>% 
  mutate("real_or_random" = "real") %>% 
  rbind(all_pk_random2) 
just2 %>% 
  #filter(timescale %in% c("30mins", "daily")) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = real_or_random, color = real_or_random), alpha = 0.5)+
    #geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 8.75))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "Sequence based on real proportion of time flowing, real works better than random?",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = real_colors,
                     
                     name = "Real or Random?")+
  scale_color_manual(values = real_colors,
                     
                     name = "Real or Random?")+
  facet_grid(shed~timescale)


#Kevin thought that ecdf might look good!
just2 %>% 
  #filter(timescale == "30mins") %>% 
  ggplot(aes(prop, color = real_or_random)) +
  geom_hline(yintercept = 0.5, alpha = 0.5, color = "grey", lwd = 1)+
    geom_vline(xintercept = 0.5, alpha = 0.5, color = "grey", lwd = 1)+
  stat_ecdf(geom = "line")+
  theme_minimal()+
  labs(x = "Prop",
       y = "Percentage of values less than or equal",
       title = "How often do adjacent sensors follow a hierarchy?")+
  scale_color_manual(values = real_colors,
                     name = "Organizing Scheme")+
  facet_grid(~timescale)#+
  lims(x = c(0.5,1),
       y = c(0.5, 1))
```

###additional last task- determine minimum number of observations to get "true" proportion of time flowing
```{r}
#function from W3_STIC, 
boot <- function(shed, iterations, percent_retained){
  out <- seq(1, iterations, 1)

  one_shed <- filter(all_sheds2, 
                     wshed == shed)
  #for loop to bootstrap n = iterations times
  for(i in 1:iterations){
  
    IDs <- unique(one_shed$ID)
    set.seed(i)
    sampled <- sample(IDs, round(percent_retained * length(IDs)))
  
    pruned <- one_shed %>% 
      filter(ID %in% sampled)

    #seasonal flow permanence of whole watershed for
    perc_flow <- length(pruned$wetdry[pruned$wetdry == "wet"]) / length(pruned$wetdry)
    out[i] <- perc_flow
  }

  avg <- mean(out)
  sd <- sd(out)

final <- paste0(signif(avg, 2)," +/- ",signif(sd, 2))
}

w3_boot <- boot("W3", 100, 0.9)
```

#Determine the wettest and driest times for each watershed
```{r determine-when-it-goes-from-all-dry-to-all-wet}
#make binary columns for wet and dry
rbind(input_w3, input_fb, input_zz)


df1 <- input_w3 %>% 
  group_by(datetime) %>% 
  summarise("percent_flowing" = sum(binary)/length(binary)) %>% 
  filter(percent_flowing <= 0.1) %>% 
  mutate(date = date(datetime)) %>% 
  select(-datetime, -percent_flowing) %>% unique() %>% 
  rowid_to_column("dry_ID") %>% 
  rename("dry_date" = date)

df2 <- input_w3 %>% 
  group_by(datetime) %>% 
  summarise("percent_flowing" = sum(binary)/length(binary)) %>% 
  filter(percent_flowing >= 0.9) %>% 
  mutate(date = date(datetime)) %>% 
  select(-datetime, -percent_flowing) %>% unique()%>% 
  rowid_to_column("wet_ID") %>% 
  rename("wet_date" = date)

#dataframe that tells us when the network went from fully dry to fully wet and vice versa
crossed_up <- cross_join(df1, df2) %>% 
  mutate(difference = wet_date - dry_date) %>% 
  group_by(dry_date) %>% 
  filter(abs(difference) == min(abs(difference))) %>% 
  filter(abs(difference) <= 10 )
  #summarise(min = min(difference),
   #         max = max(difference)


f1 <- Vectorize(function(x, y) list(as.numeric(difftime(y, x, units = "d"))))
outer(df1$date, df2$date, FUN = f1)

#maximum percentage of sensors flowing is 100%
max(spots_flowing$percent_flowing)
#times when stream was at it's maximum extent
spots_flowing$datetime[spots_flowing$percent_flowing == max(spots_flowing$percent_flowing)]
#minimum extent times
min(spots_flowing$percent_flowing)
no_flow <- spots_flowing$datetime[spots_flowing$percent_flowing == min(spots_flowing$percent_flowing)]


filter(q_23_f, DATETIME %in% no_flow)


spots_flowing$datetime[spots_flowing$percent_flowing == min(spots_flowing$percent_flowing)]

#plots for the wettest times for each watershed
plot_test <- filter(all_w3, 
               datetime == "2023-07-03 15:20:00 EDT")

ggplot()+
  geom_tile(data=test, aes(x=x, y=y), fill = "lightgrey")+
  geom_point(data = plot_test, aes(x = long, y = lat, color = wetdry), size = 2)+
  scale_color_manual(drop = FALSE,
                     values = binary,
                    breaks = c("dry", "wet"),
                    labels = c("No flow", "flowing"),
                    name = ""
                    )+
  geom_text(data = plot_test, aes(x = long, y = lat,label = ID), hjust=-0.00025, vjust=0.00025)+
  theme_void()+
  labs(title = "Wettest extent in W3")+
  coord_equal()
#plot of the driest extent in W3

plot_test <- filter(all_w3, 
               datetime == "2023-09-21 16:00:00 EDT")
ggplot()+
  geom_tile(data=test, aes(x=x, y=y), fill = "lightgrey")+
  geom_point(data = plot_test, aes(x = long, y = lat, color = wetdry), size = 2)+
  scale_color_manual(drop = FALSE,
                     values = binary,
                    breaks = c("dry", "wet"),
                    labels = c("No flow", "flowing"),
                    name = ""
                    )+
  geom_text(data = plot_test, aes(x = long, y = lat,label = ID), hjust=-0.00025, vjust=0.00025)+
  theme_void()+
  labs(title = "Driest extent in W3")+
  coord_equal()
```

```{r first-fully-dry-to-wet}
 # Function factory for secondary axis transforms
train_sec <- function(primary, secondary, na.rm = TRUE) {
  # Thanks Henry Holm for including the na.rm argument!
  from <- range(secondary, na.rm = na.rm)
  to   <- range(primary, na.rm = na.rm)
  # Forward transform for the data
  forward <- function(x) {
    rescale(x, from = from, to = to)
  }
  # Reverse transform for the secondary axis
  reverse <- function(x) {
    rescale(x, from = to, to = from)
  }
  list(fwd = forward, rev = reverse)
}

# sec <- with(economics, train_sec(unemploy, psavert))
# 
# ggplot(economics, aes(date)) +
#   geom_line(aes(y = unemploy), colour = "blue") +
#   geom_line(aes(y = sec$fwd(psavert)), colour = "red") +
#   scale_y_continuous(sec.axis = sec_axis(~sec$rev(.), name = "psavert"))

#add precipitation
#add hydrograph to plot
q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

    

start <- ymd_hms("2023-10-20 00:00:00")
stop <- ymd_hms("2023-10-22 00:00:00")

#precip <- 
  read_csv("./HB/dailyWatershedPrecip1956-2024.csv") %>% 
    filter(watershed == "W3") %>% 
        filter(DATE >= start & DATE <= stop) %>% 
    ggplot()+
    geom_bar(aes(x = DATE, y = Precip), stat = "identity")

precip1 <- read_csv("./HB/HBEF_W3precipitation_15min.csv") %>% 
    filter(DateTime >= start & DateTime <= stop) %>% 
    mutate(day = day(DateTime),
           hour = hour(DateTime)) %>% 
    group_by(day, hour) %>% 
    reframe(hourly_precip = sum(precip), across()) %>% 
    mutate(mins = minute(DateTime)) %>% 
    filter(mins == 0) %>%
  rename("datetime" = DateTime) %>% 
    left_join(q_23_plotting, by = "datetime") %>% 
  select(datetime, hourly_precip, Q_mm_day)


sec3 <- with(precip1, train_sec(hourly_precip, Q_mm_day))


water <- precip1 %>% 
    ggplot(aes(x = datetime))+
    geom_col(aes(y = hourly_precip))+
    geom_line(aes(y = sec3$fwd(Q_mm_day)), colour = "blue") +
    scale_y_continuous("Hourly Precipitation (mm)",
                       sec.axis = sec_axis(~sec3$rev(.),
                       name = "Discharge (mm/day)"))+
    labs(x = "",
         y = "Precipitation (mm)")+
      theme_classic()+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y = element_text(angle = 0),
        axis.title.y.right = element_text(angle = 0)
        )


test_IDS <- input_w3 %>% 
    filter(datetime > start & datetime < stop)

remainder <- test_IDS$ID

pks_ordered <- pks_w3 %>% 
filter(ID %in% remainder) %>% 
  arrange(desc(pk)) %>% 
  rowid_to_column("pk_order")

test <- input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
sec2 <- with(test, train_sec(pk_order, pk))

test <- 
  input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
    mutate(day = day(datetime),
           hour = hour(datetime)) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(precip1, by = c("day", "hour")) %>% 
  left_join(pks_ordered, by = "ID") %>% 
  filter(mins.x == 0)
test2 <- test %>% 
  select(datetime.x, binary, hourly_precip, pk_order)
sec2 <- with(test2, train_sec(pk_order, hourly_precip))



tiles <- test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = pk_order, fill = as.character(binary)))+
  scale_y_continuous("Supposed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23"))+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 0))

p_load(patchwork)

(water)/tiles + plot_layout(heights = c(1, 5))


test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = (pk_order), fill = as.character(binary)))+
  geom_col(aes(y = sec2$fwd(hourly_precip)))+
  ylab("Supposed activation order")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$fwd(.), name = "Instantaneous Q (mm/day)")) +
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(title = "Very dry to very wet",
       subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23"))+
  theme_classic()+
    theme(legend.position="right")


```
```{r second}


d <- 1
start <- ymd_hms(paste0("2023-07-15 00:00:00"))
stop <- ymd_hms(paste0(crossed_up$wet_date[d]," 00:00:00"))
#add precipitation
#add hydrograph to plot
q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

    




precip1 <- read_csv("./HB/HBEF_W3precipitation_15min.csv") %>% 
    filter(DateTime >= start & DateTime <= stop) %>% 
    mutate(day = day(DateTime),
           hour = hour(DateTime)) %>% 
    group_by(day, hour) %>% 
    reframe(hourly_precip = sum(precip), across()) %>% 
    mutate(mins = minute(DateTime)) %>% 
    filter(mins == 0) %>%
  rename("datetime" = DateTime) %>% 
    left_join(q_23_plotting, by = "datetime") %>% 
  select(datetime, hourly_precip, Q_mm_day)


sec3 <- with(precip1, train_sec(hourly_precip, Q_mm_day))


water <- precip1 %>% 
    ggplot(aes(x = datetime))+
    geom_col(aes(y = hourly_precip))+
    geom_line(aes(y = sec3$fwd(Q_mm_day)), colour = "blue") +
    scale_y_continuous("Hourly Precipitation (mm)",
                       sec.axis = sec_axis(~sec3$rev(.),
                       name = "Discharge (mm/day)"))+
    labs(x = "",
         y = "Precipitation (mm)")+
      theme_classic()+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y = element_text(angle = 0),
        axis.title.y.right = element_text(angle = 0)
        )


test_IDS <- input_w3 %>% 
    filter(datetime > start & datetime < stop)

remainder <- test_IDS$ID

pks_ordered <- pks_w3 %>% 
filter(ID %in% remainder) %>% 
  arrange(desc(pk)) %>% 
  rowid_to_column("pk_order")


#sec2 <- with(test, train_sec(pk_order, pk))

test <- 
  input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
    filter(mins == 0) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(precip1, by = "datetime") %>% 
  left_join(pks_ordered, by = "ID") 

sec2 <- with(test, train_sec(pk_order, hourly_precip))



tiles <- test %>% 
  ggplot(aes(x = datetime))+
  geom_tile(aes(y = pk_order, fill = as.character(binary)))+
  scale_y_continuous("Supposed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
# scale_x_continuous(breaks=c(ymd_hms("2023-07-20 00:00:00"),
#                             ymd_hms("2023-07-21 00:00:00"),
#                             ymd_hms("2023-07-22 00:00:00")),
#                    labels = c("7/20/23",
#                               "7/21/23",
#                               "7/22/23"))+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 0))


(water)/tiles + plot_layout(heights = c(1, 5))




```
```{r old-plot-presence/absence-with-discharge}

d <- 1
start <- ymd_hms(paste0(crossed_up$dry_date[d]," 00:00:00"))
stop <- ymd_hms(paste0(crossed_up$wet_date[d]," 00:00:00"))


test_IDS <- input_w3 %>% 
    filter(datetime > start & datetime < stop)

remainder <- test_IDS$ID

pks_ordered <- pks_w3 %>% 
filter(ID %in% remainder) %>% 
  arrange(desc(pk)) %>% 
  rowid_to_column("pk_order")

q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()


test <- input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(q_23_plotting, by = "datetime") %>% 
  left_join(pks_ordered, by = "ID")
sec2 <- with(test, train_sec(pk_order, Q_mm_day))

test %>% 
  ggplot(aes(x = datetime))+
  geom_tile(aes(y = (pk_order), fill = as.character(binary)))+
      geom_line(aes(y = sec2$fwd(Q_mm_day)), color = "white")+
  ylab("Supposed activation order")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$rev(.), name = "Instantaneous Q (mm/day)")) +
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(title = "Very dry to very wet",
       subtitle = "W3, 7/20 - 7/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-07-20 00:00:00"),
                            ymd_hms("2023-07-21 00:00:00"),
                            ymd_hms("2023-07-22 00:00:00")),
                   labels = c("7/20/23",
                              "7/21/23",
                              "7/22/23"))+
  theme_classic()+
    theme(legend.position="right")

#add hydrograph to plot
crossed_up
```
##Add maps of flowing condition throughout watershed at points during rising/falling limb

#Test hierarchies spanning all three watersheds
```{r prepare-combined-input}
#create input that only uses sensors that were deployed during both deployments
#need to make a list of sensors deployed in both campaigns for each watershed
#W3
w3_deployed24 <- unique(filter(data_24, wshed == "W3")$number)
w3_deployed23 <- unique(filter(data_23, wshed == "W3")$ID)
W3_IDs <- intersect(w3_deployed24, w3_deployed23)
#FB
fb_deployed24 <- unique(filter(data_24, wshed == "FB")$number)
fb_deployed23 <- unique(filter(data_23, wshed == "FB")$ID)
FB_IDs <- intersect(fb_deployed24, fb_deployed23)
#ZZ
zz_deployed24 <- unique(filter(data_24, wshed == "ZZ")$number)
zz_deployed23 <- unique(filter(data_23, wshed == "ZZ")$ID)
ZZ_IDs <- intersect(zz_deployed24, zz_deployed23)


bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)


bind23 <- data_23 %>% 
  filter(ID %in% intersect(bind24$ID, data_23$ID)) %>% 
  select(datetime, ID, wshed, binary, mins)

#filter by each watershed, then recombine at the end
input_w3 <- 
  rbind(bind23, bind24) %>%
  filter(wshed == "W3") %>% 
  filter(ID %in% W3_IDs) %>% 
  mutate(ID = paste0("W3_",ID)) %>% 
  select(-wshed)
input_fb <- 
  rbind(bind23, bind24) %>%
  filter(wshed == "FB") %>% 
  filter(ID %in% FB_IDs) %>% 
  mutate(ID = paste0("FB_",ID)) %>% 
  select(-wshed)
input_zz <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ") %>% 
  filter(ID %in% ZZ_IDs) %>% 
  mutate(ID = paste0("ZZ_",ID)) %>% 
  select(-wshed)

big_input <- rbind(input_w3, input_fb, input_zz) %>% 
  pivot_wider(names_from = ID, values_from = binary)
```
```{r all-combos-functions}
input <- big_input
calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  sequence_df <- data.frame("Sequence" = NA, 
                            "Frequency" = NA,
                            "up" = up,
                            "down" = down)
  return(sequence_df)
} 
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")


sub <- filter(sequence_df, Sequence %in% supports)
output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)}
}

#test function
calc_support_combos("ZZ_27", "FB_14", big_input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_combos <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support_combos(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

iterate_groups("ZZ_27", "FB_14", big_input, minutes(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_combos <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- routes$up[x]
  down <- routes$down[x]
  #print(x)
  
  out <- iterate_groups_combos(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down, Sequence) %>%
    summarise(Frequency = sum(Frequency))
  return(final_output)
}

calculate_hierarchy_combos(big_routes, big_input, minutes(30))

fantastic_four_combos <- function(routes, shed){
  theFour <- c("30mins", "hourly", "4hr", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- big_input %>%
      filter(mins %in% c(0, 30))
    timestep <- minutes(30)
  } 
  else if(timescale == "hourly"){
    input <- big_input %>%
      filter(mins %in% c(0))
    timestep <- hours(1)
  } 
  else if(timescale == "4hr"){
    input <- big_input %>%
      mutate(hour = hour(datetime)) %>% 
      filter(hour %in% c(0,4,8,12,16,20,24), mins %in% c(0))
    timestep <- hours(4)
  } 
  else if(timescale == "daily"){
    input <- big_input %>%
      mutate(hour = hour(datetime)) %>% 
      filter(hour %in% c(12), mins %in% c(0))
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_combos(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  
  return(fanfar)
}

routes_w3 <- read_csv("w3_flowrouting.csv") %>% 
  filter(sensor %in% W3_IDs, drains_from %in% W3_IDs) %>% 
  rename("up" = drains_from, "down" = sensor) %>% 
  select(up, down) %>% 
  filter(down != 100, up != 0)

#test <- fantastic_four_combos(routes_w3, "W3")


#must calculate prop values after and outside of fantastic 4
calc_props <- function(routes){
  full_combos <- fantastic_four_combos(routes)
total_state_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    group_by(up, down, timescale) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down, timescale) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down", "timescale")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
return(un_split)
}

```
```{r pk}
routes_w3 <- 
  pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  mutate(ID = paste0("W3_",ID)) %>% 
  select(-wshed)

routes_fb <- 
  pks_fb %>%
    filter(ID %in% FB_IDs) %>%
  mutate(ID = paste0("FB_",ID)) %>% 
  select(-wshed)
  #filter(pk != 1) %>% 
  
routes_zz <- 
  pks_zz %>%
    filter(ID %in% ZZ_IDs) %>% 
    mutate(ID = paste0("ZZ_",ID)) %>% 
  select(-wshed)
big_routes <- 
  rbind(routes_w3, routes_fb, routes_zz) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

big_output_pk <- calc_props(big_routes) %>% 
  mutate("hierarchy" = "Proportion of Time Flowing")

```
```{r drainage-area}
#I was not confident in my R determined values, so I extracted drainage area in Arc. Reading in results

#.csv files with extracted 1m drainage area values, but also contain snapped coords in UTM
fb_uaa_1m <- read_csv("./STIC_uaa/fb2.csv")
zz_uaa_1m <- read_csv("./STIC_uaa/zz2.csv")
w3_uaa_1m <- read_csv("./STIC_uaa/w32.csv")

routes_w3 <- w3_uaa_1m %>% 
    filter(ID %in% W3_IDs) %>% 
  mutate(ID = paste0("W3_",ID)) %>% 
  select(ID, RASTERVALU)
   

routes_fb <- fb_uaa_1m %>%
    filter(ID %in% FB_IDs)%>% 
  mutate(ID = paste0("FB_",ID))%>% 
  select(ID, RASTERVALU)
 

routes_zz <- zz_uaa_1m %>% 
    filter(ID %in% ZZ_IDs) %>% 
  mutate(ID = paste0("ZZ_",ID))%>% 
  select(ID, RASTERVALU)
 
big_routes <- 
  rbind(routes_w3, routes_fb, routes_zz) %>% 
  arrange(desc(RASTERVALU)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID, "uaa" = RASTERVALU) %>% drop_na() %>% 
  select(up, down)

big_output_uaa <- calc_props(big_routes) %>% 
  mutate("hierarchy" = "Drainage Area")

#write_csv(all_uaa, "./hierarchy_analysis_results/drainageArea.csv")
```
```{r topographic-position-index}
# fb_uaa_1m <- read_csv("./STIC_uaa/fb2.csv")
# zz_uaa_1m <- read_csv("./STIC_uaa/zz2.csv")
# w3_uaa_1m <- read_csv("./STIC_uaa/w32.csv")
routes_w3 <- w3_uaa_1m %>% 
    filter(ID %in% W3_IDs) %>% 
  mutate(ID = paste0("W3_",ID)) %>% 
  select(ID, RASTERVALU)
routes_w3 <- w3_tpi %>% 
    filter(ID %in% W3_IDs) %>%
    mutate(ID = paste0("W3_",ID)) 

routes_fb <- fb_tpi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_tpi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(tpi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

big_output_tpi <- calc_props(big_routes) %>% 
  mutate("hierarchy" = "Topographic Position Index")

#write_csv(all_tpi, "./hierarchy_analysis_results/tpi.csv")
```
```{r topographic-wetness-index}
routes_w3 <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
     mutate(ID = paste0("W3_",ID))


routes_fb <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>% 
       mutate(ID = paste0("FB_",ID))


routes_zz <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
       mutate(ID = paste0("ZZ_",ID))

 big_routes <- 
  rbind(routes_w3, routes_fb, routes_zz) %>% 
  arrange(desc(twi)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

big_output_twi <- calc_props(big_routes) %>% 
  mutate("hierarchy" = "Topographic Wetness Index") 

#write_csv(all_twi, "./hierarchy_analysis_results/twi.csv")
```
```{r slope}
routes_w3 <- w3_slope %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(slope) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_slope %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange((slope)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_slope %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange((slope)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_slope_combo <- rbind(calc_props(routes_w3, "W3"),
                 calc_props(routes_fb, "FB"),
                 calc_props(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Slope")

#write_csv(all_slope, "./hierarchy_analysis_results/slope.csv")
```
```{r plot-big-results}
rbind(big_output_uaa, big_output_pk, big_output_twi) %>% 
  # mutate(timescale = fct_relevel("timescale",
  #                             c("30mins", "hourly", "4hr", "daily"))) 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = timescale, color = timescale), alpha = 0.5
  )+
    #geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  #ylim(c(0, 8.75))+
  #xlim(c(0,1))+
  labs(title = "One watershed, one world",
       subtitle = "What if a hierarchy transcended watershed boundaries?",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~hierarchy)
```

#Identify individual events, to run my analysis for each event- how hierarchical was each precipitation event?
```{r figure-out-hydroEvents-pacakge}
#testing out package that will automatically extract rising and falling limb
#install.packages("hydroEvents")
p_load(hydroEvents)
#qdata = WQ_Q$qdata[[1]]
#BF_res = eventBaseflow(qdata$Q_cumecs)
#limbs(data = qdata$Q_cumecs, dates = NULL, events = BF_res, main = "with 'eventBaseflow'")
#Use this plot to manually extract rising and falling limbs
q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
justQ <- q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  arrange(datetime) %>% 
  unique() %>% 
  drop_na() %>% 
  mutate(lab = as.numeric(row_number()))


plot <- justQ %>%  
  ggplot(aes(x = datetime, y = Q_mm_day))+
  geom_line()+
  geom_point(aes(color = start))
plot
ggplotly(plot)



#justQ[5812:5826,]
  
BF_res <- eventBaseflow(justQ$Q_mm_day, BFI_Th = 0.5)
limbs(data = justQ$Q_mm_day, 
               dates =justQ$datetime, 
               events = BF_res, 
               to.plot = TRUE)

#more tidy attempt
events <- lick %>% 
  mutate(event = as.numeric(row_number())) %>% 
  select(ris.srt:event) %>% 
  pivot_longer(cols = ris.srt:fal.end) %>% 
  rename(lab = value) %>% 
  left_join(justQ, by = "lab") %>% 
  mutate(startOrstop = substr(name, 5,7),
         name = substr(name, 1,3)) 

#sometimes you have to use a for loop
id_events <- function(df, ){
  
}

number_of_events <- length(unique(events$event))
for(i in 1:number_of_events){
  one_event <- events %>% 
    filter(event == i)
  
  #rising start and end
  start_df <- one_event %>% 
    filter(name == "ris" & startOrstop == "srt")
  start <- start_df$DATETIME[1]
  print(start)
  # justQ %>% 
  #   filter(DATETIME >= start & DATETIME <= end)
#output a dataframe with all dates, event number id, rising or falling limb, and end
}

#so frustrated, the wheels are spinning, just ditch this for now and come back to it later.

```
```{r identifying-events}
start <- ymd_hms("2023-7-20 00:00:00")
stop <- ymd_hms("2023-10-22 00:00:00")


q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#calculate baseflow
bf = baseflowB(q_23_plotting$Q_mm_day, alpha = 0.980)
#subtract baseflow from discharge
PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)
#plot the events
plotEvents(data = q_23_plotting$Q_mm_day, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = q_23_plotting$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm
PoT_res$srt

q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  arrange(datetime) %>% 
  unique() %>% 
  mutate(lab = as.numeric(row_number())) %>% 
  mutate(group = data.table::rleid(lab, cols = PoT_res$srt))

ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))

ranges[26,2] <- 5762
#trying fuzzy join method?
p_load(fuzzyjoin)
id_events <- q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  mutate(ID = row_number()) |> fuzzy_left_join(ranges, by = c(ID = "starts", ID = "stops"),
                              match_fun = list(`>=`, `<=`)) |> 
   mutate(event_ID = event_ID) |> 
   select(-starts, -stops)

id_events %>% 
  ggplot()+
  geom_line(aes(x = datetime, y = Q_mm_day, color = event_ID))


```
#NOT DONE plot hypsometric curves for each stream network
Extract elevation along the streams (might need ARC), then plot versus distance

#Determine extent of hierarchical wetting and drying through time
Within a single time step, what proportion of sensors are behaving hierarchically?
Or maybe two timesteps, because of the way my analysis works?

If the sequence is defined by a chain, then there is a threshold value at any given time, with values below not flowing and values above flowing. This is the basic premise of botter and Durighetto model.

Construct my chain of flow permanence, then see if there are sensors that do not fall on the right side of the threshold.
000111
-1-1-1 111

If there are -1 and 1, I can determine where they are symmetrical?

Or just at every time step, see if a flowing sensor follows hierarchy

What I ended up doing- lagged the flow state by one after sorting by flow permanence, and determined how many sensors at each timestep were flowing when they should not be according to sequence or hierarchy. Totalled for each timestep, and showed proportion of ones that obeyed the hierarchy through time/along hydrograph.

```{r figure-out-how-to-count}
pks_w3
filter(input_w3, datetime == ymd_hms("2023-07-03 16:30:00")) %>% 
  left_join(pks_w3, by = "ID") %>% 
  select(-c(wshed.x, wshed.y, mins, datetime)) %>% 
  arrange(desc(pk)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  filter(following == -1) %>% 
  summarise(count = length(following))
```
```{r calculate-how-many-follow-sequence}
sequential <- input_w3 %>% 
  left_join(pks_w3, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed.x, wshed.y, mins, datetime)) %>% 
  arrange(desc(pk)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  filter(following != -1) %>% 
    summarise(count_non = length(following))

total <- input_w3 %>% 
  left_join(pks_w3, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed.x, wshed.y, mins, datetime)) %>% 
  arrange(desc(pk)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
    summarise(count_all = length(following))

final <- left_join(sequential, total, by = "datetime") %>% 
  mutate(prop = count_non/count_all)
  
```
```{r plot-along-discharge}
ggplot(q_23_f, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

ggplot(q_24_f, aes(x  = datetime, y = Q_mmperday))+
  geom_line()+
  labs(title = "Discharge from W3, May to July 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

q_23_bind <- 
  q_23_f %>% 
  select(DATETIME, Q_mm_day) %>% 
  rename("datetime" = DATETIME
         )

q_24_bind <- 
  q_24_f %>% 
  select(datetime, Q_mmperday) %>% 
  rename("Q_mm_day" = Q_mmperday)

q_24_bind %>% 
  inner_join(final, by = "datetime") %>% 
  ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line(aes(color = prop))+
  labs(title = "Discharge from W3, 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()+
  scale_color_continuous(type = "viridis")+
                       theme(rect = element_rect(fill = "transparent", color = NA))
```

Plot of discharge versus threshold flow permanence?
#OG analysis, but average flow state at coarser timesteps
All I have to do is create a new fantastic four function I believe, that will calculate average state instead of filtering to everyday at noon or whatever
```{r}
fantastic_four <- function(routes, shed){
  theFour <- c("30mins", "hourly", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "hourly"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
       filter(wshed == shed, mins %in% 0) %>% 
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(1)
  } 
  else if(timescale == "4hr"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed) %>% 
      group_by(ID) %>% 
      mutate(group_id = (row_number() - 1) %/% 8 + 1) %>% 
      ungroup() %>% 
      group_by(group_id, ID) %>%
      summarise(avg_state = mean(binary),
                datetime = min(datetime)) %>% 
      ungroup() %>% 
      mutate(avg_state = round(avg_state)) %>% 
      rename("binary" = avg_state) %>% 
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
    arrange(datetime) %>% 
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(4)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed) %>% 
      mutate(
             "day" = day(datetime),
             "month" = month(datetime),
             "year" = year(datetime)) %>% 
      group_by(day, month, year, ID) %>%
      summarise(avg_state = mean(binary)) %>% 
      ungroup() %>% 
      mutate(avg_state = round(avg_state)) %>% 
      rename("binary" = avg_state) %>% 
      mutate("datetime" = ymd_hms(paste0(year,"-",month,"-",day," ","00:00:00"))) %>% 
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
          arrange(datetime) %>% 
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}

#experimental 4 hour
#input <- 
#test <- 
  rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == "FB") %>% mutate(
             "day" = day(datetime),
             "month" = month(datetime),
             "year" = year(datetime)) %>% 
      group_by(ID) %>%
  mutate(group_id = rep(1:ceiling(n()/8), each = 8, length.out = n())) %>% 
  ungroup() %>% 
      group_by(group_id, ID) %>%
      summarise(avg_state = mean(binary),
                datetime = min(datetime)) %>% 
      ungroup() %>% 
      mutate(avg_state = round(avg_state)) %>% 
      rename("binary" = avg_state) %>% 
      #mutate("datetime" = ymd_hms(paste0(year,"-",month,"-",day," ",hour,":00:00"))) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
    arrange(datetime) %>% 
      pivot_wider(names_from = ID, values_from = binary) %>% View()

calculate_hierarchy(routes_w3, test, hours(4))
iterate_groups("r_23", "r_2", test, hours(4))
calc_support("r_7", "r_9", test)

test$group <- cumsum(c(TRUE, diff(test$datetime) != hours(4)))


  
#experimental. daily
  #input <- 
    rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == "W3") %>% 
      mutate(
             "day" = day(datetime),
             "month" = month(datetime),
             "year" = year(datetime)) %>% 
      group_by(day, month, year, ID) %>%
      summarise(avg_state = mean(binary)) %>% 
      ungroup() %>% 
      mutate(avg_state = round(avg_state)) %>% 
      rename("binary" = avg_state) %>% 
      mutate("datetime" = ymd_hms(paste0(year,"-",month,"-",day," ","00:00:00"))) %>% 
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
          arrange(datetime) %>% 
      pivot_wider(names_from = ID, values_from = binary) %>% View()


  #input <- 
    rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == "W3", mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    
    
#original fantastic 4 components
    #input <- 
      rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == "W3", hour %in% c(0,4,8,12,16,20,24), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(4)
  
    #input <- 
      rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == "W3", hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
      

      #print('John is stinky')

```

#number them based on order that they turn on, then average that
Molly idea- for each rising and falling limb, number them
```{r}
inner_join(input_w3, id_events, by = "datetime") %>% 
  
```

