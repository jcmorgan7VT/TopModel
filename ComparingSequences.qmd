---
title: "ComparingSequences"
format: html
editor_options: 
  chunk_output_type: console
---

5/27/25

New clean script for analysis determining ideal sequences for each watershed, and comparing them to sequences that are random, and dictated by proportion of time flowing, and topography.  

Last markdown document was almost 10,000 lines, and getting unwieldy to navigate.  

Explanation of graph theory stuff: finding the best path using the traveling salesman's problem. Usually this provides a cycle, but by making a dummy node at the end and then removing it we can trick the algorithm to produce a very efficient 1-way path.  

Not sure if I should include the dummy or not; FB is a lot better with a dummy, but W3 and ZZ are a lot better without dummies at the end.

```{r setup}
#loading packages
library(pacman)
p_load(tidyverse, terra, tidyterra, whitebox, scales, wesanderson, caret, plotly,ggnewscale, sf, elevatr, patchwork, ggspatial, zoo, igraph, TSP, ggnetwork, intergraph, ggpubr)
#whitebox::install_whitebox()

#reading in final format data for summer 23
data_23 <- read_csv("./DataForMary/HB_stic.csv")
#reading in final format data for summer 24
data_24 <- read_csv("./summer2024/STICS2024.csv")

data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

bind23 <- data_23 %>% 
  select(datetime, ID, wshed, binary, mins)
bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)
```
#Preparing inputs
Preparing the STIC dataset as inputs for everything I am doing.  
```{r prepare-inputs}
#create input that only uses sensors that were deployed during both deployments
#need to make a list of sensors deployed in both campaigns for each watershed
#W3
w3_deployed24 <- unique(filter(data_24, wshed == "W3")$number)
w3_deployed23 <- unique(filter(data_23, wshed == "W3")$ID)
W3_IDs <- intersect(w3_deployed24, w3_deployed23)
#FB
fb_deployed24 <- unique(filter(data_24, wshed == "FB")$number)
fb_deployed23 <- unique(filter(data_23, wshed == "FB")$ID)
FB_IDs <- intersect(fb_deployed24, fb_deployed23)
#ZZ
zz_deployed24 <- unique(filter(data_24, wshed == "ZZ")$number)
zz_deployed23 <- unique(filter(data_23, wshed == "ZZ")$ID)
ZZ_IDs <- intersect(zz_deployed24, zz_deployed23)


#detach("package:fitdistrplus")
#detach("package:MASS")


bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)%>% 
  filter(mins %in% c(0, 30))


bind23 <- data_23 %>% 
  filter(ID %in% intersect(bind24$ID, data_23$ID)) %>% 
  select(datetime, ID, wshed, binary, mins) %>% 
  filter(mins %in% c(0, 30))

#filter by each watershed, then recombine at the end
input_w3 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3") %>% 
  filter(ID %in% W3_IDs)
input_fb <- rbind(bind23, bind24) %>%
  filter(wshed == "FB") %>% 
  filter(ID %in% FB_IDs)
input_zz <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ") %>% 
  filter(ID %in% ZZ_IDs)

input_all <- rbind(input_w3, input_fb, input_zz)
write_csv(input_w3, "calc_support_inputs_w3.csv")
write_csv(input_fb, "calc_support_inputs_fb.csv")
write_csv(input_zz, "calc_support_inputs_zz.csv")

      
```
Define functions that calculate proportion of time that nodes wet and dry sequentially.  
```{r define-functions}
calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  sequence_df <- data.frame("Sequence" = NA, 
                            "Frequency" = NA,
                            "up" = up,
                            "down" = down)
  return(sequence_df)
} 
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")


sub <- filter(sequence_df, Sequence %in% supports)
output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)}
}

#test function

#calc_support_combos("r_18", "r_11", input_test)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_combos <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support_combos(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

#iterate_groups("r_13", "r_19", input, min(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_combos <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- iterate_groups_combos(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down, Sequence) %>%
    summarise(Frequency = sum(Frequency))
  return(final_output)
}

#fantastic four function has been modified, to do average daily state. Removed hourly and 4 hr time blocks
fantastic_four_combos <- function(routes, shed){
  theFour <- c("30mins", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed) %>% 
      mutate(
             "day" = day(datetime),
             "month" = month(datetime),
             "year" = year(datetime)) %>% 
      group_by(day, month, year, ID) %>%
      summarise(avg_state = mean(binary)) %>% 
      ungroup() %>% 
      mutate(avg_state = round(avg_state)) %>% 
      rename("binary" = avg_state) %>% 
      mutate("datetime" = ymd_hms(paste0(year,"-",month,"-",day," ","00:00:00"))) %>% 
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
          arrange(datetime) %>% 
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_combos(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}

calc_props <- function(routes, shed){
  full_combos <- fantastic_four_combos(routes, shed)
total_state_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    group_by(up, down, timescale, shed) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down, timescale, shed) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down", "shed", "timescale")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
return(un_split)
}
```

#Set up for analysis- construct full graphs of networks
Determine all possible combinations
```{r W3}
input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- W3_IDs

#create empty list to hold repeated node IDs
zzz <- length(combos)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}
#create data frame with all possible combinations
W3_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#Run suite of functions to determine proportion of time that state changes follow the parent -> child relationship
W3_all_combos <- calc_props(W3_combos_routes, "W3")
```

```{r FB}
#set routes for all 3 watersheds
input <- rbind(bind23, bind24) %>%
  filter(wshed == "FB", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- FB_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)

#run calc_support for all sheds and timesteps for relative position
FB_all_combos <- calc_props(all_combos_routes, "FB")
```

```{r ZZ}
#set routes for all 3 watersheds

input <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
ZZ_all_combos <- calc_props(all_combos_routes, "ZZ")
```

For each watershed, find the best chain using solutions to traveling salesman problem.  
```{r W3}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

# methods <- c("random",
#   "cheapest_insertion")

#set up edges and distance matrix, convert to TSP object
edges_W3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()

keyz <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))

# Convert to TSP object

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  atsp <- as.ATSP(edges_W3)

    atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], two_opt = TRUE)
    #tour <- solve_TSP(tour, method = "two_opt")

  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "W3") %>% 
    mutate("method" = methods[i])

    if(i == 1) all_chain_outputs <- chain_output
    if(i > 1) all_chain_outputs <- rbind(all_chain_outputs, chain_output)
}

all_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

```

```{r FB-analysis}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

#methods <- c("random", "cheapest_insertion")
keyz <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edges_FB <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  atsp <- as.ATSP(edges_FB)
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i])#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "FB") %>% 
    mutate("method" = methods[i])

    if(i == 1) FB_chain_outputs <- chain_output
    if(i > 1) FB_chain_outputs <- rbind(FB_chain_outputs, chain_output)
}


#plots for committee meeting
FB_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

```

```{r ZZ-analysis}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

#methods <- c("random", "cheapest_insertion")
keyz <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edges_ZZ <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edges_ZZ)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(ZZ_IDs) + 1)#, start = 25)#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "ZZ") %>% 
    mutate("method" = methods[i])

    if(i == 1) ZZ_chain_outputs <- chain_output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, chain_output)
}


#plots for committee meeting
ZZ_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)


atsp <- as.ATSP(edges_ZZ)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[4], start = length(ZZ_IDs) + 1)#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  #path <- as.integer(tour)
  path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])



# Create a chain graph from the path
edges <- cbind(path[-length(path)], path[-1])
chain_graph <- graph_from_edgelist(edges, directed = TRUE)
plot(chain_graph,
       layout = layout_nicely(chain_graph),
     edge.arrow.size = 0.25,
  vertex.size = 10,
  vertex.label.cex = 0.75)

ggnet <- fortify(chain_graph, layout = igraph::layout_nicely(chain_graph))
ggplot(ggnet, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 8) +
  # geom_nodetext(aes(label =LETTERS[ path ]),
  #               fontface = "bold", color = "white", size = 3) +
  theme_blank()
```

For final plots for tomorrow, keep arbitrary insertion and cheapest insertion
```{r generalized-function}
#define all possible methods
#removed 3 methods
all_methods = c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")
#methods <- c("random", "cheapest_insertion")

general_graph <- function(combos, 
                          shed, 
                          methods = c("random", "cheapest_insertion"),
                          two_opt = TRUE){
#combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
allcombosIn <- calc_props(all_combos_routes, shed)

#methods <- c("random", "cheapest_insertion")
keyz <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edgesIn <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edgesIn)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], 
                    start = length(combos) + 1, two_opt = two_opt)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, shed) %>% 
    mutate("method" = methods[i])

    if(i == 1) ZZ_chain_outputs <- chain_output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, chain_output)
}
return(ZZ_chain_outputs)
}

#test general graph solution
ZZ_graph_solution <- general_graph(ZZ_IDs, "ZZ")

ZZ_graph_solution %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

#separate function to output the path, for non-determinstic methods may result in a different chain than other function
#can take multiple methods
# True/false whether output includes visualization of the chain
##works on a single method to develop a chain
chain_solution <- function(combos, 
                           shed, 
                           methods = "cheapest_insertion", 
                           plot = FALSE){
#combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
allcombosIn <- calc_props(all_combos_routes, shed)

#methods <- c("random", "cheapest_insertion")
keyz <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edgesIn <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edgesIn)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(combos) + 1, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  #edges <- cbind(path[-length(path)], path[-1])
   output <- data.frame("node_ID" = path) %>% 
     left_join(key2, by = "node_ID") %>% 
     mutate(method = methods[i])
  
    if(i == 1) ZZ_chain_outputs <- output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, output)
}



chains_test <- ZZ_chain_outputs
edges <- cbind(chains_test$node_ID[-length(chains_test$node_ID)], chains_test$node_ID[-1])

chain_graph <- graph_from_edgelist(edges, directed = TRUE)

V(chain_graph)$labels <- chains_test$sensor_ID

ggmst <- fortify(chain_graph, layout = igraph::layout_as_tree(chain_graph))

# if(plot == TRUE){
# ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
#   geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
#                                                  type = "open")) +
#   geom_nodes(color = "black", size = 3) +
#   geom_nodetext(aes(label = labels),
#                 fontface = "bold", color = "white", size = 1) +
#   theme_blank()
# }

order <- ggmst %>% 
  arrange(desc(y))

#unique(order$labels)

#perfecting chains function to just output a dataframe, with one column for the sequence number, and the other for sensor ID
chains_output <- data.frame("ID" = unique(order$labels),
           "sequence" = seq(1, length(unique(order$labels)), 1))

return(chains_output)

}
chain_solution(ZZ_IDs, "ZZ", plot = TRUE)


#generate chain solution for a series of methods, or the same method many times?
#NOT FINISHED
chain_solution_multiple <- function(combos, 
                           shed, 
                           methods = "cheapest_insertion", 
                           plot = FALSE){
#combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
allcombosIn <- calc_props(all_combos_routes, shed)

#methods <- c("random", "cheapest_insertion")
keyz <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edgesIn <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edgesIn)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(combos) + 1, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  #edges <- cbind(path[-length(path)], path[-1])
   output <- data.frame("node_ID" = path) %>% 
     left_join(key2, by = "node_ID") %>% 
     mutate(method = methods[i])
  
    if(i == 1) ZZ_chain_outputs <- output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, output)
}



chains_test <- ZZ_chain_outputs
edges <- cbind(chains_test$node_ID[-length(chains_test$node_ID)], chains_test$node_ID[-1])

chain_graph <- graph_from_edgelist(edges, directed = TRUE)

V(chain_graph)$labels <- chains_test$sensor_ID

ggmst <- fortify(chain_graph, layout = igraph::layout_as_tree(chain_graph))

# if(plot == TRUE){
# ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
#   geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
#                                                  type = "open")) +
#   geom_nodes(color = "black", size = 3) +
#   geom_nodetext(aes(label = labels),
#                 fontface = "bold", color = "white", size = 1) +
#   theme_blank()
# }

order <- ggmst %>% 
  arrange(desc(y))

#unique(order$labels)

#perfecting chains function to just output a dataframe, with one column for the sequence number, and the other for sensor ID
chains_output <- data.frame("ID" = unique(order$labels),
           "sequence" = seq(1, length(unique(order$labels)), 1))

return(chains_output)

}
```

#graph theory
```{r}
all_graph_solutions <- rbind(general_graph(W3_IDs, "W3"),
                             general_graph(FB_IDs, "FB"),
                             general_graph(ZZ_IDs, "ZZ"))

showcase <- rbind(general_graph(W3_IDs, "W3", all_methods),
                             general_graph(FB_IDs, "FB", all_methods),
                             general_graph(ZZ_IDs, "ZZ", all_methods))

showcase %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    #geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(shed~timescale)
```


My big graph theory solution provides the random hierarhcy and graph theory solution. All I need is the proportion of time flowing and maybe one based on topography... maybe drainage area or TWI?
#proportion of time flowing
```{r flow-permanence-W3}
#just summer 2023
data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

pks_23 <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>% 
  #left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()
#just summer 2024
pks_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number) %>% 
  #left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()

both <- inner_join(pks_23, pks_24, by = "ID")
ggplot()+
  geom_point(data = both,aes(x = pk.x, y = pk.y))+
  labs(title = "Change in pk from '23 to '24, W3",
       x = "2023",
       y = "2024")+
  geom_abline(slope=1, intercept=0)+
  theme_classic()

#both summers
#just rbind summers 23 and 24
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_w3 <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "W3")
```
```{r flow-permanence-FB}
#both summers
#just rbind summers 23 and 24
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_fb <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "FB")
```
```{r flow-permanence-ZZ}
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_zz <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "ZZ")
```
```{r proportion-of-time-flowing-analysis}
routes_w3 <- pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- pks_fb %>%
    filter(ID %in% FB_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- pks_zz %>%
    filter(ID %in% ZZ_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

all_pk <- rbind(calc_props(routes_w3, "W3"),
                calc_props(routes_fb, "FB"),
                calc_props(routes_zz, "ZZ")) %>% 
  mutate("method" = "Flow Permanence")
```

#TWI
```{r locs-from-arc}
#get snapped sensor locations from extracted values from arc
w3_locs <- read_csv("./STIC_uaa/w32.csv")%>% 
  select(ID, POINT_X, POINT_Y)
fb_locs <- read_csv("./STIC_uaa/fb2.csv") %>% 
  select(ID, POINT_X, POINT_Y)
zz_locs <- read_csv("./STIC_uaa/zz1.csv")%>% 
  select(ID, POINT_X, POINT_Y)
```
```{r tpi-W3}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(w3_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

w3_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble()

w3_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble()

w3_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble()

```
```{r tpi-fb}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(fb_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

fb_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble()

fb_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble()

fb_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble()
```
```{r tpi-zz}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(zz_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

zz_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble()

zz_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble()

zz_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble()
```
```{r topographic-wetness-index}
routes_w3 <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_twi <- rbind(calc_props(routes_w3, "W3"),
                 calc_props(routes_fb, "FB"),
                 calc_props(routes_zz, "ZZ")) %>% 
  mutate("method" = "Topographic Wetness Index")

```
#Figure 7: Final distributions of proportion of transitions that follow a sequence
```{r combine-and-plot}
four_colors <- c("#231f20", "#bb4430", "#7ebdc2", "#f3dfa2")

rbind(all_graph_solutions,
      all_pk,
      all_twi) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    #geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = four_colors,
                     name = "Method")+
  scale_color_manual(values = four_colors,
                     name = "Method")+
  facet_grid(shed~timescale)
```
```{r combine-and-plot-two}
#fixing randomly generated sequence
fixed_combined <- 
rbind(general_graph(W3_IDs, "W3", methods = c("random"), two_opt = FALSE),
      general_graph(W3_IDs, "W3", methods = c("cheapest_insertion"), two_opt = TRUE),
      general_graph(FB_IDs, "FB", methods = c("random"), two_opt = FALSE),
      general_graph(FB_IDs, "FB", methods = c("cheapest_insertion"), two_opt = TRUE),
      general_graph(ZZ_IDs, "ZZ", methods = c("random"), two_opt = FALSE),
      general_graph(ZZ_IDs, "ZZ", methods = c("cheapest_insertion"), two_opt = TRUE),
      all_pk,
      all_twi) 

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")

fixed_combined %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    stat_central_tendency(aes(color = method), type = "median", linetype = "dashed")+
    #geom_density(alpha = 0.5, lty = 3)+
     # geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = four_colors,
                     name = "Method")+
  scale_color_manual(values = four_colors,
                     name = "Method")+
  facet_grid(shed~timescale)
```

#maps of where sequences work in space NOT DONE

#map showing the sequence in space
```{r}
edges2 <- all_graph_solutions %>% 
  filter(shed == "W3", method == "random", timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down) %>% as.matrix()
chain_graph <- graph_from_edgelist(edges2, directed = TRUE)
plot(chain_graph)

ggmst <- fortify(chain_graph, layout = igraph::layout_as_tree(chain_graph))
ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 3) +
  geom_nodetext(aes(label = name),
                fontface = "bold", color = "white", size = 1) +
  theme_blank()
```

#How well different sequences work through time and along the hydrograph
```{r calculate-how-many-follow-sequence}
sequential <- input_w3 %>% 
  left_join(pks_w3, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed.x, wshed.y, mins, datetime)) %>% 
  arrange(desc(pk)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  filter(following != -1) %>% 
    summarise(count_non = length(following))

total <- input_w3 %>% 
  left_join(pks_w3, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed.x, wshed.y, mins, datetime)) %>% 
  arrange(desc(pk)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
    summarise(count_all = length(following))

final <- left_join(sequential, total, by = "datetime") %>% 
  mutate(prop = count_non/count_all)


#create a reference dataframe with the sensor ID, then the number in the sequence
w3_twi_sequence <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(twi)) %>% 
  rename(sequence = twi)

fb_twi_sequence <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>% 
  arrange(desc(twi))

zz_twi_sequence <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  arrange(desc(twi))

```
```{r}
#creating chains

W3_cheap <- chain_solution(W3_IDs, "W3", methods = "cheapest_insertion")
W3_random <- chain_solution(W3_IDs, "W3", methods = "random")

FB_cheap <- chain_solution(FB_IDs, "FB", methods = "cheapest_insertion")
FB_random <- chain_solution(FB_IDs, "FB", methods = "random")

ZZ_cheap <- chain_solution(ZZ_IDs, "ZZ", methods = "cheapest_insertion")
ZZ_random <- chain_solution(ZZ_IDs, "ZZ", methods = "random")

```
Once sequence is defined, I can compare to the actual flow data
```{r}
#sequential <- 
input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  arrange((sequence)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  #filter(following != -1) %>% 
  filter(datetime == "2023-07-15 19:00:00") %>% View()
   # summarise(count_non = length(following)) %>% View()
    
input_w3 %>% 
  left_join(W3_pk_seq, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  arrange((sequence)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  #filter(following != -1) %>% 
  filter(datetime == "2023-07-15 19:00:00") %>% View()
    summarise(count_non = length(following))

#make a df with ID and the number in the sequence
W3_pk_seq <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_w3$ID), 1)) %>% 
  select(ID, sequence)
  

total <- input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins, datetime)) %>% 
  arrange(sequence) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
    summarise(count_all = length(following))

final <- left_join(sequential, total, by = "datetime") %>% 
  mutate(prop = count_non/count_all)

#write a function to calculate this in 1 quick step
calc_time <- function(sequence_df, shed, seq_label){
  
  if(shed == "W3"){
    input_f <- input_w3
  } else if(shed == "FB"){
        input_f <- input_fb
  } else if(shed == "ZZ"){
        input_f <- input_zz
  }
  
sequential <- input_f %>% 
  left_join(sequence_df, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins, datetime)) %>% 
  arrange(sequence) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  filter(following != -1) %>% 
    summarise(count_non = length(following))

total <- input_f %>% 
  left_join(sequence_df, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins, datetime)) %>% 
  arrange(sequence) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
    summarise(count_all = length(following))

final <- left_join(sequential, total, by = "datetime") %>% 
  mutate(prop = count_non/count_all) %>% 
  mutate(shed = shed)

output <- q_24_bind %>% 
  inner_join(final, by = "datetime") %>% 
  mutate(seq_label = seq_label)

return(output)
}

calc_time(W3_cheap, "W3", "cheapest")
```

```{r prepare-discharge}
#from TestingFrameworks script

#read in discharge from W3-- input to Carrie's model, discharge in L/s
#q <- read_csv("https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-hbr.1.17&entityid=efc477b3ef1bb3dd8b9355c9115cd849")
#write.csv(q, "HB_5minQ.csv")
q <- read_csv("HB_5minQ.csv")

#input discharge needs to be in mm/day?
#reference to understand difference between daily mean and instantaneous streamflow:
#https://hydrofunctions.readthedocs.io/en/master/notebooks/DailyMean_vs_Instant.html

#creating minute column, used to filter out higher temporal resolution measurements for plotting
data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

#find the range of dates that I need discharge for
start <- min(data_23$datetime)
stop <- max(data_23$datetime)

#filtering discharge down to the range of dates
q_23 <- q %>% 
  filter(DATETIME > start & DATETIME < stop) %>% 
  #convert to mm/day.
  #converting instantaneous streamflow to mm/day by taking measurement, and scaling   it up as if that was the discharge for the whole day. It is not, it is just at that   moment, but should fix any units/order of magnitude issues
  mutate("Q_mm_day" = Discharge_ls * 0.001 * 86400 / 420000 * 1000) 
q_23$mins <- minute(q_23$DATETIME)

#removing times that are not coincident with STIC observations
q_23_f <- filter(q_23, mins %in% c(0, 30))

ggplot(q_23_f, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#also read in provisional 2024 data
q_24 <- read_csv("w3_discharge_24.csv")

#find the range of dates that I need discharge for
start24 <- ymd_hms("2024-05-15 00:00:00 UTC")
stop24 <- max(data_24$datetime)

#filtering discharge down to the range of dates
q_24 <- q_24 %>% 
  mutate(datetime = mdy_hm(datetime)) %>% 
  filter(datetime > start24 & datetime < stop24)  
q_24$mins <- minute(q_24$datetime)

#removing times that are not coincident with STIC observations
q_24_f <- filter(q_24, mins %in% c(0, 30))

ggplot(q_24_f, aes(x  = datetime, y = Q_mmperday))+
  geom_line()+
  labs(title = "Discharge from W3, May to July 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#get discharge ready to bind to my other data
q_23_bind <- 
  q_23_f %>% 
  select(DATETIME, Q_mm_day) %>% 
  rename("datetime" = DATETIME
         )

q_24_bind <- 
  q_24_f %>% 
  select(datetime, Q_mmperday) %>% 
  rename("Q_mm_day" = Q_mmperday)
```

```{r plot-along-discharge}

rbind(
calc_time(W3_cheap, "W3", "cheapest"),
calc_time(W3_random, "W3", "random"),
calc_time(w3_twi_sequence, "W3", "twi"),
calc_time(W3_pk_seq, "W3", "pk")
) %>% 
  ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line(aes(color = prop))+
  labs(title = "Discharge from W3, 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()+
  scale_color_continuous(type = "viridis")+
                       theme(rect = element_rect(fill = "transparent", color = NA))+
  facet_grid(seq_label~shed)
```

```{r}
pk_props_join <- calc_time(W3_pk_seq, "W3", "pk") %>% 
  select(datetime, prop) %>% 
  rename("pk_prop" = prop)

rbind(
calc_time(W3_cheap, "W3", "cheapest"),
calc_time(W3_random, "W3", "random"),
calc_time(w3_twi_sequence, "W3", "twi"),
calc_time(W3_pk_seq, "W3", "pk")
) %>% 
  left_join(pk_props_join, by = "datetime") %>% 
  filter(seq_label != "pk") %>% 
  mutate(diff = prop - pk_prop) %>% 
  ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line(aes(color = diff))+
  labs(title = "Difference between Flow permanence sequence and others, 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()+
  scale_color_gradient2(low = "red", 
    mid = "white", 
    high = "blue", 
    midpoint = 0,
    limits = c(-0.3, 0.3))+
                       theme(rect = element_rect(fill = "transparent", color = NA))+
  facet_grid(seq_label~shed)
```

This method of evaluating did not quite produce what i was hoping, I am going to try one more then i might have to change my results and discussion...

Use same method as Botter and Durighetto paper, model! Calculate the number of nodes that are active at each time step, and turn nodes following sequence until the same number of flowing nodes are on
```{r}
input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  arrange((sequence)) %>% 
  # mutate("downstream" = lag(binary),
  #        "following" = downstream - binary) %>% 
  #filter(following != -1) %>% 
  filter(datetime == "2023-07-15 19:00:00") %>% View()
   # summarise(count_non = length(following)) %>% View()

# calculate the number of flowing nodes at each timestep
number_activated <- 
  input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  #arrange(sequence) %>% 
      filter(binary == 1) %>% 
    summarise(number_flowing = length(binary))


  
#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy
number_activated %>% mutate(all_nodes = W3_pk_seq$ID)


```

```{r}
# 1. Ordered node sequence
node_sequence <- W3_pk_seq$ID

# 2. Dataframe with timestamps and how many nodes to activate
number_activated

# 3. Build the activation matrix
activation_matrix <- t(sapply(number_activated$number_flowing, function(n) {
  active_flags <- rep(0, length(node_sequence))
  if (n > 0) active_flags[1:n] <- 1
  active_flags
}))

# 4. Assign column names and combine with timestamps
colnames(activation_matrix) <- node_sequence
activation_df <- cbind(number_activated["datetime"], as.data.frame(activation_matrix))

# View result
activation_final <- activation_df %>% pivot_longer(-datetime) %>% 
  rename("ID" = name, "pred_binary" = value) %>% 
  mutate(ID = as.numeric(ID))


#now compare the result to the actual
code <- data.frame(diff = c(-1, 0, 1),
                   code = c("commission", "correct", "ommission"))

comparison <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% left_join(activation_final, by = c("datetime", "ID")) %>% 
  mutate("diff" = binary - pred_binary) %>%
  left_join(code, by = "diff") %>% 
  #filter(diff != 0) %>%
  #filter(datetime == "2023-07-11 19:00:00") %>% 
  group_by(code, datetime) %>% 
   summarise(count = length(code))

comparison %>% 
  filter(datetime >= ymd_hms("2023-07-24 00:00:00") & datetime <= ymd_hms("2023-08-20 00:00:00")) %>% 
  ggplot() +
  geom_histogram(aes(x = datetime, y = count, fill = code), stat = "identity")
```

Function to calculate the comparison between botter model based on a provided sequence.
```{r}

calc_model_result <- function(input_sequence, output_col){
# 1. Ordered node sequence
node_sequence <- input_sequence

# 2. Dataframe with timestamps and how many nodes to activate
number_activated

# 3. Build the activation matrix
activation_matrix <- t(sapply(number_activated$number_flowing, function(n) {
  active_flags <- rep(0, length(node_sequence))
  if (n > 0) active_flags[1:n] <- 1
  active_flags
}))

# 4. Assign column names and combine with timestamps
colnames(activation_matrix) <- node_sequence
activation_df <- cbind(number_activated["datetime"], as.data.frame(activation_matrix))

# View result
activation_final <- activation_df %>% pivot_longer(-datetime) %>% 
  rename("ID" = name, "pred_binary" = value) %>% 
  mutate(ID = as.numeric(ID))


#now compare the result to the actual
# code <- data.frame(diff = c(-1, 0, 1),
#                    code = c("commission", "correct", "ommission"))
# 
# comparison <- input_w3 %>% 
#   filter(mins %in% c(0, 30)) %>% left_join(activation_final, by = c("datetime", "ID")) #%>% 
#   # mutate("diff" = binary - pred_binary) %>%
#   # left_join(code, by = "diff") %>% 
#   # #filter(diff != 0) %>%
#   # #filter(datetime == "2023-07-11 19:00:00") %>% 
#   # group_by(code, datetime) %>% 
#   #  summarise(count = length(code))

return(activation_final)
}




comparison <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model, by = c("datetime", "ID")) %>% 
  left_join(pk_model, by = c("datetime", "ID")) %>% 
    left_join(random_model, by = c("datetime", "ID")) %>% 
  left_join(twi_model, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins) %>% 
  drop_na()

#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
get_eval_label(0,0)
#apply get_eval function to each model run
comparison$cheapest_eval <- mapply(get_eval_label, comparison$binary, comparison$cheapest)
comparison$pk_eval <- mapply(get_eval_label, comparison$binary, comparison$pk)
comparison$random_eval <- mapply(get_eval_label, comparison$binary, comparison$random)
comparison$twi_eval <- mapply(get_eval_label, comparison$binary, comparison$twi)


comparison$comparison <- with(comparison, ifelse(
  cheapest_eval == pk_eval, "equal",
  ifelse(cheapest_eval == "correct", "cheap_better",
         ifelse(pk_eval == "correct", "pk_better", "neither_better"))
))

library(dplyr)

summary_by_node <- comparison %>%
  group_by(ID) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    cheap_omission = sum(cheapest_eval == "omission"),
    pk_omission = sum(pk_eval == "omission"),
    random_omission = sum(random_eval == "omission"),
    twi_omission = sum(twi_eval == "omission"),
    cheap_commission = sum(cheapest_eval == "commission"),
    pk_commission = sum(pk_eval == "commission"),
    random_commission = sum(random_eval == "commission"),
    twi_commission = sum(twi_eval == "commission")
    #A_better_count = sum(comparison == "A_better"),
    #B_better_count = sum(comparison == "B_better"),
    #Equal_count = sum(comparison == "equal")
  )


ggplot(summary_by_node, aes(x=A_correct, y=B_correct, label=ID)) +
  geom_point() +
  geom_abline(slope=1, intercept=0, linetype="dashed", color="red") +
  geom_text(nudge_y=1.5) +
  labs(title="Model Comparison per Node", x="Model A Correct Predictions", y="Model B Correct Predictions")

ggplot(summary_by_node, aes(x=A_better_count, y=B_better_count)) +
  geom_point() +
  geom_abline(slope=1, intercept=0, linetype="dashed") +
  labs(x="Times Model A was better", y="Times Model B was better")

#summarize through time
df_summary_time <- comparison %>%
  group_by(datetime) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total
  )

df_summary_time <- df_summary_time %>%
  mutate(diff = A_accuracy - B_accuracy)

df_long <- df_summary_time %>%
  select(datetime, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy")

ggplot(df_long, aes(x = datetime, y = accuracy, color = model)) +
  geom_line() +
  labs(title = "Model Accuracy Over Time", y = "Accuracy", x = "Time")

ggplot(df_summary_time, aes(x = datetime, y = diff)) +
  geom_line() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Difference in Accuracy (A - B)", y = "Accuracy Difference", x = "Time")

#plot the difference in performance versus discharge
df_summary_time %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  #rename("Hydrograph Component", event_type)
  #group_by(event_type, code) %>% 
  #summarise(mean = mean(count)) %>% 
  ggplot(aes(x = Q_mm_day, y = diff, color = event_type)) +
  geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Difference in Accuracy (Cheapest - Prop of time flowing)", y = " ", x = "Discharge (mm/day)")+
   scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1"))+
  labs(color='Hydrograph Component') +
  theme_classic()

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")

#plot of the accuracy versus discharge, colored by scheme, shape by hydrograph component
df_long %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  #rename("Hydrograph Component", event_type)
  #group_by(event_type, code) %>% 
  #summarise(mean = mean(count)) %>% 
  ggplot(aes(x = Q_mm_day, y = accuracy, color = model, shape = event_type)) +
  geom_point(alpha = 0.3, size = 2) +
    #geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Accuracy (correctly modeled states/total)", y = " ", x = "Discharge (mm/day)")+
   scale_color_manual(values = four_colors)+
  labs(color='Hydrograph Component') +
  theme_classic()+
  facet_wrap(~model)

#make companion plot that shows the number of transitions during each part of hydrograph
test <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>%
  group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

library(knitr)
test %>% 
  inner_join(discharge_df, by = "datetime") %>%
  group_by(state_change, event_type) %>% 
  summarise(count = n()) %>% 
  pivot_wider(names_from = state_change, values_from = count) %>% 
  kable(format = "markdown")


```

Chunk straight from mapsForStoryboard, of my current best attempt to extract rising and falling limbs from discharge record for W3. Plan to use to compare how the different methods work during rising, falling, and baseflow.
```{r identifying-events}
start <- ymd_hms("2023-7-20 00:00:00")
stop <- ymd_hms("2023-10-22 00:00:00")


q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#calculate baseflow
bf = baseflowB(q_23_plotting$Q_mm_day, alpha = 0.980)
#subtract baseflow from discharge
PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)
#plot the events
plotEvents(data = q_23_plotting$Q_mm_day, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = q_23_plotting$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm
PoT_res$srt

q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  arrange(datetime) %>% 
  unique() %>% 
  mutate(lab = as.numeric(row_number())) %>% 
  mutate(group = data.table::rleid(lab, cols = PoT_res$srt))

ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))

ranges[26,2] <- 5762
#trying fuzzy join method?
p_load(fuzzyjoin)
id_events <- q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  mutate(ID = row_number()) |> fuzzy_left_join(ranges, by = c(ID = "starts", ID = "stops"),
                              match_fun = list(`>=`, `<=`)) |> 
   mutate(event_ID = event_ID) |> 
   select(-starts, -stops)

id_events %>% 
  ggplot()+
  geom_line(aes(x = datetime, y = Q_mm_day, color = event_ID))


```

#all discharge stuff

```{r id-join-limbs}
library(dplyr)
library(purrr)


# Example: discharge dataframe with datetime and discharge
discharge_df <- q_23_plotting %>% 
  select(datetime, Q_mm_day)

# Copy of your event table
events <- limbs(data = q_23_plotting$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

discharge_df %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_type))+
  geom_point()

```

Combine the model results above, and the identified events
```{r}
comparison %>% 
  drop_na() %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  group_by(event_type, code) %>% 
  summarise(mean = mean(count)) %>% 
  ggplot()+
  geom_bar(aes(x = event_type, y = mean, fill = code), stat = "identity")

comparison %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  filter(code == "ommission") %>% 
  ggplot()+
  geom_line(aes(x = datetime, y = Q_mm_day, color = count))

#for the graph theory solution, the commission and ommission are symmetrical most of the time... I guess this makes sense if one is out of order, it causes another to be in order, like when I did the lagging method to calculate this
comparison %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  pivot_wider(names_from = code, values_from = count) %>% 
  mutate(diff = commission - ommission) %>% 
  filter(diff != 0) %>% View()


```


#Stage for FB and ZZ
```{r}
FB_air <- read_csv("./PressureTransducers_11_14_23/FB_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water <- read_csv("./PressureTransducers_11_14_23/FB_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  ggplot(aes(x = DATETIME, y = stage_cm))+
  geom_line()+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")

#exclude windows where they were not deployed
FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(stage_cm < 5) %>% View()

#7/22/2023 00:00:00
#9/20/23 00 - 2023-09-22 00:00:00
#2023-11-12 00:30:00

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  ggplot(aes(x = DATETIME, y = stage_cm))+
  geom_line()+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")

processed_fb_stage <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30))

q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")

```

```{r}
#Read in measurements from 2024

FB_air <- read_csv("./PressureTransducers_summer24/FB_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water <- read_csv("./PressureTransducers_summer24/FB_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  ggplot(aes(x = DATETIME, y = stage_cm))+
  geom_line()+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")

processed_fb_stage <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30))

q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")

#BIG PROBLEM
```

Big problem- I do not have stage measurements from the dilutions that Logan did in August... the pressure transducers are still in the streams, I can download them in July if they are still logging.

```{r}
#chunk that reads in stage, converts to proper units
#read in stage, convert to a height
ZZ_air <- read_csv("./PressureTransducers_11_14_23/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water <- read_csv("./PressureTransducers_11_14_23/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  ggplot(aes(x = DATETIME, y = stage_cm))+
  geom_line()+
  theme_classic()+
  labs(title = "ZZ Stage",
       x = "",
       y = "Stage (in)")
```

#Apply Botter model to evaluate sequences in other sheds
```{r W3}
cheap_model <- calc_model_result(W3_cheap$ID) %>% rename("cheapest" = pred_binary)
pk_model <- calc_model_result(W3_pk_seq$ID) %>% rename("pk" = pred_binary)
random_model <- calc_model_result(W3_random$ID) %>% rename("random" = pred_binary)
twi_model <- calc_model_result(w3_twi_sequence$ID) %>% rename("twi" = pred_binary)


W3_pk_seq <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_w3$ID), 1)) %>% 
  select(ID, sequence)

# calculate the number of flowing nodes at each timestep
number_activated <- 
  input_w3 %>% 
  #left_join(ZZ_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  #arrange(sequence) %>% 
      filter(binary == 1) %>% 
    summarise(number_flowing = length(binary))
#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy

#applying evaluation model using different sequences
calc_model_result <- function(input_sequence){
# 1. Ordered node sequence
node_sequence <- input_sequence

# 2. Dataframe with timestamps and how many nodes to activate
#defined previously
#number_activated

# 3. Build the activation matrix
activation_matrix <- t(sapply(number_activated$number_flowing, function(n) {
  active_flags <- rep(0, length(node_sequence))
  if (n > 0) active_flags[1:n] <- 1
  active_flags
}))

# 4. Assign column names and combine with timestamps
colnames(activation_matrix) <- node_sequence
activation_df <- cbind(number_activated["datetime"], as.data.frame(activation_matrix))

# View result
activation_final <- activation_df %>% pivot_longer(-datetime) %>% 
  rename("ID" = name, "pred_binary" = value) %>% 
  mutate(ID = as.numeric(ID))

return(activation_final)
}

cheap_model <- calc_model_result(W3_cheap$ID) %>% rename("cheapest" = pred_binary)
pk_model <- calc_model_result(W3_pk_seq$ID) %>% rename("pk" = pred_binary)
random_model <- calc_model_result(W3_random$ID) %>% rename("random" = pred_binary)
twi_model <- calc_model_result(w3_twi_sequence$ID) %>% rename("twi" = pred_binary)

comparison <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model, by = c("datetime", "ID")) %>% 
  left_join(pk_model, by = c("datetime", "ID")) %>% 
    left_join(random_model, by = c("datetime", "ID")) %>% 
  left_join(twi_model, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins) %>% 
  drop_na()

#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
get_eval_label(0,0)
#apply get_eval function to each model run
comparison$cheapest_eval <- mapply(get_eval_label, comparison$binary, comparison$cheapest)
comparison$pk_eval <- mapply(get_eval_label, comparison$binary, comparison$pk)
comparison$random_eval <- mapply(get_eval_label, comparison$binary, comparison$random)
comparison$twi_eval <- mapply(get_eval_label, comparison$binary, comparison$twi)


comparison$comparison_cheap <- with(comparison, ifelse(
  cheapest_eval == pk_eval, "equal",
  ifelse(cheapest_eval == "correct", "cheap_better",
         ifelse(pk_eval == "correct", "pk_better", "neither_better"))
))
comparison$comparison_random <- with(comparison, ifelse(
  random_eval == pk_eval, "equal",
  ifelse(random_eval == "correct", "cheap_better",
         ifelse(pk_eval == "correct", "pk_better", "neither_better"))
))
comparison$comparison_twi <- with(comparison, ifelse(
  twi == pk_eval, "equal",
  ifelse(twi == "correct", "cheap_better",
         ifelse(pk_eval == "correct", "pk_better", "neither_better"))
))


#summarize through time
df_summary_time <- comparison %>%
  group_by(datetime) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total
  )

df_summary_time <- df_summary_time %>%
  select(contains("accuracy"))
plot(df_summary_time)

summary_by_node <- comparison %>%
  group_by(ID) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    cheap_omission = sum(cheapest_eval == "omission"),
    pk_omission = sum(pk_eval == "omission"),
    random_omission = sum(random_eval == "omission"),
    twi_omission = sum(twi_eval == "omission"),
    cheap_commission = sum(cheapest_eval == "commission"),
    pk_commission = sum(pk_eval == "commission"),
    random_commission = sum(random_eval == "commission"),
    twi_commission = sum(twi_eval == "commission")
    #A_better_count = sum(comparison == "A_better"),
    #B_better_count = sum(comparison == "B_better"),
    #Equal_count = sum(comparison == "equal")
  )



#make data long for final plot
comparison_long <- 
  comparison %>% 
  select(datetime, ends_with("eval")) %>% 
    pivot_longer(-datetime) %>% 
  mutate(year = year(datetime))
#final plot
comparison_long %>% 
  #group_by(datetime) %>% 
  filter(year == 2023) %>% 
  mutate(total = as.numeric(length(value))) %>% 
  ggplot() +
  geom_histogram(aes(x = datetime, fill = value), bins = 100, na.rm = TRUE)+
  facet_grid(name~year)+
  labs(x = "", y = "Count")+
    ylim(0,1750)+
  theme(
  strip.background = element_blank(),
  strip.text.y = element_blank(),
  legend.position = "none"
)
comparison_long %>% 
  #group_by(datetime) %>% 
  filter(year == 2024) %>% 
  mutate(total = as.numeric(length(value))) %>% 
  ggplot() +
  geom_histogram(aes(x = datetime, fill = value), bins = 50, na.rm = TRUE)+
  facet_grid(name~year)+
  labs(x = "", y = "Count")+
  ylim(0,1750)+
  theme(
  strip.background = element_blank()#,
  #strip.text.y = element_blank(),
  #legend.position = "none"
)

comparison_long %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  #group_by(datetime) %>% 
  #filter(year == 2023) %>% 
  mutate(total = as.numeric(length(value))) %>% 
  ggplot() +
  geom_histogram(aes(x = day_of_year, fill = value), binwidth = 1, na.rm = TRUE)+
  facet_grid(name~year)



```
```{r accuracy-W3}
w3_acc <- rbind(produce_metrics(W3_cheap, "W3", "cheapest_insertion"),
      produce_metrics(W3_pk_seq, "W3", "pk"),
      produce_metrics(W3_random, "W3", "random"),
      produce_metrics(w3_twi_sequence, "W3", "twi")
      )
kable(w3_acc, digits = 2)

```

Define sequences to test for FB
```{r}
#make a df with ID and the number in the sequence
FB_pk_seq <- pks_fb %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_fb$ID), 1)) %>% 
  select(ID, sequence)
```

```{r FB}
#chains used in graph theory solutions
FB_cheap <- chain_solution(FB_IDs, "FB", methods = "cheapest_insertion")
FB_random <- chain_solution(FB_IDs, "FB", methods = "random")

ZZ_cheap <- chain_solution(ZZ_IDs, "ZZ", methods = "cheapest_insertion")
ZZ_random <- chain_solution(ZZ_IDs, "ZZ", methods = "random")

# calculate the number of flowing nodes at each timestep
number_activated <- 
  input_fb %>% 
  left_join(FB_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  #arrange(sequence) %>% 
      filter(binary == 1) %>% 
    summarise(number_flowing = length(binary))
#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy

#applying evaluation model using different sequences
calc_model_result <- function(input_sequence){
# 1. Ordered node sequence
node_sequence <- input_sequence

# 2. Dataframe with timestamps and how many nodes to activate
#defined previously
#number_activated

# 3. Build the activation matrix
activation_matrix <- t(sapply(number_activated$number_flowing, function(n) {
  active_flags <- rep(0, length(node_sequence))
  if (n > 0) active_flags[1:n] <- 1
  active_flags
}))

# 4. Assign column names and combine with timestamps
colnames(activation_matrix) <- node_sequence
activation_df <- cbind(number_activated["datetime"], as.data.frame(activation_matrix))

# View result
activation_final <- activation_df %>% pivot_longer(-datetime) %>% 
  rename("ID" = name, "pred_binary" = value) %>% 
  mutate(ID = as.numeric(ID))

return(activation_final)
}

cheap_model <- calc_model_result(FB_cheap$ID) %>% rename("cheapest" = pred_binary)
pk_model <- calc_model_result(FB_pk_seq$ID) %>% rename("pk" = pred_binary)
random_model <- calc_model_result(FB_random$ID) %>% rename("random" = pred_binary)
#defined twi sequence earlier
twi_model <- calc_model_result(fb_twi_sequence$ID) %>% rename("twi" = pred_binary)

comparison <- input_fb %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model, by = c("datetime", "ID")) %>% 
  left_join(pk_model, by = c("datetime", "ID")) %>% 
    left_join(random_model, by = c("datetime", "ID")) %>% 
  left_join(twi_model, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins) %>% 
  drop_na()

#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
get_eval_label(0,0)
#apply get_eval function to each model run
comparison$cheapest_eval <- mapply(get_eval_label, comparison$binary, comparison$cheapest)
comparison$pk_eval <- mapply(get_eval_label, comparison$binary, comparison$pk)
comparison$random_eval <- mapply(get_eval_label, comparison$binary, comparison$random)
comparison$twi_eval <- mapply(get_eval_label, comparison$binary, comparison$twi)


comparison$comparison_cheap <- with(comparison, ifelse(
  cheapest_eval == pk_eval, "equal",
  ifelse(cheapest_eval == "correct", "cheap_better",
         ifelse(pk_eval == "correct", "pk_better", "neither_better"))
))
comparison$comparison_random <- with(comparison, ifelse(
  random_eval == pk_eval, "equal",
  ifelse(random_eval == "correct", "cheap_better",
         ifelse(pk_eval == "correct", "pk_better", "neither_better"))
))
comparison$comparison_twi <- with(comparison, ifelse(
  twi == pk_eval, "equal",
  ifelse(twi == "correct", "cheap_better",
         ifelse(pk_eval == "correct", "pk_better", "neither_better"))
))


summary_by_node <- comparison %>%
  group_by(ID) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    cheap_omission = sum(cheapest_eval == "omission"),
    pk_omission = sum(pk_eval == "omission"),
    random_omission = sum(random_eval == "omission"),
    twi_omission = sum(twi_eval == "omission"),
    cheap_commission = sum(cheapest_eval == "commission"),
    pk_commission = sum(pk_eval == "commission"),
    random_commission = sum(random_eval == "commission"),
    twi_commission = sum(twi_eval == "commission")
    #A_better_count = sum(comparison == "A_better"),
    #B_better_count = sum(comparison == "B_better"),
    #Equal_count = sum(comparison == "equal")
  )


ggplot(summary_by_node, aes(x=A_correct, y=B_correct, label=ID)) +
  geom_point() +
  geom_abline(slope=1, intercept=0, linetype="dashed", color="red") +
  geom_text(nudge_y=1.5) +
  labs(title="Model Comparison per Node", x="Model A Correct Predictions", y="Model B Correct Predictions")



#summarize through time
df_summary_time <- comparison %>%
  group_by(datetime) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total
  )

df_summary_time <- df_summary_time %>%
  select(contains("accuracy"))
plot(df_summary_time)

df_long <- df_summary_time %>%
  select(datetime, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy), names_to = "model", values_to = "accuracy")

ggplot(df_long, aes(x = datetime, y = accuracy, color = model)) +
  geom_line() +
  labs(title = "Model Accuracy Over Time", y = "Accuracy", x = "Time")

ggplot(df_summary_time, aes(x = datetime, y = diff)) +
  geom_line() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Difference in Accuracy (A - B)", y = "Accuracy Difference", x = "Time")

#plot the difference in performance versus discharge
df_summary_time %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  #rename("Hydrograph Component", event_type)
  #group_by(event_type, code) %>% 
  #summarise(mean = mean(count)) %>% 
  ggplot(aes(x = Q_mm_day, y = diff, color = event_type)) +
  geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Difference in Accuracy (Cheapest - Prop of time flowing)", y = " ", x = "Discharge (mm/day)")+
   scale_color_manual(values = c("#397367", "#FFA400", "#93C2F1"))+
  labs(color='Hydrograph Component') +
  theme_classic()

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")

#plot of the accuracy versus discharge, colored by scheme, shape by hydrograph component
df_long %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  #rename("Hydrograph Component", event_type)
  #group_by(event_type, code) %>% 
  #summarise(mean = mean(count)) %>% 
  ggplot(aes(x = Q_mm_day, y = accuracy, color = model, shape = event_type)) +
  geom_point(alpha = 0.3, size = 2) +
    #geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Accuracy (correctly modeled states/total)", y = " ", x = "Discharge (mm/day)")+
   scale_color_manual(values = four_colors)+
  labs(color='Hydrograph Component') +
  theme_classic()+
  facet_wrap(~model)


#original way of comparing- the grid
comparison_long <- 
  comparison %>% 
  select(datetime, ends_with("eval")) %>% 
    pivot_longer(-datetime) %>% 
  mutate(year = year(datetime))

# filter(datetime >= ymd_hms("2023-07-24 00:00:00") & datetime <= ymd_hms("2023-08-20 00:00:00")) %>% 
comparison_long %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  #group_by(datetime) %>% 
  #filter(year == 2023) %>% 
  mutate(total = as.numeric(length(value))) %>% 
  ggplot() +
  geom_histogram(aes(x = day_of_year, fill = value), binwidth = 1, na.rm = TRUE)+
  facet_grid(name~year)
```
```{r accuracy-FB}
fb_acc <- rbind(produce_metrics(FB_cheap, "FB", "cheapest_insertion"),
      produce_metrics(FB_pk_seq, "FB", "pk"),
      produce_metrics(FB_random, "FB", "random"),
      produce_metrics(fb_twi_sequence, "FB", "twi")
      )
kable(fb_acc, digits = 2)

```
Now do again for ZZ
```{r}
ZZ_cheap <- chain_solution(ZZ_IDs, "ZZ", methods = "cheapest_insertion")
ZZ_random <- chain_solution(ZZ_IDs, "ZZ", methods = "random")

ZZ_pk_seq <- pks_zz %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_zz$ID), 1)) %>% 
  select(ID, sequence)

# calculate the number of flowing nodes at each timestep
number_activated <- 
  input_zz %>% 
  #left_join(ZZ_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  #arrange(sequence) %>% 
      filter(binary == 1) %>% 
    summarise(number_flowing = length(binary))
#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy

#applying evaluation model using different sequences
calc_model_result <- function(input_sequence){
# 1. Ordered node sequence
node_sequence <- input_sequence

# 2. Dataframe with timestamps and how many nodes to activate
#defined previously
#number_activated

# 3. Build the activation matrix
activation_matrix <- t(sapply(number_activated$number_flowing, function(n) {
  active_flags <- rep(0, length(node_sequence))
  if (n > 0) active_flags[1:n] <- 1
  active_flags
}))

# 4. Assign column names and combine with timestamps
colnames(activation_matrix) <- node_sequence
activation_df <- cbind(number_activated["datetime"], as.data.frame(activation_matrix))

# View result
activation_final <- activation_df %>% pivot_longer(-datetime) %>% 
  rename("ID" = name, "pred_binary" = value) %>% 
  mutate(ID = as.numeric(ID))

return(activation_final)
}

cheap_model <- calc_model_result(ZZ_cheap$ID) %>% rename("cheapest" = pred_binary)
pk_model <- calc_model_result(ZZ_pk_seq$ID) %>% rename("pk" = pred_binary)
random_model <- calc_model_result(ZZ_random$ID) %>% rename("random" = pred_binary)
#defined twi sequence earlier
twi_model <- calc_model_result(zz_twi_sequence$ID) %>% rename("twi" = pred_binary)

comparison <- input_zz %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model, by = c("datetime", "ID")) %>% 
  left_join(pk_model, by = c("datetime", "ID")) %>% 
    left_join(random_model, by = c("datetime", "ID")) %>% 
  left_join(twi_model, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins) %>% 
  drop_na()

#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
get_eval_label(0,0)
#apply get_eval function to each model run
comparison$cheapest_eval <- mapply(get_eval_label, comparison$binary, comparison$cheapest)
comparison$pk_eval <- mapply(get_eval_label, comparison$binary, comparison$pk)
comparison$random_eval <- mapply(get_eval_label, comparison$binary, comparison$random)
comparison$twi_eval <- mapply(get_eval_label, comparison$binary, comparison$twi)


comparison$comparison_cheap <- with(comparison, ifelse(
  cheapest_eval == pk_eval, "equal",
  ifelse(cheapest_eval == "correct", "cheap_better",
         ifelse(pk_eval == "correct", "pk_better", "neither_better"))
))
comparison$comparison_random <- with(comparison, ifelse(
  random_eval == pk_eval, "equal",
  ifelse(random_eval == "correct", "cheap_better",
         ifelse(pk_eval == "correct", "pk_better", "neither_better"))
))
comparison$comparison_twi <- with(comparison, ifelse(
  twi == pk_eval, "equal",
  ifelse(twi == "correct", "cheap_better",
         ifelse(pk_eval == "correct", "pk_better", "neither_better"))
))


#summarize through time
df_summary_time <- comparison %>%
  group_by(datetime) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total
  )

df_summary_time <- df_summary_time %>%
  select(contains("accuracy"))
plot(df_summary_time)

summary_by_node <- comparison %>%
  group_by(ID) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    cheap_omission = sum(cheapest_eval == "omission"),
    pk_omission = sum(pk_eval == "omission"),
    random_omission = sum(random_eval == "omission"),
    twi_omission = sum(twi_eval == "omission"),
    cheap_commission = sum(cheapest_eval == "commission"),
    pk_commission = sum(pk_eval == "commission"),
    random_commission = sum(random_eval == "commission"),
    twi_commission = sum(twi_eval == "commission")
    #A_better_count = sum(comparison == "A_better"),
    #B_better_count = sum(comparison == "B_better"),
    #Equal_count = sum(comparison == "equal")
  )



#make data long for final plot
comparison_long <- 
  comparison %>% 
  select(datetime, ends_with("eval")) %>% 
    pivot_longer(-datetime) %>% 
  mutate(year = year(datetime))
#final plot
comparison_long %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  #group_by(datetime) %>% 
  #filter(year == 2023) %>% 
  mutate(total = as.numeric(length(value))) %>% 
  ggplot() +
  geom_histogram(aes(x = day_of_year, fill = value), binwidth = 1, na.rm = TRUE)+
  facet_grid(name~year)

comparison

```

chunk to calculate accuracy metrics and confusion matrices
```{r}
# Load required packages
p_load(caret)

# ---- STEP 1: Assume your data is already loaded into `df` ----
# df should have columns: binary, cheapest, pk, random, twi
df <- comparison

# ---- STEP 2: Create confusion matrices ----
conf_cheapest <- table(Predicted = df$cheapest, Actual = df$binary)
conf_pk       <- table(Predicted = df$pk,       Actual = df$binary)
conf_random   <- table(Predicted = df$random,   Actual = df$binary)
conf_twi      <- table(Predicted = df$twi,      Actual = df$binary)

# ---- STEP 3: Define helper function to extract performance metrics ----
get_metrics <- function(pred, true) {
  cm <- confusionMatrix(factor(pred), factor(true), positive = "1")
  data.frame(
    Accuracy = cm$overall["Accuracy"],
    Kappa = cm$overall["Kappa"],
    Sensitivity = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    Precision = cm$byClass["Precision"],
    F1 = cm$byClass["F1"]
  )
}

# ---- STEP 4: Summarize performance for all models ----
metrics_df <- rbind(
  cheapest = get_metrics(df$cheapest, df$binary),
  pk       = get_metrics(df$pk, df$binary),
  random   = get_metrics(df$random, df$binary),
  twi      = get_metrics(df$twi, df$binary)
)

# Add model name as a column
metrics_df <- metrics_df %>% 
  mutate(model = rownames(metrics_df)) %>%
  select(model, everything())

# Print the performance summary
kable(metrics_df, digits = 2)

# ---- STEP 5: Define function to plot confusion matrix ----
plot_conf_mat <- function(cm, title = "Confusion Matrix") {
  cm_df <- as.data.frame(cm)
  ggplot(cm_df, aes(x = Actual, y = Predicted)) +
    geom_tile(aes(fill = Freq), color = "white") +
    geom_text(aes(label = Freq)) +
    scale_fill_gradient(low = "white", high = "steelblue", limits = c(0, 100000)) +
    labs(title = title, fill = "Count") +
    theme_minimal()
}

# ---- STEP 6: Plot confusion matrices for each model ----
cheap_plot <- plot_conf_mat(conf_cheapest, title = "Cheapest") + theme(legend.position = "none")
pk_plot <- plot_conf_mat(conf_pk,       title = "PK")+ theme(legend.position = "none")
random_plot <- plot_conf_mat(conf_random,   title = "Random")+ theme(legend.position = "none")
twi_plot <- plot_conf_mat(conf_twi,      title = "TWI")

(cheap_plot + pk_plot) / (random_plot + twi_plot)
```


```{r}
W3_cheap <- chain_solution(W3_IDs, "W3", methods = "cheapest_insertion")
W3_random <- chain_solution(W3_IDs, "W3", methods = "random")
```

Create a function that will compare a chain output and it's sequential wetting model to the actual observations.
```{r}
methods
#function to determine a chain solution
ZZ_cheap <- chain_solution(ZZ_IDs, "ZZ", methods = "cheapest_insertion")

#function that will take a chain solution, and calculate accuracy metrics
produce_metrics <- function(chain, shed, method){
# chain <- ZZ_cheap
# shed <- "ZZ"
# method = "cheapest_insertion"
# calculate the number of flowing nodes at each timestep
  if(shed == "W3"){
    number_activated <-
      input_w3 %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  } 
  else if(shed == "FB"){
    number_activated <-
      input_fb %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  }
  else if(shed == "ZZ"){
    number_activated <-
      input_zz %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  }

#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy

#applying evaluation model using different sequences
  calc_model_result <- function(input_sequence){
# 1. Ordered node sequence
node_sequence <- input_sequence

# 2. Dataframe with timestamps and how many nodes to activate
#defined previously
#number_activated

# 3. Build the activation matrix
activation_matrix <- t(sapply(number_activated$number_flowing, function(n) {
  active_flags <- rep(0, length(node_sequence))
  if (n > 0) active_flags[1:n] <- 1
  active_flags
}))

# 4. Assign column names and combine with timestamps
colnames(activation_matrix) <- node_sequence
activation_df <- cbind(number_activated["datetime"], as.data.frame(activation_matrix))

# View result
activation_final <- activation_df %>% pivot_longer(-datetime) %>% 
  rename("ID" = name, "pred_binary" = value) %>% 
  mutate(ID = as.numeric(ID))

return(activation_final)
}

  model_result <- calc_model_result(chain$ID)

#combine all model results
  if(shed == "W3"){
    comparison <- input_w3 %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  } 
  else if(shed == "FB"){
    comparison <- input_fb %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  }
  else if(shed == "ZZ"){
    comparison <- input_zz %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  }


#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
#get_eval_label(0,0)
#apply get_eval function to each model run
comparison$pred_eval <- mapply(get_eval_label, 
                                   comparison$binary, 
                                   comparison$pred_binary)

# ---- STEP 3: Define helper function to extract performance metrics ----
get_metrics <- function(pred, true) {
  cm <- confusionMatrix(factor(pred), factor(true), positive = "1")
  data.frame(
    Accuracy = cm$overall["Accuracy"],
    Kappa = cm$overall["Kappa"],
    Sensitivity = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    Precision = cm$byClass["Precision"],
    F1 = cm$byClass["F1"]
  )
}

# ---- STEP 4: Summarize performance for all models ----
metrics_df <- tibble(get_metrics(comparison$pred_binary, comparison$binary)) %>% 
  mutate(shed = shed,
         method = method)
#output <- list(metrics = metrics_df, sequence = ZZ_cheap)
#make data long for final plot
return(metrics_df)
}

produce_metrics(ZZ_cheap, "ZZ", "cheapest_insertion")

#testing every method
for(i in 1:length(methods)){
  chain <- chain_solution(ZZ_IDs, "ZZ", methods = methods[i])
  
  output <- produce_metrics(chain, "ZZ", methods[i])
  
  if(i == 1) many_chains <- output
  if(i > 1) many_chains <- rbind(many_chains, output)
}

all_methods_acc <- many_chains
```
What if we test 100 different cheapest_insertions?
```{r}
many_cheap <- rep("cheapest_insertion", 10)
for(i in 1:length(many_cheap)){
  chain <- chain_solution(ZZ_IDs, "ZZ", methods = many_cheap[i])
  
  output <- produce_metrics(chain, "ZZ", many_cheap[i])
  
  if(i == 1) many_chains <- output
  if(i > 1) many_chains <- rbind(many_chains, output)
}

many_chains
```

