---
title: "ComparingSequences"
format: html
editor_options: 
  chunk_output_type: console
---

5/27/25
New clean script for analysis determining ideal sequences for each watershed, and comparing them to sequences that are random, and dictated by proportion of time flowing, and topography.  

Last markdown document was almost 10,000 lines, and getting unwieldy to navigate.  

Explanation of graph theory stuff: finding the best path using the traveling salesman's problem. Usually this provides a cycle, but by making a dummy node at the end and then removing it we can trick the algorithm to produce a very efficient 1-way path.  

Not sure if I should include the dummy or not; FB is a lot better with a dummy, but W3 and ZZ are a lot better without dummies at the end.

6/30/25
Formatted, should contain everything needed for current analysis. Have not checked for dependencies in other scripts. 

# Setup and Preparing inputs
```{r setup}
#loading packages
library(pacman)
p_load(tidyverse, terra, tidyterra, whitebox, scales, wesanderson, caret, plotly,ggnewscale, sf, elevatr, patchwork, ggspatial, zoo, igraph, TSP, ggnetwork, intergraph, ggpubr, knitr, purrr, spatialEco, hydroEvents,
       maptools)
#whitebox::install_whitebox()

#reading in final format data for summer 23
data_23 <- read_csv("./DataForMary/HB_stic.csv")
#reading in final format data for summer 24
data_24 <- read_csv("./summer2024/STICS2024.csv")

data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

bind23 <- data_23 %>% 
  select(datetime, ID, wshed, binary, mins)
bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)
```

Preparing the STIC dataset as inputs for everything I am doing.  
```{r prepare-inputs}
#create input that only uses sensors that were deployed during both deployments
#need to make a list of sensors deployed in both campaigns for each watershed
#W3
w3_deployed24 <- unique(filter(data_24, wshed == "W3")$number)
w3_deployed23 <- unique(filter(data_23, wshed == "W3")$ID)
W3_IDs <- intersect(w3_deployed24, w3_deployed23)
#FB
fb_deployed24 <- unique(filter(data_24, wshed == "FB")$number)
fb_deployed23 <- unique(filter(data_23, wshed == "FB")$ID)
FB_IDs <- intersect(fb_deployed24, fb_deployed23)
#ZZ
zz_deployed24 <- unique(filter(data_24, wshed == "ZZ")$number)
zz_deployed23 <- unique(filter(data_23, wshed == "ZZ")$ID)
ZZ_IDs <- intersect(zz_deployed24, zz_deployed23)


#detach("package:fitdistrplus")
#detach("package:MASS")


bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)%>% 
  filter(mins %in% c(0, 30))


bind23 <- data_23 %>% 
  filter(ID %in% intersect(bind24$ID, data_23$ID)) %>% 
  select(datetime, ID, wshed, binary, mins) %>% 
  filter(mins %in% c(0, 30))

#filter by each watershed, then recombine at the end
input_w3 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3") %>% 
  filter(ID %in% W3_IDs) %>% 
  drop_na()
input_fb <- rbind(bind23, bind24) %>%
  filter(wshed == "FB") %>% 
  filter(ID %in% FB_IDs)
input_zz <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ") %>% 
  filter(ID %in% ZZ_IDs)

input_all <- rbind(input_w3, input_fb, input_zz)
write_csv(input_w3, "calc_support_inputs_w3.csv")
write_csv(input_fb, "calc_support_inputs_fb.csv")
write_csv(input_zz, "calc_support_inputs_zz.csv")

      
```

Define functions that calculate proportion of time that nodes wet and dry sequentially.  
```{r define-functions}
calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  sequence_df <- data.frame("Sequence" = NA, 
                            "Frequency" = NA,
                            "up" = up,
                            "down" = down)
  return(sequence_df)
} 
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")


sub <- filter(sequence_df, Sequence %in% supports)
output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)}
}

#test function

#calc_support_combos("r_18", "r_11", input_test)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_combos <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support_combos(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

#iterate_groups("r_13", "r_19", input, min(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_combos <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- iterate_groups_combos(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down, Sequence) %>%
    summarise(Frequency = sum(Frequency))
  return(final_output)
}

#fantastic four function has been modified, to do average daily state. Removed hourly and 4 hr time blocks
fantastic_four_combos <- function(routes, shed){
  theFour <- c("30mins", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed) %>% 
      mutate(
             "day" = day(datetime),
             "month" = month(datetime),
             "year" = year(datetime)) %>% 
      group_by(day, month, year, ID) %>%
      summarise(avg_state = mean(binary)) %>% 
      ungroup() %>% 
      mutate(avg_state = round(avg_state)) %>% 
      rename("binary" = avg_state) %>% 
      mutate("datetime" = ymd_hms(paste0(year,"-",month,"-",day," ","00:00:00"))) %>% 
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
          arrange(datetime) %>% 
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_combos(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}

# function to determine proportion of transitions that state changes follow all parent child relationships
calc_props <- function(routes, shed){
  full_combos <- fantastic_four_combos(routes, shed)
total_state_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    group_by(up, down, timescale, shed) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down, timescale, shed) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down", "shed", "timescale")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
return(un_split)
}
```

## Discharge and Stage
### W3 Discharge
```{r prepare-discharge-W3}
#from TestingFrameworks script

#read in discharge from W3-- input to Carrie's model, discharge in L/s
#q <- read_csv("https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-hbr.1.17&entityid=efc477b3ef1bb3dd8b9355c9115cd849")
#write.csv(q, "HB_5minQ.csv")
q <- read_csv("HB_5minQ.csv")

#input discharge needs to be in mm/day?
#reference to understand difference between daily mean and instantaneous streamflow:
#https://hydrofunctions.readthedocs.io/en/master/notebooks/DailyMean_vs_Instant.html

#creating minute column, used to filter out higher temporal resolution measurements for plotting
data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

#find the range of dates that I need discharge for
start <- min(data_23$datetime)
stop <- max(data_23$datetime)

#filtering discharge down to the range of dates
q_23 <- q %>% 
  filter(DATETIME > start & DATETIME < stop) %>% 
  #convert to mm/day.
  #converting instantaneous streamflow to mm/day by taking measurement, and scaling   it up as if that was the discharge for the whole day. It is not, it is just at that   moment, but should fix any units/order of magnitude issues
  mutate("Q_mm_day" = Discharge_ls * 0.001 * 86400 / 420000 * 1000) 
q_23$mins <- minute(q_23$DATETIME)

#removing times that are not coincident with STIC observations
q_23_f <- filter(q_23, mins %in% c(0, 30))

ggplot(q_23_f, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#also read in provisional 2024 data
q_24 <- read_csv("w3_discharge_24.csv")

#find the range of dates that I need discharge for
start24 <- ymd_hms("2024-05-15 00:00:00 UTC")
stop24 <- max(data_24$datetime)

#filtering discharge down to the range of dates
q_24 <- q_24 %>% 
  mutate(datetime = mdy_hm(datetime)) %>% 
  filter(datetime > start24 & datetime < stop24)  
q_24$mins <- minute(q_24$datetime)

#removing times that are not coincident with STIC observations
q_24_f <- filter(q_24, mins %in% c(0, 30))

ggplot(q_24_f, aes(x  = datetime, y = Q_mmperday))+
  geom_line()+
  labs(title = "Discharge from W3, May to July 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#get discharge ready to bind to my other data
q_23_bind <- 
  q_23_f %>% 
  select(DATETIME, Q_mm_day) %>% 
  rename("datetime" = DATETIME
         )

q_24_bind <- 
  q_24_f %>% 
  select(datetime, Q_mmperday) %>% 
  rename("Q_mm_day" = Q_mmperday)
```
  
Chunk straight from mapsForStoryboard, of my current best attempt to extract rising and falling limbs from discharge record for W3. Plan to use to compare how the different methods work during rising, falling, and baseflow.
```{r identifying-events}
start <- ymd_hms("2023-7-20 00:00:00")
stop <- ymd_hms("2023-10-22 00:00:00")


q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#calculate baseflow
#old values greyed
#bf = baseflowB(q_23_plotting$Q_mm_day, alpha = 0.980)
bf = baseflowB(q_23_plotting$Q_mm_day, alpha = 0.925)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 0.5, min.diff = 30)
#plot the events
plotEvents(data = q_23_plotting$Q_mm_day, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = q_23_plotting$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm
PoT_res$srt

q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  arrange(datetime) %>% 
  unique() %>% 
  mutate(lab = as.numeric(row_number())) %>% 
  mutate(group = data.table::rleid(lab, cols = PoT_res$srt))

ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))

ranges[26,2] <- 5762
#trying fuzzy join method?
p_load(fuzzyjoin)
id_events <- q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  mutate(ID = row_number()) |> fuzzy_left_join(ranges, by = c(ID = "starts", ID = "stops"),
                              match_fun = list(`>=`, `<=`)) |> 
   mutate(event_ID = event_ID) |> 
   select(-starts, -stops)

id_events %>% 
  ggplot()+
  geom_point(aes(x = datetime, y = Q_mm_day, color = event_ID))

id_events <- q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  mutate(ID = row_number()) |> fuzzy_left_join(ranges, by = c(ID = "starts", ID = "stops"),
                              match_fun = list(`>=`, `<=`)) |> 
   mutate(event_ID = event_ID) |> 
   select(-starts, -stops)

id_events %>% 
  ggplot()+
  geom_point(aes(x = datetime, y = Q_mm_day, color = event_ID))


```

```{r W3-2023}
# Example: discharge dataframe with datetime and discharge
discharge_df <- q_23_plotting %>% 
  select(datetime, Q_mm_day)

# Copy of your event table
events <- limbs(data = q_23_plotting$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

#save output for 2023
discharge_df_23 <- discharge_df


discharge_df %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_type))+
  geom_point()

discharge_df %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_id))+
  geom_point()

```
```{r W3-2024}
#identifying events
bf = baseflowB(q_24_bind$Q_mm_day, alpha = 0.925)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(q_24_bind$Q_mm_day - bf$bf, threshold = 0.25, min.diff = 30)
#plot the events
plotEvents(data = q_24_bind$Q_mm_day, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = q_24_bind$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- q_24_bind

# Copy of your event table
events <- limbs(data = q_24_bind$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

discharge_df_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_type))+
  geom_point()

w3_limbs <- rbind(discharge_df_24, discharge_df_23)

```

Doing a thing where I define a sequence based on the activation order during events
```{r}
events

discharge_df <- q_23_plotting %>% 
  select(datetime, Q_mm_day)

discharge_df$event_id <- NA_integer_

for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:events[i+1, ]$ris.srt] <- i  # Event ID for rising+falling
}

discharge_df %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = as.character(event_id)))+
  geom_point()



#now, take this new dataframe and for each event find the start of flow
ttt <- 
input_w3 %>%
  inner_join(discharge_df,  by = c("datetime")) %>% 
    filter(binary == 1) %>% 
    group_by(ID, event_id) %>% 
    summarise(start_of_flow = min(datetime)) %>% 
    arrange(start_of_flow)

#write a for loop to go through event ids, calculate the sequence for each event, and then find an "average" sequence
for(i in 1:length(unique(ttt$event_id))){
  event <- unique(ttt$event_id)[i]
  
  sequenced_event <- filter(ttt, event_id == event) %>% 
    arrange(start_of_flow) %>% 
      rowid_to_column("Sequence")
  
  if(i == 1) all_events <- sequenced_event
  if(i > 1) all_events <- rbind(all_events, sequenced_event)
}
  mutate(Sequence = )
  group_by(ID) %>% 
  summarise(mean(Sequence))
```


Combine the model results above, and the identified events
```{r}
comparison %>% 
  drop_na() %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  group_by(event_type, code) %>% 
  summarise(mean = mean(count)) %>% 
  ggplot()+
  geom_bar(aes(x = event_type, y = mean, fill = code), stat = "identity")

comparison %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  filter(code == "ommission") %>% 
  ggplot()+
  geom_line(aes(x = datetime, y = Q_mm_day, color = count))

#for the graph theory solution, the commission and ommission are symmetrical most of the time... I guess this makes sense if one is out of order, it causes another to be in order, like when I did the lagging method to calculate this
comparison %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  pivot_wider(names_from = code, values_from = count) %>% 
  mutate(diff = commission - ommission) %>% 
  filter(diff != 0) %>% View()


```
### Stage for FB and ZZ
```{r FB-2023}
FB_air <- read_csv("./PressureTransducers_11_14_23/FB_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water <- read_csv("./PressureTransducers_11_14_23/FB_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  ggplot(aes(x = DATETIME, y = stage_cm))+
  geom_line()+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")

#exclude windows where they were not deployed
FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(stage_cm < 5)

#7/22/2023 00:00:00
#9/20/23 00 - 2023-09-22 00:00:00
#2023-11-12 00:30:00

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>%
  ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")

processed_fb_stage <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()


#output processed salt dilutions
# q_combined %>% filter(shed == "FB") %>% 
#   mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
#   rename(DATETIME = datetime) %>% 
#   left_join(processed_fb_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_fb_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_fb_stage$roll_mean - bf$bf, threshold = 0.4, min.diff = 20)
#plot the events
plotEvents(data = processed_fb_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_fb_stage

# Copy of your event table
events <- limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_df_23 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()


```
```{r FB-2024}
#Read in measurements from 2024

FB_air <- read_csv("./PressureTransducers_summer24/FB_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water <- read_csv("./PressureTransducers_summer24/FB_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

# processed salt dilutions
dilutions <- q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")
dilutions

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701,
         #apply a rolling mean to get rid of temperature artifacts
         roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+
  
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")+
  geom_point(data = dilutions,
             aes(x = DATETIME, y = stage_cm),
             color = "blue")





processed_fb_stage <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    filter(DATETIME <= ymd_hms("2024-07-21 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()


#output processed salt dilutions
q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_fb_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_fb_stage$roll_mean - bf$bf, threshold = 0.2, min.diff = 20)
#plot the events
plotEvents(data = processed_fb_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_fb_stage

# Copy of your event table
events <- limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_df_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()

fb_limbs <- rbind(stage_df_24, stage_df_23) %>% 
  rename("datetime" = DATETIME)
#BIG PROBLEM
```

```{r ZZ-2023}
#chunk that reads in stage, converts to proper units
#read in stage, convert to a height
ZZ_air <- read_csv("./PressureTransducers_11_14_23/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water <- read_csv("./PressureTransducers_11_14_23/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))


processed_zz_stage <- ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>%
  #filter(DATETIME <= ymd_hms("2024-07-21 10:30:00 UTC")) %>% 
   filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  drop_na()

#View(processed_zz_stage)
processed_zz_stage %>%
  ggplot()+
    geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+  theme_classic()+
  labs(title = "ZZ Stage",
       x = "",
       y = "Stage (in)")


#identifying events
bf = baseflowB(processed_zz_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_zz_stage$roll_mean - bf$bf, threshold = 0.25, min.diff = 85)
#plot the events
plotEvents(data = processed_zz_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_zz_stage

# Copy of your event table
events <- limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_23 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()


#BIG PROBLEM
```
```{r ZZ-2024}
#chunk that reads in stage, converts to proper units
#read in stage, convert to a height
ZZ_air <- read_csv("./PressureTransducers_summer24/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water <- read_csv("./PressureTransducers_summer24/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))


processed_zz_stage <- ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>%
  filter(DATETIME <= ymd_hms("2024-07-21 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()

processed_zz_stage %>% ggplot()+
    geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+  theme_classic()+
  labs(title = "ZZ Stage",
       x = "",
       y = "Stage (in)")

#output processed salt dilutions
# q_combined %>% filter(shed == "ZZ") %>% 
#   mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
#   rename(DATETIME = datetime) %>% 
#   left_join(processed_zz_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_zz_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_zz_stage$roll_mean - bf$bf, threshold = 0.25, min.diff = 85)
#plot the events
plotEvents(data = processed_zz_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_zz_stage

# Copy of your event table
events <- limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()

zz_limbs <- rbind(stage_24, stage_23)%>% 
  rename("datetime" = DATETIME)
#BIG PROBLEM

```

### Number of transitions in each component of hydrograph
```{r W3}
#make companion plot that shows the number of transitions during each part of hydrograph
test <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>%
  group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

num_trans_w3 <- test %>% 
  group_by(datetime, state_change) %>% 
  summarise(number_of_nodes = length(state_change))

test %>% 
  inner_join(discharge_df, by = "datetime") %>%
  group_by(state_change, event_type) %>% 
  summarise(count = n()) %>% 
  pivot_wider(names_from = state_change, values_from = count) %>% 
  kable(format = "markdown")


```
```{r FB}
num_trans_fb <- input_fb %>% 
  filter(mins %in% c(0, 30)) %>%
  group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

num_trans_fb$state_change <- "none"
num_trans_fb$state_change[num_trans_fb$transition == -1] <- "wetting"
num_trans_fb$state_change[num_trans_fb$transition == 1] <- "drying"

num_trans_fb <- num_trans_fb %>% 
  group_by(datetime, state_change) %>% 
  summarise(number_of_nodes = length(state_change))
```
```{r ZZ}
num_trans_zz <- input_zz %>% 
  filter(mins %in% c(0, 30)) %>%
  group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

num_trans_zz$state_change <- "none"
num_trans_zz$state_change[num_trans_zz$transition == -1] <- "wetting"
num_trans_zz$state_change[num_trans_zz$transition == 1] <- "drying"

num_trans_zz <- num_trans_zz %>% 
  group_by(datetime, state_change) %>% 
  summarise(number_of_nodes = length(state_change))
```
# Graph theory sequences
## Determine all possible combinations, or the full graph
Determine all possible combinations
```{r W3}
input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- W3_IDs

#create empty list to hold repeated node IDs
zzz <- length(combos)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}
#create data frame with all possible combinations
W3_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#Run suite of functions to determine proportion of time that state changes follow the parent -> child relationship
W3_all_combos <- calc_props(W3_combos_routes, "W3")

#general_graph(W3_IDs, "W3", methods = c("cheapest_insertion"), two_opt = TRUE)
```

What if I tried to show this in some way- map where each sensor was the average prop value when it is a parent?
```{r}
parents <- W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  rename(parent = down, child = up) %>% 
  group_by(parent) %>% 
  summarise(mean_parent = mean(prop)) %>% 
  rename(ID = parent)
  
childs <- W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  rename(parent = down, child = up) %>% 
  group_by(child) %>% 
  summarise(mean_child = mean(prop)) %>% 
  rename(ID = child)

left_join(parents, childs, by = "ID") %>% 
  ggplot(aes(x = mean_parent, y = mean_child))+
  geom_point()

parents <- FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  rename(parent = down, child = up) %>% 
  group_by(parent) %>% 
  summarise(mean_parent = mean(prop)) %>% 
  rename(ID = parent)
  
childs <- FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  rename(parent = down, child = up) %>% 
  group_by(child) %>% 
  summarise(mean_child = mean(prop)) %>% 
  rename(ID = child)

left_join(parents, childs, by = "ID") %>% 
  ggplot(aes(x = mean_parent, y = mean_child))+
  geom_point()
```

```{r FB}
#set routes for all 3 watersheds
input <- rbind(bind23, bind24) %>%
  filter(wshed == "FB", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- FB_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)

#run calc_support for all sheds and timesteps for relative position
FB_all_combos <- calc_props(all_combos_routes, "FB")
```
```{r ZZ}
#set routes for all 3 watersheds

input <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
ZZ_all_combos <- calc_props(all_combos_routes, "ZZ")
```

## Find a path through the full graph using the TSP
For each watershed, find the best chain using solutions to traveling salesman problem.  Testing every method for solving the TSP manually.  Done before developing the generalized function below.   
```{r W3}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

# methods <- c("random",
#   "cheapest_insertion")

#set up edges and distance matrix, convert to TSP object
edges_W3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()

keyz <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))

# Convert to TSP object

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  atsp <- as.ATSP(edges_W3)

    atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], two_opt = TRUE)
    #tour <- solve_TSP(tour, method = "two_opt")

  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "W3") %>% 
    mutate("method" = methods[i])

    if(i == 1) all_chain_outputs <- chain_output
    if(i > 1) all_chain_outputs <- rbind(all_chain_outputs, chain_output)
}

all_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

```

```{r fooling around}
# 8/12/25, fooling around
keyz <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))


edges_W3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()

  #concorde solver?
#downloaded concorde but could not figure out how to get it working
concorde_path("/Users/johnmorgan/Documents/concorde/TSP")

data("USCA312")

## run concorde in verbose mode (-v) with fast cuts only (-V)
solve_TSP(USCA312, method = "concorde", control = list(clo = "-v -V"))

#now try applying
atsp <- as.ATSP(edges_W3)

    atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = "concorde", as_TSP = TRUE, precision = 6, two_opt = TRUE)
  #control = list(clo = "-B")
    #tour <- solve_TSP(atsp, method = "cheapest_insertion", as_TSP = TRUE, two_opt = TRUE)

    #tour <- solve_TSP(tour, method = "two_opt")

  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
con_sol <- c(edges[,1], edges[25,2])

concon <- tibble(ID = con_sol, sequence = seq(1, length(con_sol), 1))
concon <- tibble(node_ID = con_sol, sequence = seq(1, length(con_sol), 1)) %>% 
  left_join(key2, by = "node_ID") %>%
  rename(ID = sensor_ID) %>% 
  select(ID, sequence)
  

#con_model <- calc_model_result(concon$ID, "W3") %>% rename("con" = pred_out)
produce_metrics(concon, "W3", "concorde")


#acc_0.755 <- concon

#trying another thing
C_int <- round(1e6 * edges_W3)
diag(C_int) <- max(C_int) + 1L


reformed <- reformulate_ATSP_as_TSP(atsp)

write_TSPLIB(reformed, file = "myinstance.tsp")

sol <- as.integer(scan("~/Documents/concorde/myinstance.sol", quiet = TRUE)[-1])

read_delim("~/Documents/concorde/myinstance.sol", delim = " ")

as.TOUR(sol+1)

n_orig <- 26
map_to_atsp <- function(id) {
  if (id < n_orig) {
    return(id)
  } else if (id < 2 * n_orig) {
    return(id - n_orig)
  } else {
    return(NA)  # dummy
  }
}
tour_atsp <- vapply(sol, map_to_atsp, integer(1))
tour_atsp <- tour_atsp[!is.na(tour_atsp)]         # remove dummy
tour_atsp <- tour_atsp[!duplicated(tour_atsp)]    # keep first visit only


filter_ATSP_as_TSP_dummies(as.TOUR(sol+1))

```


Trying one more thing- what if the sequence was defined in order by average prop value when it is the parent?
```{r}
#new_seq <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  rename(parent = down,
         child = up) %>% 
  group_by(parent) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  arrange(desc(mean)) %>% View()
  mutate(sequence = seq(1, length(con_big), 1)) %>% 
  rename(ID = parent) %>% 
  mutate(ID = as.numeric(substr(ID, 3, 4))) %>% 
  select(ID, sequence)
  #filter(prop != 0) %>% 
  
produce_metrics(new_seq, "W3", "ordered")


w3_pk_renum <- W3_pk_seq %>% 
  filter(ID %in% (W3_IDs)) %>% 
  mutate(sequence = seq(1, 26, 1))
new_seq %>% 
  left_join(w3_pk_renum, by = "ID") %>% 
View()
```
New sequence is just as good as proportion of time flowing, but definitely a different sequence... definitely related to the number of transitions I think

Calculate new sequence for figure 5:
```{r}
new_seq <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  rename(parent = down,
         child = up) %>% 
  group_by(parent) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  arrange(desc(mean)) %>% View()
  mutate(sequence = seq(1, length(con_big), 1)) %>% 
  rename(ID = parent) %>% 
  mutate(ID = as.numeric(substr(ID, 3, 4))) %>% 
  select(ID, sequence)
  
opt_routes <- 
  new_seq %>% 
  mutate(down = lag(ID)) %>% 
    rename("up" = ID) %>% 
    select(up, down) %>% drop_na()
  
calc_props(opt_routes, "W3")
```


```{r FB-analysis}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

#methods <- c("random", "cheapest_insertion")
keyz <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edges_FB <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  atsp <- as.ATSP(edges_FB)
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i])#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "FB") %>% 
    mutate("method" = methods[i])

    if(i == 1) FB_chain_outputs <- chain_output
    if(i > 1) FB_chain_outputs <- rbind(FB_chain_outputs, chain_output)
}


#plots for committee meeting
FB_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

```
```{r ZZ-analysis}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

#methods <- c("random", "cheapest_insertion")
keyz <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edges_ZZ <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edges_ZZ)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(ZZ_IDs) + 1)#, start = 25)#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "ZZ") %>% 
    mutate("method" = methods[i])

    if(i == 1) ZZ_chain_outputs <- chain_output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, chain_output)
}


#plots for committee meeting
ZZ_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)


atsp <- as.ATSP(edges_ZZ)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[4], start = length(ZZ_IDs) + 1)#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  #path <- as.integer(tour)
  path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])



# Create a chain graph from the path
edges <- cbind(path[-length(path)], path[-1])
chain_graph <- graph_from_edgelist(edges, directed = TRUE)
plot(chain_graph,
       layout = layout_nicely(chain_graph),
     edge.arrow.size = 0.25,
  vertex.size = 10,
  vertex.label.cex = 0.75)

ggnet <- fortify(chain_graph, layout = igraph::layout_nicely(chain_graph))
ggplot(ggnet, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 8) +
  # geom_nodetext(aes(label =LETTERS[ path ]),
  #               fontface = "bold", color = "white", size = 3) +
  theme_blank()
```
```{r plot-chain}
edges2 <- all_graph_solutions %>% 
  filter(shed == "W3", method == "random", timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down) %>% as.matrix()
chain_graph <- graph_from_edgelist(edges2, directed = TRUE)
plot(chain_graph)

ggmst <- fortify(chain_graph, layout = igraph::layout_as_tree(chain_graph))
ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 3) +
  geom_nodetext(aes(label = name),
                fontface = "bold", color = "white", size = 1) +
  theme_blank()
```

Make a map of the activation sequence
```{r}

```

## Concorde solver
```{r W3}
keyz_w3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2_w3 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz_w3), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz_w3), 3, 4))), 1))

edges_W3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()



# 1. JV-transform + dummy
atsp_to_stsp <- function(cost_mat, dummy_cost = 0) {
  n <- nrow(cost_mat)
  M <- max(cost_mat) * n + 1  # big constant
  
  # JV transform: create 2n x 2n symmetric matrix
  sym_mat <- matrix(M, nrow = 2*n, ncol = 2*n)
  
  # link in->out copies (zero cost)
  for (i in 1:n) {
    sym_mat[i, n+i] <- 0
    sym_mat[n+i, i] <- 0
  }
  
  # original costs from out(i) to in(j)
  for (i in 1:n) {
    for (j in 1:n) {
      if (i != j) {
        sym_mat[n+i, j] <- cost_mat[i, j]
        sym_mat[j, n+i] <- cost_mat[i, j]
      }
    }
  }
  
  # Add dummy node
  sym_with_dummy <- matrix(M, nrow = 2*n + 1, ncol = 2*n + 1)
  sym_with_dummy[1:(2*n), 1:(2*n)] <- sym_mat
  
  dummy_idx <- 2*n + 1
  # Connect dummy to all "in" nodes with dummy_cost
  sym_with_dummy[dummy_idx, 1:n] <- dummy_cost
  sym_with_dummy[1:n, dummy_idx] <- dummy_cost
  
  # return symmetric matrix
  sym_with_dummy
}

out_matrix_w3 <- atsp_to_stsp(edges_W3)
# 2. Write TSPLIB file
write_tsplib <- function(sym_mat, file) {
  tsp_obj <- TSP(as.dist(sym_mat))
  write_TSPLIB(tsp_obj, file = file)
}

write_tsplib(out_matrix_w3, "~/Documents/concorde/W3.tsp")

#/Users/johnmorgan/Documents/concorde/TSP/concorde -o W3solution.tour /Users/johnmorgan/Documents/concorde/W3.tsp

# 3. Read Concorde .sol and convert to ATSP tour
read_concorde_sol <- function(sol_file, cost_mat) {
  n <- nrow(cost_mat)
  tour_nodes <- scan(sol_file, what = integer(), quiet = TRUE)
  
  # Remove first line if it's length
  if (length(tour_nodes) == n + 1 || length(tour_nodes) == 2*n + 1 || length(tour_nodes) == 2*n + 2) {
    # If first number equals length of tour, drop it
    if (tour_nodes[1] == length(tour_nodes) - 1) {
      tour_nodes <- tour_nodes[-1]
    }
  }
  
  # Remove dummy index if present
  dummy_idx <- 2*n + 1
  tour_nodes <- tour_nodes[tour_nodes != (dummy_idx - 1)]  # Concorde may index from 0
  
  # Map back to original ATSP nodes: out(i) -> i, in(i) -> i
  atsp_tour <- unique(((tour_nodes %% n) + 1))
  
  atsp_tour
}

con_big_w3 <- read_concorde_sol("~/Documents/concorde/TSP/W3solution.tour", edges_W3)

concon2_w3 <- tibble(node_ID = con_big_w3, sequence = seq(1, length(con_big_w3), 1)) %>% 
  left_join(key2_w3, by = "node_ID") %>%
  rename(ID = sensor_ID) %>% 
  select(ID, sequence)
  
#con_model <- calc_model_result(concon$ID, "W3") %>% rename("con" = pred_out)
produce_metrics(concon2, "W3", "concorde")

opt_routes_w3 <- 
  concon2 %>% 
  mutate(down = lag(ID)) %>% 
    rename("up" = ID) %>% 
    select(up, down) %>% drop_na()

# actually determining how often the nodes follow proportion of time flowing sequence
conc_props_w3 <- calc_props(opt_routes, "W3")

conc_props_w3 %>% 
  filter(timescale == "30mins") %>% 
  hist(prop)

hist(conc_props_w3$prop)
```
```{r FB}
keyz_fb <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2_fb <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz_fb), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz_fb), 3, 4))), 1))

edges_FB <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()


out_matrix_fb <- atsp_to_stsp(edges_FB)

write_tsplib(out_matrix_fb, "~/Documents/concorde/FB.tsp")
#/Users/johnmorgan/Documents/concorde/TSP/concorde -o FBsolution.tour /Users/johnmorgan/Documents/concorde/FB.tsp

con_big_fb <- read_concorde_sol("~/Documents/concorde/TSP/FBsolution.tour", edges_FB)

concon_fb <- tibble(node_ID = con_big_fb, sequence = seq(1, length(con_big_fb), 1)) %>% 
  left_join(key2_fb, by = "node_ID") %>%
  rename(ID = sensor_ID) %>% 
  select(ID, sequence)
  
#con_model <- calc_model_result(concon$ID, "W3") %>% rename("con" = pred_out)
produce_metrics(concon_fb, "FB", "concorde")

opt_routes_fb <- 
  tibble(node_ID = con_big_fb) %>% 
  left_join(key2_fb, by = "node_ID") %>% 
  mutate(down = lag(sensor_ID)) %>% 
    rename("up" = sensor_ID) %>% 
    select(up, down) %>% drop_na()

# actually determining how often the nodes follow proportion of time flowing sequence
hist(calc_props(opt_routes, "FB")$prop)
conc_props_fb <- calc_props(opt_routes, "FB")
```
```{r ZZ}
keyz_zz <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2_zz <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz_zz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz_zz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB

edges_ZZ <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()


out_matrix_zz <- atsp_to_stsp(edges_ZZ)

write_tsplib(out_matrix_zz, "~/Documents/concorde/ZZ.tsp")
#/Users/johnmorgan/Documents/concorde/TSP/concorde -o ZZsolution.tour /Users/johnmorgan/Documents/concorde/ZZ.tsp

con_big_zz <- read_concorde_sol("~/Documents/concorde/TSP/ZZsolution.tour", edges_ZZ)

concon_zz <- tibble(node_ID = con_big_zz, sequence = seq(1, length(con_big_zz), 1)) %>% 
  left_join(key2_zz, by = "node_ID") %>%
  rename(ID = sensor_ID) %>% 
  select(ID, sequence)
  
#con_model <- calc_model_result(concon$ID, "W3") %>% rename("con" = pred_out)
produce_metrics(concon_zz, "ZZ", "concorde")

opt_routes_zz <- 
  tibble(node_ID = con_big_zz) %>% 
  left_join(key2_zz, by = "node_ID") %>% 
  mutate(down = lag(sensor_ID)) %>% 
    rename("up" = sensor_ID) %>% 
    select(up, down) %>% drop_na()

# actually determining how often the nodes follow proportion of time flowing sequence
conc_props_zz <- calc_props(opt_routes, "ZZ")


all_conc <- rbind(conc_props_w3,
                  conc_props_fb,
                  conc_props_zz) %>% 
  mutate(method = "Concorde TSP")
```



## Defining genearlized function
```{r generalized-function-graph-solution}
#define all possible methods
#removed 3 methods
all_methods = c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")
#methods <- c("random", "cheapest_insertion")

general_graph <- function(combos, 
                          shed, 
                          methods = c("random", "cheapest_insertion"),
                          two_opt = TRUE){
#combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
allcombosIn <- calc_props(all_combos_routes, shed)

#methods <- c("random", "cheapest_insertion")
keyz <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edgesIn <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edgesIn)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], 
                    start = length(combos) + 1, two_opt = two_opt)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, shed) %>% 
    mutate("method" = methods[i])

    if(i == 1) ZZ_chain_outputs <- chain_output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, chain_output)
}
return(ZZ_chain_outputs)
}

#test general graph solution
ZZ_graph_solution <- general_graph(ZZ_IDs, "ZZ")

ZZ_graph_solution %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

#separate function to output the path, for non-determinstic methods may result in a different chain than other function
#can take multiple methods
# True/false whether output includes visualization of the chain
##works on a single method to develop a chain
chain_solution <- function(combos, 
                           shed, 
                           methods = "cheapest_insertion", 
                           plot = FALSE,
                           two_opt = TRUE){
#combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
allcombosIn <- calc_props(all_combos_routes, shed)

#methods <- c("random", "cheapest_insertion")
keyz <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edgesIn <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edgesIn)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(combos) + 1, two_opt = two_opt)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  #edges <- cbind(path[-length(path)], path[-1])
   output <- data.frame("node_ID" = path) %>% 
     left_join(key2, by = "node_ID") %>% 
     mutate(method = methods[i])
  
    if(i == 1) ZZ_chain_outputs <- output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, output)
}



chains_test <- ZZ_chain_outputs
edges <- cbind(chains_test$node_ID[-length(chains_test$node_ID)], chains_test$node_ID[-1])

chain_graph <- graph_from_edgelist(edges, directed = TRUE)

V(chain_graph)$labels <- chains_test$sensor_ID

ggmst <- fortify(chain_graph, layout = igraph::layout_as_tree(chain_graph))

# if(plot == TRUE){
# ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
#   geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
#                                                  type = "open")) +
#   geom_nodes(color = "black", size = 3) +
#   geom_nodetext(aes(label = labels),
#                 fontface = "bold", color = "white", size = 1) +
#   theme_blank()
# }

order <- ggmst %>% 
  arrange(desc(y))

#unique(order$labels)

#perfecting chains function to just output a dataframe, with one column for the sequence number, and the other for sensor ID
chains_output <- data.frame("ID" = unique(order$labels),
           "sequence" = seq(1, length(unique(order$labels)), 1))

return(chains_output)

}
chain_solution(ZZ_IDs, "ZZ", plot = TRUE)


#generate chain solution for a series of methods, or the same method many times?
#NOT FINISHED
chain_solution_multiple <- function(combos, 
                           shed, 
                           methods = "cheapest_insertion", 
                           plot = FALSE){
#combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
allcombosIn <- calc_props(all_combos_routes, shed)

#methods <- c("random", "cheapest_insertion")
keyz <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edgesIn <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edgesIn)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(combos) + 1, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  #edges <- cbind(path[-length(path)], path[-1])
   output <- data.frame("node_ID" = path) %>% 
     left_join(key2, by = "node_ID") %>% 
     mutate(method = methods[i])
  
    if(i == 1) ZZ_chain_outputs <- output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, output)
}



chains_test <- ZZ_chain_outputs
edges <- cbind(chains_test$node_ID[-length(chains_test$node_ID)], chains_test$node_ID[-1])

chain_graph <- graph_from_edgelist(edges, directed = TRUE)

V(chain_graph)$labels <- chains_test$sensor_ID

ggmst <- fortify(chain_graph, layout = igraph::layout_as_tree(chain_graph))

# if(plot == TRUE){
# ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
#   geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
#                                                  type = "open")) +
#   geom_nodes(color = "black", size = 3) +
#   geom_nodetext(aes(label = labels),
#                 fontface = "bold", color = "white", size = 1) +
#   theme_blank()
# }

order <- ggmst %>% 
  arrange(desc(y))

#unique(order$labels)

#perfecting chains function to just output a dataframe, with one column for the sequence number, and the other for sensor ID
chains_output <- data.frame("ID" = unique(order$labels),
           "sequence" = seq(1, length(unique(order$labels)), 1))

return(chains_output)

}
```
```{r applying-gen-func}
#creating chains

W3_cheap <- chain_solution(W3_IDs, "W3", methods = "cheapest_insertion")
W3_random <- chain_solution(W3_IDs, "W3", methods = "random")

FB_cheap <- chain_solution(FB_IDs, "FB", methods = "cheapest_insertion")
FB_random <- chain_solution(FB_IDs, "FB", methods = "random")

ZZ_cheap <- chain_solution(ZZ_IDs, "ZZ", methods = "cheapest_insertion")
ZZ_random <- chain_solution(ZZ_IDs, "ZZ", methods = "random", two_opt = FALSE)

```

## Comparing all solutions to the TSP for every watershed
```{r compare-graph-theory-solutions}
all_graph_solutions <- rbind(general_graph(W3_IDs, "W3"),
                             general_graph(FB_IDs, "FB"),
                             general_graph(ZZ_IDs, "ZZ"))

showcase <- rbind(general_graph(W3_IDs, "W3", all_methods),
                             general_graph(FB_IDs, "FB", all_methods),
                             general_graph(ZZ_IDs, "ZZ", all_methods))

showcase %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    #geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(shed~timescale)
```



# Defining proportion of time flowing sequences
My big graph theory solution provides the random hierarhcy and graph theory solution. All I need is the proportion of time flowing and maybe one based on topography... maybe drainage area or TWI?
```{r flow-permanence-W3}
#just summer 2023
data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

pks_23 <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>% 
  #left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()
#just summer 2024
pks_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number) %>% 
  #left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()

both <- inner_join(pks_23, pks_24, by = "ID")
ggplot()+
  geom_point(data = both,aes(x = pk.x, y = pk.y))+
  labs(title = "Change in pk from '23 to '24, W3",
       x = "2023",
       y = "2024")+
  geom_abline(slope=1, intercept=0)+
  theme_classic()

#both summers
#just rbind summers 23 and 24
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_w3 <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "W3")

#Define sequence by proportion of time flowing
W3_pk_seq <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_w3$ID), 1)) %>% 
  select(ID, sequence)
```
```{r flow-permanence-FB}
#both summers
#just rbind summers 23 and 24
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_fb <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "FB")

#Define proportion of time flowing sequence
#make a df with ID and the number in the sequence
FB_pk_seq <- pks_fb %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_fb$ID), 1)) %>% 
  select(ID, sequence)
```
```{r flow-permanence-ZZ}
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_zz <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "ZZ")

#define proportion of time flowing sequence for ZZ
ZZ_pk_seq <- pks_zz %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_zz$ID), 1)) %>% 
  select(ID, sequence)
```
```{r proportion-of-time-flowing-analysis}
routes_w3 <- pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- pks_fb %>%
    filter(ID %in% FB_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- pks_zz %>%
    filter(ID %in% ZZ_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

# actually determining how often the nodes follow proportion of time flowing sequence
all_pk <- rbind(calc_props(routes_w3, "W3"),
                calc_props(routes_fb, "FB"),
                calc_props(routes_zz, "ZZ")) %>% 
  mutate("method" = "Flow Permanence")
```

# Defining TWI sequences
```{r locs-from-arc}
#get snapped sensor locations from extracted values from arc
w3_locs <- read_csv("./STIC_uaa/w32.csv")%>% 
  select(ID, POINT_X, POINT_Y)
fb_locs <- read_csv("./STIC_uaa/fb2.csv") %>% 
  select(ID, POINT_X, POINT_Y)
zz_locs <- read_csv("./STIC_uaa/zz1.csv")%>% 
  select(ID, POINT_X, POINT_Y)
```
```{r calculate-curvature}
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)

# w3_shed <- "./w3_dems/w3_shed.tif"
# w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)
# #FB
# fb_shed <- "./fb_dems/fb_shed.tif"
# fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)
# #ZZ
# zz_shed <- "./zz_dems/zz_shed.tif"
# zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)

# fb_curve <- m1 %>% 
#   crop(fb_outline) %>% 
#   mask(fb_outline) %>% 
#   spatialEco::curvature()
# zz_curve <- m1 %>% 
#   crop(zz_outline) %>% 
#   mask(zz_outline)%>% 
#   spatialEco::curvature()
# w3_curve <- m1 %>% 
#   crop(w3_outline) %>% 
#   mask(w3_outline)%>% 
#   spatialEco::curvature()
#curve1 <- spatialEco::curvature(m1)
curve10 <- spatialEco::curvature(m10, type = "planform")

w3_curve <- extract(curve10, w3_locs[,2:3]) %>% 
  rename("curvature" = dem1m) %>% 
  pivot_longer(-ID)
fb_curve <- extract(curve10, fb_locs[,2:3]) %>% 
  rename("curvature" = dem1m) %>% 
  pivot_longer(-ID)
zz_curve <- extract(curve10, zz_locs[,2:3]) %>% 
  rename("curvature" = dem1m) %>% 
  pivot_longer(-ID)
```

```{r calculate-w3-topo}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(w3_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)


w3_uaa <- extract(rast(flowacc_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("uaa" = `X10mdem_flowacc`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

w3_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

w3_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

w3_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

w3_topo <- rbind(w3_uaa, w3_twi, w3_tpi, w3_slope, w3_curve)

```
```{r calculate-fb-topo}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(fb_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

fb_uaa <- extract(rast(flowacc_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("uaa" = `X10mdem_flowacc`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

fb_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

fb_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

fb_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

fb_topo <- rbind(fb_uaa, fb_tpi, fb_twi, fb_slope, fb_curve)
```
```{r calculate-zz-topo}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(zz_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

zz_uaa <- extract(rast(flowacc_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("uaa" = `X10mdem_flowacc`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

zz_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

zz_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

zz_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

zz_topo <- rbind(zz_uaa, zz_tpi, zz_twi, zz_slope, zz_curve)


```
```{r topographic-wetness-index}
routes_w3_twi <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID,
         "twi" = value) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb_twi <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID,
         "twi" = value) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz_twi <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID,
         "twi" = value) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_twi <- rbind(calc_props(routes_w3_twi, "W3"),
                 calc_props(routes_fb_twi, "FB"),
                 calc_props(routes_zz_twi, "ZZ")) %>% 
  mutate("method" = "Topographic Wetness Index")

```
```{r define-TWI-sequences}


#create a reference dataframe with the sensor ID, then the number in the sequence
w3_twi_sequence <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("twi" = value) %>% 
  arrange(desc(twi)) %>% 
  mutate(sequence = seq(1, length(w3_twi$ID), 1)) %>% 
  select(ID, sequence)

fb_twi_sequence <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>%
  rename("twi" = value) %>%
  arrange(desc(twi)) %>% 
  mutate(sequence = seq(1, length(FB_IDs), 1)) %>% 
  select(ID, sequence)

zz_twi_sequence <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("twi" = value) %>%
  arrange(desc(twi)) %>% 
  mutate(sequence = seq(1, length(ZZ_IDs), 1)) %>% 
  select(ID, sequence)

```

# Defining New Sequence- average prop value as parent node
Calculate new sequence for figure 5:
```{r combine}
#new_seq_w3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  group_by(down) %>% 
  summarise(mean = mean(prop)) %>% 
  arrange(desc(mean)) %>% 
  mutate(up = as.numeric(substr(down, 3, 4)),
         down = lag(up)) %>% 
  select(down, up) %>% 
  drop_na() 

new_seq_fb <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  group_by(down) %>% 
  summarise(mean = mean(prop)) %>% 
  arrange(desc(mean)) %>% 
  mutate(up = as.numeric(substr(down, 3, 4)),
         down = lag(up)) %>% 
  select(down, up) %>% 
  drop_na()

new_seq_zz <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  group_by(down) %>% 
  summarise(mean = mean(prop)) %>% 
  arrange(desc(mean)) %>% 
  mutate(up = as.numeric(substr(down, 3, 4)),
         down = lag(up)) %>% 
  select(down, up) %>% 
  drop_na()
  
# actually determining how often the nodes follow proportion of time flowing sequence
all_new <- rbind(calc_props(new_seq_w3, "W3"),
                calc_props(new_seq_fb, "FB"),
                calc_props(new_seq_zz, "ZZ")) %>% 
  mutate("method" = "Average Sequential Wetting")
```

Make them ID/sequence format
```{r}
#write wrapper functions to convert between the two forms easily
#routes_w3 <- 
  pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

#convert from up/down to ID/seq
convert_to_IDseq <- function(input){

  convert <-
    input %>%
    mutate(sequence = seq(1, length(input$up), 1)) %>%
    rename(ID = down) %>%
    select(ID, sequence)
return(convert)
}
  

#convert from ID/seq to up/down
input <- W3_cheap

  input %>% 
    mutate(down = "")
  mutate(up = as.numeric(substr(down, 3, 4)),
         down = lag(up)) %>% 
  select(down, up) %>% 
  drop_na()
```

# Defining random sequences

```{r}
W3_random <- 
tibble(down = sample(W3_IDs)) %>% 
  mutate(up = lag(down)) %>% 
  select(up, down) %>% 
  drop_na()

FB_random <- 
tibble(down = sample(FB_IDs)) %>% 
  mutate(up = lag(down)) %>% 
  select(up, down) %>% 
  drop_na()

ZZ_random <- 
tibble(down = sample(ZZ_IDs)) %>% 
  mutate(up = lag(down)) %>% 
  select(up, down) %>% 
  drop_na()


#create randomly generated sequences
all_random <- rbind(calc_props(W3_random, "W3"),
                    calc_props(FB_random, "FB"),
                    calc_props(ZZ_random, "ZZ")) %>% 
  mutate(method = "Random")
```


# Testing Sequences, running model
This section contains all chunks used to test the effectiveness of a given sequence.  
Model is using same Method as Botter et al. 2021, where the number of active nodes at a given timestep is known. Nodes are activated in order of the sequence until the number activated is reached. The presence or absence of flow based on the model is then compared to the actual values for each node at each timestep.  

```{r figuring-out-how}
#new method, incorporating missing nodes at different timesteps
input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  arrange((sequence)) %>% 
  # mutate("downstream" = lag(binary),
  #        "following" = downstream - binary) %>% 
  #filter(following != -1) %>% 
  filter(datetime == "2023-07-15 19:00:00") %>% View()
   # summarise(count_non = length(following)) %>% View()


#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy
number_activated %>% mutate(all_nodes = W3_pk_seq$ID)



number_activated <- 
  input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  mutate(deployed = length(binary)) %>% 
  #arrange(sequence) %>% 
      #filter(binary == 1) %>% 
    summarise(number_flowing = length(binary[binary == 1]),
            deployed = list(unique(ID))) #%>% View()

W3_pk_seq$ID[W3_pk_seq$ID %in% unlist(number_activated$deployed[1])]

# 2. Dataframe with timestamps and how many nodes to activate
number_activated <- number_activated %>% 
  select(-deployed)
# Fixed full node sequence
full_node_sequence <- W3_pk_seq$ID

# Sample activation schedule
activation_schedule <- number_activated

# Function to generate a single activation row with NA for missing nodes
    '%!in%' <- function(x,y)!('%in%'(x,y))

generate_activation_row <- function(datetime, n_active, available_nodes) {
  #n_active <- activation_schedule$number_flowing[1]
  #available_nodes <- unlist(activation_schedule$deployed[1])
  # Order available nodes based on full sequence
  ordered_available <- full_node_sequence[full_node_sequence %in% available_nodes]
  
    #not_deployed <- full_node_sequence[full_node_sequence %!in% (available_nodes)]


  
  # Identify the first `n_active` nodes to activate
  activated_nodes <- head(ordered_available, n_active)
  # ordered_available
  # activated_nodes
  # full_node_sequence
  output <- tibble("datetime" = datetime,
             "ID" = full_node_sequence,
             "binary" = 0) %>% 
    #mutate(binary = case_when(ID %in% ordered_available ~ 0)) %>% 
    mutate(binary = case_when(
                              ID %in% activated_nodes ~ 1,
                              ID %!in% ordered_available ~ NA,
                              TRUE ~ 0))
  return(output)
}

generate_activation_row(activation_schedule$datetime[1],
                        activation_schedule$number_flowing[1],
                        unlist(activation_schedule$deployed[1]))
# Apply the function to each row of the activation schedule

model_out <- Map(generate_activation_row, 
    datetime = activation_schedule$datetime, 
    n_active = activation_schedule$number_flowing, 
    available_nodes = activation_schedule$deployed) %>% dplyr::bind_rows() %>% 
  rename(pk_out = binary)

model_out
comparison <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(model_out, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins)
```
Make sure that the new way to model is working for sequences other than pk
```{r testing-cases}
#new method, incorporating missing nodes at different timesteps
number_activated <- 
  input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  mutate(deployed = length(binary)) %>% 
  #arrange(sequence) %>% 
      #filter(binary == 1) %>% 
    summarise(number_flowing = length(binary[binary == 1]),
            deployed = list(unique(ID))) #%>% View()

W3_pk_seq$ID[W3_pk_seq$ID %in% unlist(number_activated$deployed[1])]

# 2. Dataframe with timestamps and how many nodes to activate
number_activated <- number_activated %>% 
  select(-deployed)
# Fixed full node sequence
full_node_sequence <- W3_cheap$ID

# Sample activation schedule
activation_schedule <- number_activated

# Function to generate a single activation row with NA for missing nodes
    '%!in%' <- function(x,y)!('%in%'(x,y))

#generate_activation_row <- function(datetime, n_active, available_nodes) {
  n_active <- activation_schedule$number_flowing[1]
  available_nodes <- unlist(activation_schedule$deployed[1])
  datetime <- activation_schedule$datetime[1]
  # Order available nodes based on full sequence
  ordered_available <- full_node_sequence[full_node_sequence %in% available_nodes]
  
    #not_deployed <- full_node_sequence[full_node_sequence %!in% (available_nodes)]


  
  # Identify the first `n_active` nodes to activate
  activated_nodes <- head(ordered_available, n_active)
  ordered_available
  activated_nodes
  full_node_sequence
  output <- tibble("datetime" = datetime,
             "ID" = full_node_sequence,
             "binary" = 0) %>% 
    #mutate(binary = case_when(ID %in% ordered_available ~ 0)) %>% 
    mutate(binary = case_when(
                              ID %in% activated_nodes ~ 1,
                              ID %!in% ordered_available ~ NA,
                              TRUE ~ 0))
  return(output)
#}

generate_activation_row(activation_schedule$datetime[1],
                        activation_schedule$number_flowing[1],
                        unlist(activation_schedule$deployed[1]))
# Apply the function to each row of the activation schedule

model_out <- Map(generate_activation_row, 
    datetime = activation_schedule$datetime, 
    n_active = activation_schedule$number_flowing, 
    available_nodes = activation_schedule$deployed) %>% dplyr::bind_rows() #%>% 
  rename(pk_out = binary)

model_out
comparison <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(model_out, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins)
```

Now make a function to calc the model result using this method
```{r define calc-model-result function}
#new method, incorporating missing nodes at different timesteps
# only implemented for W3 rn
calc_model_result <- function(input_sequence, shed){

  input <- if(shed == "W3") input_w3
  else if (shed == "FB") input_fb
  else if (shed == "ZZ") input_zz
  
number_activated <- 
  input %>% 
  #left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  mutate(deployed = length(binary)) %>% 
  #arrange(sequence) %>% 
      #filter(binary == 1) %>% 
    summarise(number_flowing = length(binary[binary == 1]),
            deployed = list(unique(ID))) #%>% View()

#W3_pk_seq$ID[W3_pk_seq$ID %in% unlist(number_activated$deployed[1])]

# 2. Dataframe with timestamps and how many nodes to activate

# Fixed full node sequence
full_node_sequence <- input_sequence

# Sample activation schedule
activation_schedule <- number_activated

# Function to generate a single activation row with NA for missing nodes
    '%!in%' <- function(x,y)!('%in%'(x,y))

generate_activation_row <- function(datetime, n_active, available_nodes) {
  #n_active <- activation_schedule$number_flowing[1]
  #available_nodes <- unlist(activation_schedule$deployed[1])
  # Order available nodes based on full sequence
  ordered_available <- full_node_sequence[full_node_sequence %in% available_nodes]
  
    #not_deployed <- full_node_sequence[full_node_sequence %!in% (available_nodes)]


  
  # Identify the first `n_active` nodes to activate
  activated_nodes <- head(ordered_available, n_active)
  # ordered_available
  # activated_nodes
  # full_node_sequence
  output <- tibble("datetime" = datetime,
             "ID" = full_node_sequence,
             "binary" = 0) %>% 
    #mutate(binary = case_when(ID %in% ordered_available ~ 0)) %>% 
    mutate(binary = case_when(
                              ID %in% activated_nodes ~ 1,
                              ID %!in% ordered_available ~ NA,
                              TRUE ~ 0))
  return(output)
}

# Apply the function to each row of the activation schedule

model_out <- Map(generate_activation_row, 
    datetime = activation_schedule$datetime, 
    n_active = activation_schedule$number_flowing, 
    available_nodes = activation_schedule$deployed) %>% dplyr::bind_rows() %>% 
  rename(pred_out = binary)

return(model_out)
}

#Testing function on W3
calc_model_result(W3_pk_seq$ID, "W3")

# Helper function for determining type of error
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}

get_eval_label(0,0)

# comparison <- input_w3 %>% 
#   filter(mins %in% c(0, 30)) %>% 
#   left_join(model_out, by = c("datetime", "ID")) %>% 
#   select(-wshed, -mins)
# 
# 
# input_w3 %>% 
#   filter(mins %in% c(0, 30)) %>% 
#   left_join(cheap_model, by = c("datetime", "ID")) %>% 
#   select(-wshed, -mins) %>%
#   filter(datetime == ymd_hms("2023-08-08 04:00:00 UTC")) %>%  View()
```
Run on W3
```{r W3}


#running model
cheap_model_w3 <- calc_model_result(W3_cheap$ID, "W3") %>% rename("cheapest" = pred_out)
pk_model_w3 <- calc_model_result(W3_pk_seq$ID, "W3") %>% rename("pk" = pred_out)
random_model_w3 <- calc_model_result(W3_random$ID, "W3") %>% rename("random" = pred_out)
twi_model_w3 <- calc_model_result(w3_twi_sequence$ID, "W3") %>% rename("twi" = pred_out)
new_model_w3 <- calc_model_result(new_seq_w3$down, "W3") %>% rename("new" = pred_out)
con_model_w3 <- calc_model_result(opt_routes_w3$down, "W3") %>% rename("concorde" = pred_out)


comparison_w3 <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model_w3, by = c("datetime", "ID")) %>% 
  left_join(pk_model_w3, by = c("datetime", "ID")) %>% 
    left_join(random_model_w3, by = c("datetime", "ID")) %>% 
  left_join(twi_model_w3, by = c("datetime", "ID")) %>% 
  left_join(new_model_w3, by = c("datetime", "ID")) %>% 
    left_join(con_model_w3, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins) %>% 
  drop_na()

#apply get_eval function to each model run
comparison_w3$cheapest_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$cheapest)
comparison_w3$pk_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$pk)
comparison_w3$random_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$random)
comparison_w3$twi_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$twi)
comparison_w3$new_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$new)
comparison_w3$con_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$concorde)



# comparison$comparison_cheap <- with(comparison, ifelse(
#   cheapest_eval == pk_eval, "equal",
#   ifelse(cheapest_eval == "correct", "cheap_better",
#          ifelse(pk_eval == "correct", "pk_better", "neither_better"))
# ))
# comparison$comparison_random <- with(comparison, ifelse(
#   random_eval == pk_eval, "equal",
#   ifelse(random_eval == "correct", "cheap_better",
#          ifelse(pk_eval == "correct", "pk_better", "neither_better"))
# ))
# comparison$comparison_twi <- with(comparison, ifelse(
#   twi == pk_eval, "equal",
#   ifelse(twi == "correct", "cheap_better",
#          ifelse(pk_eval == "correct", "pk_better", "neither_better"))
# ))

#summarize through time
df_summary_time_w3 <- comparison_w3 %>%
  group_by(datetime) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
      new_correct = sum(new_eval == "correct"),
    con_correct = sum(con_eval == "correct"),


    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct / total,
    con_accuracy = con_correct / total


  )

df_long_w3 <- df_summary_time_w3 %>%
  select(datetime, contains("accuracy")) %>% 
  tidyr::pivot_longer(cols = c(cheap_accuracy, pk_accuracy, 
                               random_accuracy, twi_accuracy, new_accuracy, con_accuracy), 
                      names_to = "model", values_to = "accuracy")

#make data long for final plot
comparison_long_w3 <- 
  comparison_w3 %>% 
  select(datetime, ends_with("eval")) %>% 
    pivot_longer(-datetime) %>% 
  mutate(year = year(datetime))
#final plot

#plot of correct, ommission, and commission through time
comparison_long_w3 %>% 
  filter(#name != "random_eval",
         name != "cheapest_eval",
         name != "con_eval") %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  mutate(total = as.numeric(length(value))) %>% 
  ggplot() +
  geom_histogram(aes(x = day_of_year, fill = value), binwidth = 1, na.rm = TRUE)+
  facet_grid(name~year)




```
```{r FB}
#chains used in graph theory solutions
FB_cheap <- chain_solution(FB_IDs, "FB", methods = "cheapest_insertion")
#FB_random <- chain_solution(FB_IDs, "FB", methods = "random")
FB_random <- tibble(ID = sample(FB_IDs),
                    sequence = seq(1, length(FB_IDs), 1))
                    

#applying evaluation model using different sequences
cheap_model_fb <- calc_model_result(FB_cheap$ID, "FB") %>% rename("cheapest" = pred_out)
pk_model_fb <- calc_model_result(FB_pk_seq$ID, "FB") %>% rename("pk" = pred_out)
random_model_fb <- calc_model_result(FB_random$ID, "FB") %>% rename("random" = pred_out)
twi_model_fb <- calc_model_result(fb_twi_sequence$ID, "FB") %>% rename("twi" = pred_out)
new_model_fb <- calc_model_result(new_seq_fb$down, "FB") %>% rename("new" = pred_out)

comparison_fb <- input_fb %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model_fb, by = c("datetime", "ID")) %>% 
  left_join(pk_model_fb, by = c("datetime", "ID")) %>% 
    left_join(random_model_fb, by = c("datetime", "ID")) %>% 
  left_join(twi_model_fb, by = c("datetime", "ID")) %>% 
  left_join(new_model_fb, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins) %>% 
  drop_na()

#apply get_eval function to each model run
comparison_fb$cheapest_eval <- mapply(get_eval_label, comparison_fb$binary, comparison_fb$cheapest)
comparison_fb$pk_eval <- mapply(get_eval_label, comparison_fb$binary, comparison_fb$pk)
comparison_fb$random_eval <- mapply(get_eval_label, comparison_fb$binary, comparison_fb$random)
comparison_fb$twi_eval <- mapply(get_eval_label, comparison_fb$binary, comparison_fb$twi)
comparison_fb$new_eval <- mapply(get_eval_label, comparison_fb$binary, comparison_fb$new)



#summarize through time
df_summary_time_fb <- comparison_fb %>%
  group_by(datetime) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct / total
  )


df_long_fb <- df_summary_time_fb %>%
  select(datetime, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy, new_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy, new_accuracy), names_to = "model", values_to = "accuracy")

ggplot(df_long_fb, aes(x = datetime, y = accuracy, color = model)) +
  geom_line() +
  labs(title = "Model Accuracy Over Time", y = "Accuracy", x = "Time")

#original way of comparing- the grid
comparison_long_fb <- 
  comparison_fb %>% 
  select(datetime, ends_with("eval")) %>% 
    pivot_longer(-datetime) %>% 
  mutate(year = year(datetime))

# filter(datetime >= ymd_hms("2023-07-24 00:00:00") & datetime <= ymd_hms("2023-08-20 00:00:00")) %>% 
comparison_long_fb %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  #group_by(datetime) %>% 
  #filter(year == 2023) %>% 
  mutate(total = as.numeric(length(value))) %>% 
  ggplot() +
  geom_histogram(aes(x = day_of_year, fill = value), binwidth = 1, na.rm = TRUE)+
  facet_grid(name~year)
```
```{r ZZ}
ZZ_cheap <- chain_solution(ZZ_IDs, "ZZ", methods = "cheapest_insertion", two_opt = FALSE)
#ZZ_random <- chain_solution(ZZ_IDs, "ZZ", methods = "random", two_opt = FALSE)
ZZ_random <- tibble(ID = sample(ZZ_IDs),
                    sequence = seq(1, length(ZZ_IDs), 1))

cheap_model_zz <- calc_model_result(ZZ_cheap$ID, "ZZ") %>% rename("cheapest" = pred_out)
pk_model_zz <- calc_model_result(ZZ_pk_seq$ID, "ZZ") %>% rename("pk" = pred_out)
random_model_zz <- calc_model_result(ZZ_random$ID, "ZZ") %>% rename("random" = pred_out)
twi_model_zz <- calc_model_result(zz_twi_sequence$ID, "ZZ") %>% rename("twi" = pred_out)
new_model_zz <- calc_model_result(new_seq_zz$down, "ZZ") %>% rename("new" = pred_out)


comparison_zz <- input_zz %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model_zz, by = c("datetime", "ID")) %>% 
  left_join(pk_model_zz, by = c("datetime", "ID")) %>% 
    left_join(random_model_zz, by = c("datetime", "ID")) %>% 
  left_join(twi_model_zz, by = c("datetime", "ID")) %>% 
  left_join(new_model_zz, by = c("datetime", "ID")) %>%
  select(-wshed, -mins) %>% 
  drop_na()

#apply get_eval function to each model run
comparison_zz$cheapest_eval <- mapply(get_eval_label, comparison_zz$binary, comparison_zz$cheapest)
comparison_zz$pk_eval <- mapply(get_eval_label, comparison_zz$binary, comparison_zz$pk)
comparison_zz$random_eval <- mapply(get_eval_label, comparison_zz$binary, comparison_zz$random)
comparison_zz$twi_eval <- mapply(get_eval_label, comparison_zz$binary, comparison_zz$twi)
comparison_zz$new_eval <- mapply(get_eval_label, comparison_zz$binary, comparison_zz$new)



#summarize through time
df_summary_time_zz <- comparison_zz %>%
  group_by(datetime) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct / total
  )


#make data long for final plot
comparison_long_zz <- 
  comparison_zz %>% 
  select(datetime, ends_with("eval")) %>% 
    pivot_longer(-datetime) %>% 
  mutate(year = year(datetime))
#final plot
comparison_long_zz %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  #group_by(datetime) %>% 
  #filter(year == 2023) %>% 
  mutate(total = as.numeric(length(value))) %>% 
  ggplot() +
  geom_histogram(aes(x = day_of_year, fill = value), binwidth = 1, na.rm = TRUE)+
  facet_grid(name~year)


```



# Paper Figures
## Figure 1: Maps of watersheds
```{r W3-map}
#map of watershed 3 with depth to bedrock
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
hill <- rast(hillshade_out)

#dem
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
#crop to rectangular area
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#writeRaster(crop1, "1mdemw3_cropped.tif")

#watershed boundary
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)
plot(w3_outline)
expanse(w3_outline)

#w3 network- thing I need to change
#read in shapefile of stream converted in ARC
vect_stream_path <- "./AGU24posterAnalysis/vector_stream/vector_stream.shp"
#stream as a vector
vect_stream <- vect(vect_stream_path)
plot(vect_stream)
#crop to watershed boundary
w3_stream_crop <- crop(vect_stream, w3_outline)
plot(w3_stream_crop)
#or i could use old classification

#point locations- snapped points from above chunk
w3_stic_locs_snap <- "w3_stic_locs_snap.shp"

w3_stic_locs_r <- vect(w3_stic_locs_snap) %>% 
  left_join(pks_w3, by = "ID")



w3_stic_locs_r <- vect(w3_stic_locs_snap)
#writeVector(w3_stic_locs_r, "./seismic_map_exports/w3_stic_locs_snap.shp")


w3_net <- vect("./carrieZigZag/w3_network.shp")
#writeVector(w3_net, "./seismic_map_exports/network.shp")

plot(w3_net)
sum(perim(w3_net))

ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))

w3_map_f1 <- 
  ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "darkslategray3", lwd = 2) +
  geom_sf(data = w3_stic_locs_r, colour = "midnightblue", size = 2) +
  #geom_sf(data = dd, aes(color = (depth)), pch = 19, size = 3) +
  scale_color_gradient(low = "black", high = "white")+
  #geom_sf(data = w3_pour, colour = "black") +
   scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr',
                              pad_x = unit(1, "cm"),
                              pad_y = unit(0.1, "cm"))
                              
w3_map_f1
#ggsave(w3_map_f1)

```
```{r FB-map}
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

#define the rectangular area that will be shown on final map
ybounds <- c(4868850,4869650)
xbounds <- c(279350, 280450)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
points(lcc)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./fb_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
fb_crop <- "./fb_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
fb_net <- vect("./carrieZigZag/FB_network.shp")

###pour point to define where the watershed boundary is
#manually type coords from windows computer
fb_pour_coords <- data.frame("easting" = 280400,
                             "northing" = 4869120)
#convert to SpatVector object
fb_pour <- vect(fb_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
fb_pour_filename <- "./fb_dems/fb_pour.shp"
#writeVector(fb_pour, fb_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
b_crop <- "./fb_dems/1mdem_crop.tif"

fb_breached <- "./fb_dems/1mdem_breach.tif"
# wbt_breach_depressions_least_cost(
#   dem = fb_crop,
#   output = fb_breached,
#   dist = 1,
#   fill = TRUE)

fb_filled <- "./fb_dems/1mdem_fill.tif"
# wbt_fill_depressions_wang_and_liu(
#   dem = fb_breached,
#   output = fb_filled
# )
#calculate flow accumulation and direction
fb_flowacc <- "./fb_dems/1mdem_fb_flowacc.tif"
# wbt_d8_flow_accumulation(input = fb_filled,
#                          output = fb_flowacc)
# plot(rast(fb_flowacc))
fb_d8pt <- "./fb_dems/1mdem_fb_d8pt.tif"
# wbt_d8_pointer(dem = fb_filled,
#                output = fb_d8pt)
# plot(rast(fb_d8pt))


#delineate streams
fb_streams <- "./fb_dems/fb_streams.tif"
# wbt_extract_streams(flow_accum = fb_flowacc,
#                     output = fb_streams,
#                     threshold = 8000)
# plot(rast(fb_streams))
# points(lcc)
#snap pour point to streams
fb_pour_snap <- "./fb_dems/fb_pour_snap.shp"
# wbt_jenson_snap_pour_points(pour_pts = fb_pour_filename,
#                             streams = fb_streams,
#                             output = fb_pour_snap,
#                             snap_dist = 10)
fb_pour_snap_read <- vect("./fb_dems/fb_pour_snap.shp")

fb_shed <- "./fb_dems/fb_shed.tif"
# wbt_watershed(d8_pntr = fb_d8pt,
#               pour_pts = fb_pour_snap,
#               output = fb_shed)

#convert raster of watershed area to vector for final mapping
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)

#get sensor locations from STIC data, format
locs_fb <- data_23 %>% 
  filter(wshed == "FB") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape_fb <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc_fb <- terra::project(locs_shape_fb, crs(m1))

#assign destination for hillshade calculation
hillshade_out_fb <- "./fb_dems/1mdem_hillshade.tif"
# wbt_hillshade(
#   dem = fb_crop,
#   output = hillshade_out,
# )
hill_fb <- rast(hillshade_out_fb)

#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.
#fb_map <- 
  ggplot()+
  geom_spatraster(data = hill_fb)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = fb_outline, fill = NA, color = "#397367", alpha = 0.3, lwd = 2) +
  geom_sf(data = fb_net, colour = "darkslategray3", lwd = 2) +
    geom_sf(data = lcc_fb, colour = "midnightblue", pch = 19, size = 2) +
  #geom_sf(data = fb_pour, colour = "black", pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen", limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))
  

```
```{r ZZ-map}
#map for ZZ
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
#plot(m1)

#get sensor locations from STIC data, format
locs <- data_23 %>% 
  filter(wshed == "ZZ") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1))
#plot(lcc)
#define the rectangular area that will be shown on final map
ybounds <- c(4866400,4867500)
xbounds <- c(277200, 277650)
#plot(m1, xlim = xbounds, ylim = ybounds)
#points(lcc)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#plot(crop1)
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./zz_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
zz_crop <- "./zz_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
zz_net <- vect("./carrieZigZag/zigzag_streams.shp")
#plot(zz_net)

###pour point to define where the watershed boundary is
#manually type coords from windows computer


zz_pour_coords <- data.frame("easting" = 277280.45,
                             "northing" = 4867436.45)
#convert to SpatVector object
zz_pour <- vect(zz_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
zz_pour_filename <- "./zz_dems/zz_pour.shp"
#writeVector(zz_pour, zz_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
zz_crop <- "./zz_dems/1mdem_crop.tif"

zz_breached <- "./zz_dems/1mdem_breach.tif"


zz_filled <- "./zz_dems/1mdem_fill.tif"
# wbt_fill_depressions_wang_and_liu(
#   dem = zz_breached,
#   output = zz_filled
# )
#calculate flow accumulation and direction
zz_flowacc <- "./zz_dems/1mdem_zz_flowacc.tif"
# wbt_d8_flow_accumulation(input = zz_filled,
#                          output = zz_flowacc)
#plot(rast(zz_flowacc))
zz_d8pt <- "./zz_dems/1mdem_zz_d8pt.tif"
# wbt_d8_pointer(dem = zz_filled,
#                output = zz_d8pt)
#plot(rast(zz_d8pt))


#delineate streams
zz_streams <- "./zz_dems/zz_streams.tif"
# wbt_extract_streams(flow_accum = zz_flowacc,
#                     output = zz_streams,
#                     threshold = 8000)
# plot(rast(zz_streams))
# points(lcc)
#snap pour point to streams
zz_pour_snap <- "./zz_dems/zz_pour_snap.shp"
# wbt_jenson_snap_pour_points(pour_pts = zz_pour_filename,
#                             streams = zz_streams,
#                             output = zz_pour_snap,
#                             snap_dist = 10)
zz_pour_snap_read <- vect("./zz_dems/zz_pour_snap.shp")
# plot(rast(zz_streams), 
#      xlim = c(280200, 280410),
#      ylim = c(4869300, 4869000))
# points(zz_pour_snap_read, pch = 1)

zz_shed <- "./zz_dems/zz_shed.tif"
# wbt_watershed(d8_pntr = zz_d8pt,
#               pour_pts = zz_pour_snap,
#               output = zz_shed)

#plot(rast(zz_shed))
#convert raster of watershed area to vector for final mapping
zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)
#plot(zz_outline)



#assign destination for hillshade calculation
hillshade_out <- "./zz_dems/1mdem_hillshade.tif"
# wbt_hillshade(
#   dem = zz_crop,
#   output = hillshade_out,
# )
hill <- rast(hillshade_out)
#plot(hill)
#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.
#zz_map <- 
  ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2) +
  geom_sf(data = zz_net, colour = "darkslategray3", lwd = 2) +
    geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 2) +
  #geom_sf(data = zz_pour, colour = "black", pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen", limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))


```

## Figure 2: Vignettes of network wetting and drying
Chunk below copied from mapsForStoryboard.qmd, from section where I was determining times when the network went from fully dry to fully wet.
```{r W3-flowing-states}
 # Function factory for secondary axis transforms
train_sec <- function(primary, secondary, na.rm = TRUE) {
  # Thanks Henry Holm for including the na.rm argument!
  from <- range(secondary, na.rm = na.rm)
  to   <- range(primary, na.rm = na.rm)
  # Forward transform for the data
  forward <- function(x) {
    rescale(x, from = from, to = to)
  }
  # Reverse transform for the secondary axis
  reverse <- function(x) {
    rescale(x, from = to, to = from)
  }
  list(fwd = forward, rev = reverse)
}

start <- ymd_hms("2023-10-20 00:00:00")
# stop <- ymd_hms("2023-10-22 00:00:00")
stop <- ymd_hms("2023-10-24 00:00:00")

#add precipitation
#add hydrograph to plot
q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

    



#precip <- 
  read_csv("./HB/dailyWatershedPrecip1956-2024.csv") %>% 
    filter(watershed == "W3") %>% 
        filter(DATE >= start & DATE <= stop) %>% 
    ggplot()+
    geom_bar(aes(x = DATE, y = Precip), stat = "identity")

precip1 <- read_csv("./HB/HBEF_W3precipitation_15min.csv") %>% 
    filter(DateTime >= start & DateTime <= stop) %>% 
    mutate(day = day(DateTime),
           hour = hour(DateTime)) %>% 
    group_by(day, hour) %>% 
    reframe(hourly_precip = sum(precip), across()) %>% 
    mutate(mins = minute(DateTime)) %>% 
    filter(mins == 0) %>%
  rename("datetime" = DateTime) %>% 
    left_join(q_23_plotting, by = "datetime") %>% 
  select(datetime, day, hour, hourly_precip, Q_mm_day)


sec3 <- with(precip1, train_sec(hourly_precip, Q_mm_day))


water <- precip1 %>% 
    ggplot(aes(x = datetime))+
    geom_col(aes(y = hourly_precip))+
    geom_line(aes(y = sec3$fwd(Q_mm_day)), colour = "blue") +
    scale_y_continuous("Hourly Precipitation (mm)",
                       sec.axis = sec_axis(~sec3$rev(.),
                       name = "Discharge (mm/day)"))+
    labs(x = "",
         y = "Precipitation (mm)")+
      theme_classic()+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y = element_text(angle = 0),
        axis.title.y.right = element_text(angle = 0)
        )


test_IDS <- input_w3 %>% 
    filter(datetime > start & datetime < stop)

remainder <- test_IDS$ID

pks_ordered <- pks_w3 %>% 
filter(ID %in% remainder) %>% 
  arrange(desc(pk)) %>% 
  rowid_to_column("pk_order")

test <- input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
sec2 <- with(test, train_sec(pk_order, pk))

test <- 
  input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
    mutate(day = day(datetime),
           hour = hour(datetime)) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(precip1, by = c("day", "hour")) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
  #filter(mins.x == 0)
test2 <- test %>% 
  select(datetime.x, binary, hourly_precip, pk_order)
sec2 <- with(test2, train_sec(pk_order, hourly_precip))

binary <- c("#DB995A",
            "#586BA4"
  )


tiles <- 
test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = pk_order, fill = as.character(binary)))+
  scale_y_continuous("Supposed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                    values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23",
                              "10/23/23",
                              "10/24/23"))+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 0))

(water)/tiles + plot_layout(heights = c(1, 5))


test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = (pk_order), fill = as.character(binary)))+
  geom_col(aes(y = sec2$fwd(hourly_precip)))+
  ylab("Supposed activation order")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$fwd(.), name = "Instantaneous Q (mm/day)")) +
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(title = "Very dry to very wet",
       subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23",
                              "10/23/23",
                              "10/24/23"))+
  theme_classic()+
    theme(legend.position="right")


#different order, not based on Pk but the actual sequence that they activate in
#determine what the activation order is
pks_ordered <- 
input_w3 %>% 
    filter(ID %in% remainder) %>% 
    filter(datetime > start & datetime < stop,
           binary == 1) %>% 
    group_by(ID) %>% 
    summarise(start_of_flow = min(datetime)) %>% 
    arrange(start_of_flow) %>% 
  rowid_to_column("pk_order") %>% 
  select(ID, pk_order) %>% unique()

test <- input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
sec2 <- with(test, train_sec(pk_order, pk))

test <- 
  input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
    mutate(day = day(datetime),
           hour = hour(datetime)) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(precip1, by = c("day", "hour")) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
  #filter(mins.x == 0)
test2 <- test %>% 
  select(datetime.x, binary, hourly_precip, pk_order)
sec2 <- with(test2, train_sec(pk_order, hourly_precip))

binary <- c("#DB995A",
            "#586BA4"
  )


tiles <- 
test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = pk_order, fill = as.character(binary)))+
  scale_y_continuous("Supposed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                    values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23",
                              "10/23/23",
                              "10/24/23"))+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 0))

(water)/tiles + plot_layout(heights = c(1, 5))
```
```{r}
pks_ordered <- rename(pks_ordered, sequence = pk_order)
calc_model_result(pks_ordered$ID, "W3")
produce_metrics(pks_ordered, "W3", "one_event")
```

Plot it one more time, but maybe with sequence dictated by graph theory solution?
```{r}
pks_ordered <- 
W3_cheap %>% 
      filter(ID %in% remainder) %>% 
  rename("pk_order" = sequence)

test <- 
  input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
    mutate(day = day(datetime),
           hour = hour(datetime)) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(precip1, by = c("day", "hour")) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
  #filter(mins.x == 0)
test2 <- test %>% 
  select(datetime.x, binary, hourly_precip, pk_order)
sec2 <- with(test2, train_sec(pk_order, hourly_precip))

binary <- c("#DB995A",
            "#586BA4"
  )


tiles <- 
test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = pk_order, fill = as.character(binary)))+
  scale_y_continuous("Supposed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                    values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23",
                              "10/23/23",
                              "10/24/23"))+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 0))

(water)/tiles + plot_layout(heights = c(1, 5))
```

```{r W3-map-panels}
#create a template for maps, then plot the flowing state at different timesteps on the template
#map of watershed 3 with depth to bedrock
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
#hill <- rast(hillshade_out)

#dem
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
#crop to rectangular area
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#writeRaster(crop1, "1mdemw3_cropped.tif")

#watershed boundary
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)

#w3 network- thing I need to change
#read in shapefile of stream converted in ARC
vect_stream_path <- "./AGU24posterAnalysis/vector_stream/vector_stream.shp"
#stream as a vector
vect_stream <- vect(vect_stream_path)
#plot(vect_stream)
#crop to watershed boundary
w3_stream_crop <- crop(vect_stream, w3_outline)
#plot(w3_stream_crop)
#or i could use old classification

#point locations- snapped points from above chunk
w3_stic_locs_snap <- "w3_stic_locs_snap.shp"

w3_stic_locs_r <- vect(w3_stic_locs_snap) %>% 
  left_join(pks_w3, by = "ID")



w3_stic_locs_r <- vect(w3_stic_locs_snap)
#writeVector(w3_stic_locs_r, "./seismic_map_exports/w3_stic_locs_snap.shp")


w3_net <- vect("./carrieZigZag/w3_network.shp")
#writeVector(w3_net, "./seismic_map_exports/network.shp")

#plot(w3_net)


#test <- 
ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "darkslategray3", lwd = 2) +
  geom_sf(data = w3_stic_locs_r, colour = "midnightblue", size = 2) +
  #geom_sf(data = dd, aes(color = (depth)), pch = 19, size = 3) +
  scale_color_gradient(low = "black", high = "white")+
  #geom_sf(data = w3_pour, colour = "black") +
   scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))
  
# determine the flowing state
doi <- c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00"))
  
states <- input_w3 %>% 
    filter(datetime %in% doi[5])

#w3_stic_locs_r %>% left_join(states, by = "ID")

ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
   
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(binary)), size = 4, pch = 21) +
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0, "cm"),
                              pad_y = unit(1, "cm"))

```


## Figure 5: Distributions of proportion of transitions that follow a sequence
Distributions of prop values, with dashed lines for median, faceted by timescale and watershed, colored by method for determining sequence
```{r combine-and-plot}
four_colors <- c("#231f20", "#bb4430", "#7ebdc2", "#f3dfa2")

rbind(all_graph_solutions,
      all_pk,
      all_twi) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    #geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = four_colors,
                     name = "Method")+
  scale_color_manual(values = four_colors,
                     name = "Method")+
  facet_grid(shed~timescale)
```
```{r combine-and-plot-two}



#fixing randomly generated sequence
fixed_combined <- 
rbind(all_pk,
      all_twi, 
      all_new,
      #all_conc,
      all_random
      ) 

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")
five_colors <- c("#d68c45",  "#247BA0", "#A30B37", "#F0C808", "#2c6e49")
#fixing names for legend, and factor levels

fixed_combined %>% 
  ungroup() %>% 
  # mutate(method = case_when(method == "random" ~ "Random",
  #                           method == "Flow Permanence" ~ "Proportion of Time Flowing",
  #                           method == "cheapest_insertion" ~ "TSP Solution",
  #                           method == "Topographic Wetness Index" ~ "Topographic Wetness Index")) %>%
  mutate(timescale = case_when(timescale == "30mins" ~ "30 Minute Frequency",
                               timescale == "daily" ~ "Daily Average Flow State")) %>%
  mutate(shed = fct_relevel(shed,
                              c("W3", "FB", "ZZ"))) %>%
  # mutate(method = fct_relevel(method,
  #                             c("TSP Solution",  "Proportion of Time Flowing", "Random",
  #                                "Topographic Wetness Index"))) %>%
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    stat_central_tendency(aes(color = method), type = "median", linetype = "dashed")+
    #geom_density(alpha = 0.5, lty = 3)+
     # geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_classic2()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Proportion of Transitions where Sequence is Followed",
       x = "Proportion of transitions that are sequential",
       y = "Density")+
  scale_fill_manual(values = four_colors,
                     name = "Method")+
  scale_color_manual(values = four_colors,
                     name = "Method")+
  scale_linetype_manual(values = 2, name = "Median")+
  scale_x_continuous(labels = c("0", "0.25", "0.5", "0.75", "1"), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(panel.spacing = unit(1, "lines"))+
  facet_grid(shed~timescale)
```

```{r combine-and-plot-two}
#fixing randomly generated sequence
fixed_combined2 <- 
rbind(general_graph(W3_IDs, "W3", methods = c("random"), two_opt = FALSE),
      general_graph(W3_IDs, "W3", methods = c("cheapest_insertion"), two_opt = TRUE),
      all_pk,
      all_twi,
      mutate(calc_props(opt_routes, "W3"), method = "concorde")
      ) %>% 
  filter(shed == "W3")

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2", "green")

#fixing names for legend, and factor levels

fixed_combined2 %>% 
  ungroup() %>% 
  # mutate(method = case_when(method == "random" ~ "Random",
  #                           method == "Flow Permanence" ~ "Proportion of Time Flowing",
  #                           method == "cheapest_insertion" ~ "TSP Solution",
  #                           method == "Topographic Wetness Index" ~ "Topographic Wetness Index")) %>% 
  # mutate(timescale = case_when(timescale == "30mins" ~ "30 Minute Frequency",
  #                              timescale == "daily" ~ "Daily Average Flow State")) %>% 
  # mutate(method = fct_relevel(method, 
  #                             c("TSP Solution",  "Proportion of Time Flowing", "Random",
  #                                "Topographic Wetness Index", "concorde"))) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    stat_central_tendency(aes(color = method), type = "median", linetype = "dashed")+
    #geom_density(alpha = 0.5, lty = 3)+
     # geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_classic2()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Proportion of Transitions where Sequence is Followed",
       x = "Proportion of transitions that are sequential",
       y = "Density")+
  scale_fill_manual(values = four_colors,
                     name = "Method")+
  scale_color_manual(values = four_colors,
                     name = "Method")+
  scale_linetype_manual(values = 2, name = "Median")+
  scale_x_continuous(labels = c("0", "0.25", "0.5", "0.75", "1"), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(panel.spacing = unit(1, "lines"))+
  facet_grid(shed~timescale)

fixefixefixed_combined %>% 
  filter(timescale == "30mins", 
         shed == "W3",
         method %in% c("cheapest_insertion", "Flow Permanence")) %>% print(n = 50)
```

Can also calculate proportion of sequential behavior through time! Comes out looking better than modeling using Botter & Durighetto

```{r calculate-how-many-follow-sequence}
#sequential <- 
  input_w3 %>% 
  left_join(pks_w3, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed.x, wshed.y, mins, datetime)) %>% 
  arrange(desc(pk)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>%
  filter(datetime == ymd_hms("2023-07-03 17:00:00")) %>% print(n = 25)
  filter(following != -1) %>% 
    summarise(count_non = length(following))

total <- input_w3 %>% 
  left_join(pks_w3, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed.x, wshed.y, mins, datetime)) %>% 
  arrange(desc(pk)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
    summarise(count_all = length(following))

final <- left_join(sequential, total, by = "datetime") %>% 
  mutate(prop = count_non/count_all)

final %>% 
  mutate(diff = count_all - count_non) %>% 
  summarise(sum(diff),
            sum(count_all))

```

Old method to calculate how well sequences worked:
Once sequence is defined, I can compare to the actual flow data
```{r}
#sequential <- 
input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  arrange((sequence)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  #filter(following != -1) %>% 
  filter(datetime == "2023-07-15 19:00:00") %>% View()
   # summarise(count_non = length(following)) %>% View()
    
input_w3 %>% 
  left_join(W3_pk_seq, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  arrange((sequence)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  #filter(following != -1) %>% 
  filter(datetime == "2023-07-15 19:00:00") %>% View()
    summarise(count_non = length(following))

#make a df with ID and the number in the sequence
W3_pk_seq <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_w3$ID), 1)) %>% 
  select(ID, sequence)
  

total <- input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins, datetime)) %>% 
  arrange(sequence) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
    summarise(count_all = length(following))

final <- left_join(sequential, total, by = "datetime") %>% 
  mutate(prop = count_non/count_all)

#write a function to calculate this in 1 quick step
calc_time <- function(sequence_df, shed, seq_label){
  
  if(shed == "W3"){
    input_f <- input_w3
  } else if(shed == "FB"){
        input_f <- input_fb
  } else if(shed == "ZZ"){
        input_f <- input_zz
  }
  
sequential <- input_f %>% 
  left_join(sequence_df, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins, datetime)) %>% 
  arrange(sequence) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  filter(following != -1) %>% 
    summarise(count_non = length(following))

total <- input_f %>% 
  left_join(sequence_df, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins, datetime)) %>% 
  arrange(sequence) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
    summarise(count_all = length(following))

final <- left_join(sequential, total, by = "datetime") %>% 
  mutate(prop = count_non/count_all) %>% 
  mutate(shed = shed)

output <- q_24_bind %>% 
  inner_join(final, by = "datetime") %>% 
  mutate(seq_label = seq_label)

return(output)
}

calc_time(W3_cheap, "W3", "cheapest")
```
```{r plot-along-discharge}

rbind(
calc_time(W3_cheap, "W3", "cheapest"),
calc_time(W3_random, "W3", "random"),
calc_time(w3_twi_sequence, "W3", "twi"),
calc_time(W3_pk_seq, "W3", "pk")
) %>% 
  ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line(aes(color = prop))+
  labs(title = "Discharge from W3, 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()+
  scale_color_continuous(type = "viridis",
                             limits = c(0,1))+
                       theme(rect = element_rect(fill = "transparent", color = NA))+
  facet_grid(seq_label~shed)

```
```{r difference-plot}
pk_props_join <- calc_time(W3_pk_seq, "W3", "pk") %>% 
  select(datetime, prop) %>% 
  rename("pk_prop" = prop)

rbind(
calc_time(W3_cheap, "W3", "cheapest"),
calc_time(W3_random, "W3", "random"),
calc_time(w3_twi_sequence, "W3", "twi"),
calc_time(W3_pk_seq, "W3", "pk")
) %>% 
  left_join(pk_props_join, by = "datetime") %>% 
  filter(seq_label != "pk") %>% 
  mutate(diff = prop - pk_prop) %>% 
  ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line(aes(color = diff))+
  labs(title = "Difference between Flow permanence sequence and others, 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()+
  scale_color_gradient2(low = "red", 
    mid = "white", 
    high = "blue", 
    midpoint = 0,
    limits = c(-0.3, 0.3))+
                       theme(rect = element_rect(fill = "transparent", color = NA))+
  facet_grid(seq_label~shed)
```

New idea: bar graphs, where the height of the bar represents the proportion of sequential transitions
```{r}
#calculate the total number of timesteps, and how many contain a transition in state?


test <- input_w3 %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) #%>% 
  #filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change))

length(test$datetime)

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change)) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count, fill = state_change))+
  geom_histogram(stat = "identity")

#changing_dates <- 
  test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "none") #%>% select(datetime) %>% unique() %>% 
  mutate(status = "changing")

`%not_in%` <- purrr::negate(`%in%`)

#bar of total timesteps
test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates) %>% 
  group_by(status) %>% 
  summarise(count = length(status)) %>% 
  mutate(shed = "W3") %>% 
  ggplot(aes(x = shed, y = count, fill = status))+
  geom_histogram(stat = "identity")
  
test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "none") %>% unique() %>% 
  group_by(state_change) %>% 
  summarise(timesteps = length(datetime))

test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
  summarise(length(datetime))

test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "wetting",
         state_change != "drying") %>% unique() %>% 
  group_by(state_change) %>% 
  summarise(timesteps = length(datetime))

summarise(how_many = length(unique(state_change))) %>% 
  ungroup() %>% 
  filter(how_many > 1)
```

```{r define-functions}
filtered_input <- input_w3 %>%
      filter(mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
#calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
i <- 1
child <- paste0("r_",routes_w3$up[i])
parent <- paste0("r_",routes_w3$down[i])
input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(child, parent)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      #filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()

      #View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

# if(check <= 2){
#   sequence_df <- data.frame("Sequence" = NA, 
#                             "Frequency" = NA,
#                             "up" = up,
#                             "down" = down)
#   return(sequence_df)
# } 
# else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
#output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
#supports <- c("0001","0111","1101", "0100")


#sub <- filter(sequence_df, Sequence %in% supports)
#output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)
}


#test function

calc_support_combos("r_18", "r_11", input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
#iterate_groups_combos <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

i <- 4
up <- paste0("r_",routes_w3$up[i])
down <- paste0("r_",routes_w3$down[i])
timestep <- minutes(30)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
    print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support_combos(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

iterate_groups_combos("r_13", "r_19", input, minutes(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_combos <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  #out <- iterate_groups_combos(up, down, input, timestep)
    out <- calc_support_combos(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down, Sequence) %>%
    summarise(Frequency = sum(Frequency))
  return(final_output)
}
test <- calculate_hierarchy_combos(routes_w3, filtered_input, minutes(30))

test %>% 
  group_by(Sequence) %>% 
  summarise(sum(Frequency))

supports <- c("0001","0111","1101", "0100")

test %>% ungroup() %>% 
  filter(Sequence %in% supports) %>% 
  summarise(sum(Frequency))

test %>% ungroup() %>% 
  #filter(Sequence %in% supports) %>% 
  summarise(sum(Frequency))

#create a key for wetting and drying
wetting <- c("0001", "0010", "0011", "0111", "1011")
drying <- c("0100", "1000", "1100", "1101", "1110")
no_net_change <- c("0000", "0101", "0110", "1001", "1010", "1111")
#create key for sequential and non
sequential <- c("0001", "0100", "0111", "1101")
nonsequential <- c("0010", "1000", "1011", "1110")
undeterminable <- c("0011","0110", "1001","1100")
no_change2 <- c("0000", "1111", "0101", "1010")

test2 <- test %>% 
  #mutate(Sequence = as.numeric(Sequence)) %>% 
  mutate(status = case_when(Sequence %in% wetting ~ "wetting",
                            Sequence %in% drying ~ "drying",
                            Sequence %in% no_net_change ~ "no_net_change"),
         category = case_when(Sequence %in% sequential ~ "sequential",
                              Sequence %in% nonsequential ~ "nonsequential",
                              Sequence %in% undeterminable ~ "undeterminable",
                              Sequence %in% no_change2 ~ "no_change")) %>% 
  group_by(status, category) %>% 
  summarise(count = sum(Frequency))

test2 %>% ungroup() %>% 
  group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count, fill = category))+
  geom_histogram(stat = "identity")

#second plot- what proportion of transitions are sequential or not
#ultimately one of these for each sequence?
test2 %>% ungroup() %>% 
  filter(category != "no_change") %>% 
  group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count, fill = category))+
  geom_histogram(stat = "identity")


#fantastic four function has been modified, to do average daily state. Removed hourly and 4 hr time blocks
fantastic_four_combos <- function(routes, shed){
  theFour <- c("30mins", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed) %>% 
      mutate(
             "day" = day(datetime),
             "month" = month(datetime),
             "year" = year(datetime)) %>% 
      group_by(day, month, year, ID) %>%
      summarise(avg_state = mean(binary)) %>% 
      ungroup() %>% 
      mutate(avg_state = round(avg_state)) %>% 
      rename("binary" = avg_state) %>% 
      mutate("datetime" = ymd_hms(paste0(year,"-",month,"-",day," ","00:00:00"))) %>% 
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
          arrange(datetime) %>% 
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_combos(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}


fantastic_four_combos(routes_w3, "W3")
# function to determine proportion of transitions that state changes follow all parent child relationships
calc_props_silly <- function(routes, shed){
  full_combos <- routes
total_state_changes <- full_combos %>% 
    filter(Sequence %not_in% c(undeterminable, no_change2)) %>% 
    group_by(up, down) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- full_combos %>% 
    filter(Sequence %not_in% c(undeterminable, no_change2)) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
return(un_split)
}

calc_props_silly(test, "W3")
```

Final first graph
```{r W3}
#final first graph
test <- input_w3 %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) #%>% 
  #filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change))

length(test$datetime)

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change)) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count, fill = state_change))+
  geom_histogram(stat = "identity")

changing_dates <- 
  test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "none") %>% select(datetime) %>% unique() %>% 
  mutate(status = "changing")
  
  length(changing_dates$datetime)/48
  length(unique(input_w3$datetime))/48

`%not_in%` <- purrr::negate(`%in%`)

#bar of total timesteps
w3_bar_ready <- test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates) %>% 
  group_by(status) %>% 
  summarise(count = length(status)) %>% 
  mutate(shed = "W3")
  #ungroup() %>% 

w3_bar_ready %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = status))+
  geom_histogram(stat = "identity")

#one more thing, looking at the accuracy during times of transition
make_change <- test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates)

df_summary_time_w3 %>% 
  pivot_longer(-datetime) %>% 
left_join(make_change, by = "datetime") %>% 
  group_by(name, status) %>% 
  summarise(average_acc = mean(value),
            sd(value))
```
```{r FB}
#final first graph
test <- input_fb %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) #%>% 
  #filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change))

length(test$datetime)

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change)) %>% 
  mutate(shed = "FB" ) %>% 
  ggplot(aes(x = shed, y = count, fill = state_change))+
  geom_histogram(stat = "identity")

changing_dates <- 
  test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "none") %>% select(datetime) %>% unique() %>% 
  mutate(status = "changing")
  
#  length(changing_dates$datetime)/48
#  length(unique(input_w3$datetime))/48

#bar of total timesteps
fb_bar_ready <- test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates) %>% 
  group_by(status) %>% 
  summarise(count = length(status)) %>% 
  mutate(shed = "FB")

fb_bar_ready %>% 
  #ungroup() %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = status))+
  geom_histogram(stat = "identity")
```
```{r ZZ}
#final first graph
test <- input_zz %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) #%>% 
  #filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change))

length(test$datetime)

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change)) %>% 
  mutate(shed = "ZZ" ) %>% 
  ggplot(aes(x = shed, y = count, fill = state_change))+
  geom_histogram(stat = "identity")

changing_dates <- 
  test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "none") %>% select(datetime) %>% unique() %>% 
  mutate(status = "changing")
  
#  length(changing_dates$datetime)/48
#  length(unique(input_w3$datetime))/48

#bar of total timesteps
zz_bar_ready <- test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates) %>% 
  group_by(status) %>% 
  summarise(count = length(status)) %>% 
  mutate(shed = "ZZ")

zz_bar_ready %>% 
  #ungroup() %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = status))+
  geom_histogram(stat = "identity")
```

Final second graph
```{r}
filtered_input <- input_w3 %>%
      filter(mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 1
# child <- paste0("r_",routes_w3$up[i])
# parent <- paste0("r_",routes_w3$down[i])
# input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(child, parent)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      #filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()

      #View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

# if(check <= 2){
#   sequence_df <- data.frame("Sequence" = NA, 
#                             "Frequency" = NA,
#                             "up" = up,
#                             "down" = down)
#   return(sequence_df)
# } 
# else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
#output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
#supports <- c("0001","0111","1101", "0100")


#sub <- filter(sequence_df, Sequence %in% supports)
#output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)
}


#test function

#calc_support_combos("r_18", "r_11", input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_combos <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes_w3$up[i])
# down <- paste0("r_",routes_w3$down[i])
# timestep <- minutes(30)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
    print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support_combos(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

#iterate_groups_combos("r_13", "r_19", input, minutes(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_combos <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  #out <- iterate_groups_combos(up, down, input, timestep)
    out <- calc_support_combos(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down, Sequence) %>%
    summarise(Frequency = sum(Frequency))
  return(final_output)
}

test <- calculate_hierarchy_combos(routes_w3, filtered_input, minutes(30))
calc_props_silly(test, "W3")$prop %>% hist()


#create a key for wetting and drying
wetting <- c("0001", "0010", "0011", "0111", "1011")
drying <- c("0100", "1000", "1100", "1101", "1110")
no_net_change <- c("0000", "0101", "0110", "1001", "1010", "1111")
#create key for sequential and non
sequential <- c("0001", "0100", "0111", "1101")
nonsequential <- c("0010", "1000", "1011", "1110")
undeterminable <- c("0011","0110", "1001","1100")
no_change2 <- c("0000", "1111", "0101", "1010")

test2 <- test %>% 
  #mutate(Sequence = as.numeric(Sequence)) %>% 
  mutate(status = case_when(Sequence %in% wetting ~ "wetting",
                            Sequence %in% drying ~ "drying",
                            Sequence %in% no_net_change ~ "no_net_change"),
         category = case_when(Sequence %in% sequential ~ "sequential",
                              Sequence %in% nonsequential ~ "nonsequential",
                              Sequence %in% undeterminable ~ "undeterminable",
                              Sequence %in% no_change2 ~ "no_change")) %>% 
  group_by(status, category) %>% 
  summarise(count = sum(Frequency))

# test2 %>% ungroup() %>% 
#   group_by(category) %>% 
#   mutate(shed = "W3" ) %>% 
#   ggplot(aes(x = shed, y = count, fill = category))+
#   geom_histogram(stat = "identity")

#second plot- what proportion of transitions are sequential or not
#ultimately one of these for each sequence?
test2 %>% ungroup() %>% 
  filter(category != "no_change") %>% 
  #group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = category))+
  geom_histogram(stat = "identity")


#do it again, but for twi
routes_w3_twi

hist_prep <- function(routes) {
  test <- calculate_hierarchy_combos(routes, filtered_input, minutes(30))


#create a key for wetting and drying
wetting <- c("0001", "0010", "0011", "0111", "1011")
drying <- c("0100", "1000", "1100", "1101", "1110")
no_net_change <- c("0000", "0101", "0110", "1001", "1010", "1111")
#create key for sequential and non
sequential <- c("0001", "0100", "0111", "1101")
nonsequential <- c("0010", "1000", "1011", "1110")
undeterminable <- c("0011","0110", "1001","1100")
no_change2 <- c("0000", "1111", "0101", "1010")

test2 <- test %>% 
  #mutate(Sequence = as.numeric(Sequence)) %>% 
  mutate(status = case_when(Sequence %in% wetting ~ "wetting",
                            Sequence %in% drying ~ "drying",
                            Sequence %in% no_net_change ~ "no_net_change"),
         category = case_when(Sequence %in% sequential ~ "sequential",
                              Sequence %in% nonsequential ~ "nonsequential",
                              Sequence %in% undeterminable ~ "undeterminable",
                              Sequence %in% no_change2 ~ "no_change")) %>% 
  group_by(status, category) %>% 
  summarise(count = sum(Frequency))

return(test2)
}

hist_prep(routes_w3_twi) %>%
  ungroup() %>% 
  filter(category != "no_change") %>% 
  #group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = category))+
  geom_histogram(stat = "identity")

calc_props_silly(calculate_hierarchy_combos(routes_w3_twi, filtered_input, minutes(30)), "W3")$prop %>% hist()

#again for graph theory
W3_cheap
W3_random

routes_w3_cheap <- W3_cheap %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID)  %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

hist_prep(routes_w3_cheap) %>%
  ungroup() %>% 
  filter(category != "no_change") %>% 
  #group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = category))+
  geom_histogram(stat = "identity")

calc_props_silly(calculate_hierarchy_combos(routes_w3_cheap, filtered_input, minutes(30)), "W3")$prop %>% hist()


#again for random sequence
routes_w3_random <- W3_random %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID)  %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

hist_prep(routes_w3_random) %>%
  ungroup() %>% 
  filter(category != "no_change") %>% 
  #group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = category))+
  geom_histogram(stat = "identity")
```

## Figure 6: Chains
Plot that will show multiple chains beside each other, and highlight where they are different
```{r}
#Make dataframe to contain the locations of points
rbind(#mutate(W3_cheap, method = "cheapest_insertion",x = 1),
      mutate(W3_pk_seq, method = "pk", x = 1),
      #mutate(convert_to_IDseq(W3_random), method = "random", x = 3),
       mutate(convert_to_IDseq(new_seq_w3), method = "new", x = 2),
      mutate(w3_twi_sequence, method = "twi", x = 3)
     
      #mutate(convert_to_IDseq(opt_routes_w3), method = "con", x = 6)
      ) %>% 
  ggplot()+
  geom_point(aes(x = x, y = sequence), size = 5)+
  geom_text(aes(x = x, y = sequence, label = ID), color = "white", size = 2)+
  labs()
```


## Figure 7: Hydrograph, colored by which sequence works the best
NEED TO FIX COLORS
```{r}
five_colors <- c("#d68c45",  "#247BA0", "#A30B37", "#F0C808", "#2c6e49")


#plot the best at each timestep
p_load(ggbreak)
df_long_w3 %>% 
  filter(model != "random_accuracy",
         model != "cheap_accuracy",
         model != "con_accuracy") %>% 
  group_by(datetime) %>% 
  filter(accuracy == max(accuracy)) %>% 
  mutate(dup = as.character(n() > 1)) %>% ungroup()  %>%
  mutate(unique_sol = case_when(dup == TRUE ~ "z_tie",
                                dup == FALSE ~ model)) %>%
inner_join(big_comb, by = "datetime") %>%
  ggplot(aes(x = datetime, y = Q_mm_day, color = unique_sol)) +
  #geom_point() +
  geom_point(size = 1)+
  theme_classic()+
  scale_color_manual(values = c(five_colors[1:3], "lightgrey"))+
  labs(title = "Most accurate model along hydrograph", y = "Discharge (mm/day)", x = "")+
 # facet_wrap(~ unique_sol)+
    scale_x_break(c(ymd_hms("2023-11-12 00:00:00"), ymd_hms("2024-05-20 00:00:00")))
  

```

## Figure 8: Maps of where sequences work in space
Set of map panels where symbol color = flowing state, outline color or shape indicating correctly predicted or not

For these plots I need:
- actual flowing state
- map template
- predicted state based on a sequence

Run lines 2815 and beyond
```{r model_predictions}
cheap_model <- calc_model_result(W3_cheap$ID) %>% rename("cheapest" = pred_binary)
pk_model <- calc_model_result(W3_pk_seq$ID) %>% rename("pk" = pred_binary)
random_model <- calc_model_result(W3_random$ID) %>% rename("random" = pred_binary)
twi_model <- calc_model_result(w3_twi_sequence$ID) %>% rename("twi" = pred_binary)

comparison <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model, by = c("datetime", "ID")) %>% 
  left_join(pk_model, by = c("datetime", "ID")) %>% 
    left_join(random_model, by = c("datetime", "ID")) %>% 
  left_join(twi_model, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins) %>% 
  drop_na()

#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
get_eval_label(0,0)
#apply get_eval function to each model run
comparison$cheapest_eval <- mapply(get_eval_label, comparison$binary, comparison$cheapest)
comparison$pk_eval <- mapply(get_eval_label, comparison$binary, comparison$pk)
comparison$random_eval <- mapply(get_eval_label, comparison$binary, comparison$random)
comparison$twi_eval <- mapply(get_eval_label, comparison$binary, comparison$twi)

```
```{r sample-based-on-discharge}
#select random timesteps evenly distributed along flow duration curve
q_23_bind <- 
  q_23_f %>% 
  select(DATETIME, Q_mm_day) %>% 
  rename("datetime" = DATETIME
         )

q_24_bind <- 
  q_24_f %>% 
  select(datetime, Q_mmperday) %>% 
  rename("Q_mm_day" = Q_mmperday)

big_comb <- rbind(q_23_bind, q_24_bind) #%>% 
  inner_join(comparison, by = "datetime")
```
### W3
From earlier, contains map template and actual flowing state
```{r W3-map-panels}
#create a template for maps, then plot the flowing state at different timesteps on the template
#map of watershed 3 with depth to bedrock
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
#hill <- rast(hillshade_out)

#dem
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
#crop to rectangular area
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#writeRaster(crop1, "1mdemw3_cropped.tif")

#watershed boundary
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)

#w3 network- thing I need to change
#read in shapefile of stream converted in ARC
vect_stream_path <- "./AGU24posterAnalysis/vector_stream/vector_stream.shp"
#stream as a vector
vect_stream <- vect(vect_stream_path)
#plot(vect_stream)
#crop to watershed boundary
w3_stream_crop <- crop(vect_stream, w3_outline)
#plot(w3_stream_crop)
#or i could use old classification

#point locations- snapped points from above chunk
w3_stic_locs_snap <- "w3_stic_locs_snap.shp"

w3_stic_locs_r <- vect(w3_stic_locs_snap) %>% 
  left_join(pks_w3, by = "ID")



w3_stic_locs_r <- vect(w3_stic_locs_snap)
#writeVector(w3_stic_locs_r, "./seismic_map_exports/w3_stic_locs_snap.shp")


w3_net <- vect("./carrieZigZag/w3_network.shp")
#writeVector(w3_net, "./seismic_map_exports/network.shp")

#plot(w3_net)


#test <- 
ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "darkslategray3", lwd = 2) +
  geom_sf(data = w3_stic_locs_r, colour = "midnightblue", size = 2) +
  #geom_sf(data = dd, aes(color = (depth)), pch = 19, size = 3) +
  scale_color_gradient(low = "black", high = "white")+
  #geom_sf(data = w3_pour, colour = "black") +
   scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))
  
# determine the flowing state
doi <- c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00"))
  
states <- big_comb %>% 
    filter(datetime %in% doi[1])

#w3_stic_locs_r %>% left_join(states, by = "ID")

#figure without scale bar and legends
ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(binary),
            color = cheapest_eval), size = 4, pch = 21, stroke = 2) +
  scale_color_manual(values = c("orange", "black", "limegreen"),
                     name = "")+
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position = "")
  # ggspatial::annotation_scale(location = 'tr', pad_x = unit(0, "cm"),
  #                             pad_y = unit(1, "cm"))
#figure with scale bar and legends
ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(binary),
            color = cheapest_eval), size = 4, pch = 21) +
  scale_color_manual(values = c("orange", "black", "limegreen"),
                     name = "")+
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0, "cm"),
                              pad_y = unit(1, "cm"))

```
```{r}
#make the same thing as above, but pivot longer and then plot using facets
#states <- 
states <- comparison %>% 
    filter(datetime %in% doi) %>% 
  pivot_longer(cols = c(cheapest, pk, random, twi),
               values_to = "prediction", 
               names_to = "sequence")

states$eval <- mapply(get_eval_label, states$binary, states$prediction)

ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 1)+
  geom_sf(data = w3_net, colour = "grey", lwd = 0.8) +
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(prediction),
            color = eval), size = 2, stroke = 1, pch = 21) +
  scale_color_manual(values = c("green", "black", "orange"),
                     name = "")+
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position = "")+
  facet_grid(sequence~datetime)

```
```{r}
#make a mini bar plot to show the flowing accuracy at each time step
maps <- ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 1)+
  geom_sf(data = w3_net, colour = "grey", lwd = 0.8) +
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(prediction),
            color = eval), size = 2, stroke = 1, pch = 21) +
  scale_color_manual(values = c("green", "black", "orange"),
                     name = "")+
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position = "")
# +
#   facet_grid(sequence~datetime)

bars <- ggplot(filter(states, datetime == doi[1]), aes(y = datetime, fill = eval)) + 
  geom_bar()+
    theme_void()+
  scale_fill_manual(values = c("green", "black", "orange"))+
  theme(
        legend.position = "")
# +
#   facet_grid(sequence~datetime)

(maps/(bars)) +
  plot_layout(widths = c(1, 1), heights = unit(c(12, 1), c('cm', 'null')))


```
Map where the points are colored by accuracy, and faceted by hydrograph component and sequence
```{r W3-map}
#summarize through time
ID_acc <- comparison_w3 %>%
    inner_join(discharge_df, by = "datetime") %>% 
  group_by(ID, event_type) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total
  ) %>%
  select(ID, event_type, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy,pk_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy")


#figure with scale bar and legends
ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(ID_acc, by = "ID"), 
        aes(fill = accuracy, shape = event_type), size = 4) +
  # scale_color_manual(values = c("orange", "black", "limegreen"),
  #                    name = "")+
  scale_fill_continuous(type = "viridis",
                             limits = c(0,1))+
  scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0, "cm"),
                              pad_y = unit(1, "cm"))+
  facet_grid(event_type ~ model)

#another version, where the points maintain their colors from earlier, but accuracy is illustrated using size or alpha
ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(ID_acc, by = "ID"), 
        aes(alpha = accuracy, shape = event_type, fill = model), size = 4) +
  scale_fill_manual(values = four_colors,
                     name = "")+
  scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid(event_type ~ model)

## Plot that is just a single row
ID_acc_w3 <- comparison_w3 %>%
    #inner_join(fb_limbs, by = "datetime") %>% 
  group_by(ID) %>%
  summarize(
    #cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
    total = n(),
    #cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct/total
  ) %>%
  select(ID, new_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(new_accuracy,pk_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy")

ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(ID_acc_w3, by = "ID"), 
        aes(alpha = accuracy, fill = model), size = 4, pch = 21) +
  scale_fill_manual(values = four_colors,
                     name = "")+
  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid( ~ model)
```

```{r W3-one-row}
ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(ID_acc_w3, by = "ID"), 
        aes(fill = accuracy), size = 4, pch = 21) +
  scale_fill_gradientn(colors = c("white","lightgrey","darkgrey", "black"))+
  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid( ~ model)
```

### FB
Make map template:
```{r FB-map}
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

#define the rectangular area that will be shown on final map
ybounds <- c(4868850,4869650)
xbounds <- c(279350, 280450)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
#points(lcc)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./fb_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
fb_crop <- "./fb_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
fb_net <- vect("./carrieZigZag/FB_network.shp")

###pour point to define where the watershed boundary is
#manually type coords from windows computer
fb_pour_coords <- data.frame("easting" = 280400,
                             "northing" = 4869120)
#convert to SpatVector object
fb_pour <- vect(fb_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
fb_pour_filename <- "./fb_dems/fb_pour.shp"
#writeVector(fb_pour, fb_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
b_crop <- "./fb_dems/1mdem_crop.tif"

fb_breached <- "./fb_dems/1mdem_breach.tif"
# wbt_breach_depressions_least_cost(
#   dem = fb_crop,
#   output = fb_breached,
#   dist = 1,
#   fill = TRUE)

fb_filled <- "./fb_dems/1mdem_fill.tif"
# wbt_fill_depressions_wang_and_liu(
#   dem = fb_breached,
#   output = fb_filled
# )
#calculate flow accumulation and direction
fb_flowacc <- "./fb_dems/1mdem_fb_flowacc.tif"
# wbt_d8_flow_accumulation(input = fb_filled,
#                          output = fb_flowacc)
# plot(rast(fb_flowacc))
fb_d8pt <- "./fb_dems/1mdem_fb_d8pt.tif"
# wbt_d8_pointer(dem = fb_filled,
#                output = fb_d8pt)
# plot(rast(fb_d8pt))


#delineate streams
fb_streams <- "./fb_dems/fb_streams.tif"
# wbt_extract_streams(flow_accum = fb_flowacc,
#                     output = fb_streams,
#                     threshold = 8000)
# plot(rast(fb_streams))
# points(lcc)
#snap pour point to streams
fb_pour_snap <- "./fb_dems/fb_pour_snap.shp"
# wbt_jenson_snap_pour_points(pour_pts = fb_pour_filename,
#                             streams = fb_streams,
#                             output = fb_pour_snap,
#                             snap_dist = 10)
fb_pour_snap_read <- vect("./fb_dems/fb_pour_snap.shp")

fb_shed <- "./fb_dems/fb_shed.tif"
# wbt_watershed(d8_pntr = fb_d8pt,
#               pour_pts = fb_pour_snap,
#               output = fb_shed)

#convert raster of watershed area to vector for final mapping
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)

#get sensor locations from STIC data, format
locs_fb <- data_23 %>% 
  filter(wshed == "FB") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape_fb <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc_fb <- terra::project(locs_shape_fb, crs(m1))

#assign destination for hillshade calculation
hillshade_out <- "./fb_dems/1mdem_hillshade.tif"
# wbt_hillshade(
#   dem = fb_crop,
#   output = hillshade_out,
# )
hill <- rast(hillshade_out)

#also clip and mask shapefiles
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)
fb_net <- vect("./carrieZigZag/FB_network.shp")
plot(ext(fb_outline))

plot(maptools::elide(sf::st_as_sf(fb_outline), 90), add = TRUE)
s <- sf::st_as_sf(v)

plot(ext(fb_net), add = TRUE)
plot(fb_net, add = TRUE)
plot(fb_outline, add = TRUE)
plot(centroids(vect(ext(fb_net))))
unname(ext(fb_outline)[1:4])

polygon_cropped <- crop(fb_net, unname(ext(fb_outline)[1:4]))
plot(ext(polygon_cropped), add = TRUE, col = "red")

fb_net <- terra::crop(fb_net, ext(fb_outline))

plot(ext(fb_net), add = TRUE, col = "red")


rescale()

plot(ext(fb_net))
plot(fb_net, add = TRUE)
#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.

  
#copy and pasted from W3 chunk
  #summarize through time
ID_acc_fb <- comparison_fb %>%
    inner_join(fb_limbs, by = "datetime") %>% 
  group_by(ID, event_type) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total
  ) %>%
  select(ID, event_type, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy,pk_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy")

#figure with scale bar and legends

#another version, where the points maintain their colors from earlier, but accuracy is illustrated using size or alpha
ggplot()+
  theme_void()+
  geom_sf(data = fb_outline, fill = NA, color = "#397367", alpha = 0.3, lwd = 2)+
  geom_sf(data = fb_net, colour = "grey", lwd = 1) +
geom_sf(data = locs_shape %>% inner_join(ID_acc_fb, by = "ID"), 
        aes(alpha = accuracy, shape = event_type, fill = model), size = 4) +
  scale_fill_manual(values = four_colors,
                     name = "")+
  scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid(event_type ~ model)


## Plot that is just a single row
ID_acc_fb <- comparison_fb %>%
    #inner_join(fb_limbs, by = "datetime") %>% 
  group_by(ID) %>%
  summarize(
    #cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
    total = n(),
    #cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct / total
  ) %>%
  select(ID, pk_accuracy, random_accuracy, twi_accuracy, new_accuracy) %>%
  tidyr::pivot_longer(cols = c(pk_accuracy, random_accuracy, twi_accuracy, new_accuracy), 
                      names_to = "model", values_to = "accuracy")

ggplot()+
  theme_void()+
  geom_sf(data = fb_outline, fill = NA, color = "#397367", alpha = 0.3, lwd = 2)+
  geom_sf(data = fb_net, colour = "grey", lwd = 1) +
geom_sf(data = locs_shape %>% inner_join(ID_acc_fb, by = "ID"), 
        aes(alpha = accuracy, fill = model), size = 4, pch = 21) +
  scale_fill_manual(values = four_colors,
                     name = "")+
  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid( ~ model)

```
```{r final-map-one-row}

#terra::spin(fb_outline, 90, 279933.5, 4869173)
ggplot()+
  theme_void()+
  geom_sf(data = terra::spin(fb_outline, 90, 279933.5, 4869173), fill = NA, color = "#397367", alpha = 0.3, lwd = 2)+
  geom_sf(data = terra::spin(fb_net, 90, 279933.5, 4869173), colour = "grey", lwd = 1) +
geom_sf(data =  terra::spin(lcc_fb, 90, 279933.5, 4869173) %>% inner_join(ID_acc_fb, by = "ID"),
        aes(fill = accuracy), size = 4, pch = 21) +
     scale_fill_gradient(low = "white", high = "black")+

  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA)) +
  facet_grid( ~ model)
```


### ZZ
```{r ZZ-map}
#map for ZZ
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
#plot(m1)

#get sensor locations from STIC data, format
locs_zz <- data_23 %>% 
  filter(wshed == "ZZ") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape_zz <- vect(locs_zz, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc_zz <- terra::project(locs_shape_zz, crs(m1))
#plot(lcc)
#define the rectangular area that will be shown on final map
ybounds <- c(4866400,4867500)
xbounds <- c(277200, 277650)
#plot(m1, xlim = xbounds, ylim = ybounds)
#points(lcc)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#plot(crop1)
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./zz_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
zz_crop <- "./zz_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
zz_net <- vect("./carrieZigZag/zigzag_streams.shp")
#plot(zz_net)

###pour point to define where the watershed boundary is
#manually type coords from windows computer


zz_pour_coords <- data.frame("easting" = 277280.45,
                             "northing" = 4867436.45)
#convert to SpatVector object
zz_pour <- vect(zz_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
zz_pour_filename <- "./zz_dems/zz_pour.shp"
#writeVector(zz_pour, zz_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
zz_crop <- "./zz_dems/1mdem_crop.tif"

zz_breached <- "./zz_dems/1mdem_breach.tif"


zz_filled <- "./zz_dems/1mdem_fill.tif"
# wbt_fill_depressions_wang_and_liu(
#   dem = zz_breached,
#   output = zz_filled
# )
#calculate flow accumulation and direction
zz_flowacc <- "./zz_dems/1mdem_zz_flowacc.tif"
# wbt_d8_flow_accumulation(input = zz_filled,
#                          output = zz_flowacc)
#plot(rast(zz_flowacc))
zz_d8pt <- "./zz_dems/1mdem_zz_d8pt.tif"
# wbt_d8_pointer(dem = zz_filled,
#                output = zz_d8pt)
#plot(rast(zz_d8pt))


#delineate streams
zz_streams <- "./zz_dems/zz_streams.tif"
# wbt_extract_streams(flow_accum = zz_flowacc,
#                     output = zz_streams,
#                     threshold = 8000)
# plot(rast(zz_streams))
# points(lcc)
#snap pour point to streams
zz_pour_snap <- "./zz_dems/zz_pour_snap.shp"
# wbt_jenson_snap_pour_points(pour_pts = zz_pour_filename,
#                             streams = zz_streams,
#                             output = zz_pour_snap,
#                             snap_dist = 10)
zz_pour_snap_read <- vect("./zz_dems/zz_pour_snap.shp")
# plot(rast(zz_streams), 
#      xlim = c(280200, 280410),
#      ylim = c(4869300, 4869000))
# points(zz_pour_snap_read, pch = 1)

zz_shed <- "./zz_dems/zz_shed.tif"
# wbt_watershed(d8_pntr = zz_d8pt,
#               pour_pts = zz_pour_snap,
#               output = zz_shed)

#plot(rast(zz_shed))
#convert raster of watershed area to vector for final mapping
zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)
#plot(zz_outline)



#assign destination for hillshade calculation
hillshade_out <- "./zz_dems/1mdem_hillshade.tif"
# wbt_hillshade(
#   dem = zz_crop,
#   output = hillshade_out,
# )
hill <- rast(hillshade_out)
#plot(hill)

centroids(vect(ext(zz_net)))

#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.
#zz_map <- 
  ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2) +
  geom_sf(data = zz_net, colour = "darkslategray3", lwd = 2) +
    geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 2) +
  #geom_sf(data = zz_pour, colour = "black", pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen", limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))

#copy and pasted from W3 chunk
  #summarize through time
# ID_acc_zz <- comparison_zz %>%
#     inner_join(zz_limbs, by = "datetime") %>% 
#   group_by(ID, event_type) %>%
#   summarize(
#     cheap_correct = sum(cheapest_eval == "correct"),
#     pk_correct = sum(pk_eval == "correct"),
#     random_correct = sum(random_eval == "correct"),
#     twi_correct = sum(twi_eval == "correct"),
#     total = n(),
#     cheap_accuracy = cheap_correct / total,
#     pk_accuracy = pk_correct / total,
#     random_accuracy = random_correct / total,
#     twi_accuracy = twi_correct / total
#   ) %>%
#   select(ID, event_type, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
#   tidyr::pivot_longer(cols = c(cheap_accuracy,pk_accuracy, random_accuracy, twi_accuracy), 
#                       names_to = "model", values_to = "accuracy")

#figure with scale bar and legends

#another version, where the points maintain their colors from earlier, but accuracy is illustrated using size or alpha
# ggplot()+
#   theme_void()+
#   geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2)+
#   geom_sf(data = zz_net, colour = "grey", lwd = 1) +
# geom_sf(data = locs_shape %>% inner_join(ID_acc_zz, by = "ID"), 
#         aes(alpha = accuracy, shape = event_type, fill = model), size = 4) +
#   scale_fill_manual(values = four_colors,
#                      name = "")+
#   scale_shape_manual(values = c(21, 24, 22))+
#   theme(rect = element_rect(fill = "transparent", color = NA))+#,
#         #legend.position = "")+
#   facet_grid(event_type ~ model)


## Plot that is just a single row
ID_acc_zz <- comparison_zz %>%
    #inner_join(fb_limbs, by = "datetime") %>% 
  group_by(ID) %>%
  summarize(
    #cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
    total = n(),
    #cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct / total
  ) %>%
  select(ID,  pk_accuracy, random_accuracy, twi_accuracy, new_accuracy) %>%
  tidyr::pivot_longer(cols = c(pk_accuracy, random_accuracy, twi_accuracy, new_accuracy), 
                      names_to = "model", values_to = "accuracy")

ggplot()+
  theme_void()+
  geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2)+
  geom_sf(data = zz_net, colour = "grey", lwd = 1) +
geom_sf(data = locs_shape %>% inner_join(ID_acc_zz, by = "ID"), 
        aes(alpha = accuracy, fill = model), size = 4, pch = 21) +
  scale_fill_manual(values = four_colors,
                     name = "")+
  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid( ~ model)

```
```{r ZZ-one-row}
ggplot()+
  theme_void()+
  geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2)+
  geom_sf(data = zz_net, colour = "grey", lwd = 1) +
geom_sf(data = locs_shape %>% inner_join(ID_acc_zz, by = "ID"), 
        aes(fill = accuracy), size = 4, pch = 21) +
    scale_fill_gradient(low = "white", high = "black")+

  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid( ~ model)

ggplot()+
  theme_void()+
  geom_sf(data = terra::spin(zz_outline, 180, 277401.8, 4867071), fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2)+
  geom_sf(data = terra::spin(zz_net, 180, 277401.8, 4867071), colour = "grey", lwd = 1) +
geom_sf(data =  terra::spin(lcc_zz, 180, 277401.8, 4867071) %>% inner_join(ID_acc_fb, by = "ID"),
        aes(fill = accuracy), size = 4, pch = 21) +
     scale_fill_gradient(low = "white", high = "black")+

  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA)) +
  facet_grid( ~ model)
```
### Combine all
```{r final-combined-plot}
#combine all of the spun plots from before
#W3
W3_rot <- ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 1)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(ID_acc_w3, by = "ID"), 
        aes(fill = accuracy), size = 3, pch = 21) +
  scale_fill_gradientn(colors = c("white","lightgrey","darkgrey", "black"))+
  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position = "none",
        strip.text.x = element_blank())+
  facet_grid( ~ model)
W3_rot
#FB
FB_rot <- ggplot()+
  theme_void()+
  geom_sf(data = terra::spin(fb_outline, 90, 279933.5, 4869173), fill = NA, color = "#397367", alpha = 0.3, lwd = 1)+
  geom_sf(data = terra::spin(fb_net, 90, 279933.5, 4869173), colour = "grey", lwd = 1) +
geom_sf(data =  terra::spin(lcc_fb, 90, 279933.5, 4869173) %>% inner_join(ID_acc_fb, by = "ID"),
        aes(fill = accuracy), size = 3, pch = 21) +
  scale_fill_gradientn(colors = c("white","lightgrey","darkgrey", "black"))+

  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position="none",
        strip.text.x = element_blank()) +
  facet_grid( ~ model)
FB_rot
#zz
ZZ_rot <- ggplot()+
  theme_void()+
  geom_sf(data = terra::spin(zz_outline, 180, 277401.8, 4867071), fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 1)+
  geom_sf(data = terra::spin(zz_net, 180, 277401.8, 4867071), colour = "grey", lwd = 1) +
geom_sf(data =  terra::spin(lcc_zz, 180, 277401.8, 4867071) %>% inner_join(ID_acc_fb, by = "ID"),
        aes(fill = accuracy), size = 3, pch = 21) +
       scale_fill_gradientn(colors = c("white","lightgrey","darkgrey", "black"),
                         limits = c(0,1),
                         labels = c("0", "0.25", "0.5", "0.75", "1"),
                         name = "Accuracy")+

  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position="bottom",
        panel.spacing = unit(5, "lines"),
        strip.text.x = element_blank()) +
  facet_grid( ~ model) 
ZZ_rot
#combine all plots
W3_rot / FB_rot / (ZZ_rot)

#In canva will have to add north arrows and the scale bar
```


## Table 1: Topographic Metrics
Drainage densities, drainage area, elevation range, measured expansion coefficient

drainage density is just stream length (m) divided by area of shed (m^2)
```{r drainage-density}
## W3
#determine total stream length
length_w3 <- sum(perim(w3_net))/1000 #m
#determine watershed area
area_w3 <- expanse(w3_outline)/(1000 * 1000) #m^2

length_w3 / area_w3
#end up with a drainage density a lot larger than what Carrie reported and calculated

## FB

## ZZ
```

## Table 2: Accuracy of models
Create a function that will compare a chain output and it's sequential wetting model to the actual observations.
chunk to calculate accuracy metrics and confusion matrices
```{r figuring-out-accuracy}
# Load required packages
p_load(caret)

# ---- STEP 1: Assume your data is already loaded into `df` ----
# df should have columns: binary, cheapest, pk, random, twi
df <- comparison

# ---- STEP 2: Create confusion matrices ----
conf_cheapest <- table(Predicted = df$cheapest, Actual = df$binary)
conf_pk       <- table(Predicted = df$pk,       Actual = df$binary)
conf_random   <- table(Predicted = df$random,   Actual = df$binary)
conf_twi      <- table(Predicted = df$twi,      Actual = df$binary)

# ---- STEP 3: Define helper function to extract performance metrics ----
get_metrics <- function(pred, true) {
  cm <- confusionMatrix(factor(pred), factor(true), positive = "1")
  data.frame(
    Accuracy = cm$overall["Accuracy"],
    Kappa = cm$overall["Kappa"],
    Sensitivity = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    Precision = cm$byClass["Precision"],
    F1 = cm$byClass["F1"]
  )
}

# ---- STEP 4: Summarize performance for all models ----
metrics_df <- rbind(
  cheapest = get_metrics(df$cheapest, df$binary),
  pk       = get_metrics(df$pk, df$binary),
  random   = get_metrics(df$random, df$binary),
  twi      = get_metrics(df$twi, df$binary)
)

# Add model name as a column
metrics_df <- metrics_df %>% 
  mutate(model = rownames(metrics_df)) %>%
  select(model, everything())

# Print the performance summary
kable(metrics_df, digits = 2)

# ---- STEP 5: Define function to plot confusion matrix ----
plot_conf_mat <- function(cm, title = "Confusion Matrix") {
  cm_df <- as.data.frame(cm)
  ggplot(cm_df, aes(x = Actual, y = Predicted)) +
    geom_tile(aes(fill = Freq), color = "white") +
    geom_text(aes(label = Freq)) +
    scale_fill_gradient(low = "white", high = "steelblue", limits = c(0, 100000)) +
    labs(title = title, fill = "Count") +
    theme_minimal()
}

# ---- STEP 6: Plot confusion matrices for each model ----
cheap_plot <- plot_conf_mat(conf_cheapest, title = "Cheapest") + theme(legend.position = "none")
pk_plot <- plot_conf_mat(conf_pk,       title = "PK")+ theme(legend.position = "none")
random_plot <- plot_conf_mat(conf_random,   title = "Random")+ theme(legend.position = "none")
twi_plot <- plot_conf_mat(conf_twi,      title = "TWI")

(cheap_plot + pk_plot) / (random_plot + twi_plot)
```

Can probably delete the chunk below, and keep one with pred_out instead
```{r produce-accuracy-metrics}
#function that will take a chain solution, and calculate accuracy metrics
produce_metrics <- function(chain, shed, method){
# chain <- ZZ_cheap
# shed <- "ZZ"
# method = "cheapest_insertion"
# calculate the number of flowing nodes at each timestep
  if(shed == "W3"){
    number_activated <-
      input_w3 %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  } 
  else if(shed == "FB"){
    number_activated <-
      input_fb %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  }
  else if(shed == "ZZ"){
    number_activated <-
      input_zz %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  }

#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy

  model_result <- calc_model_result(chain$ID, shed)

#combine all model results
  if(shed == "W3"){
    comparison <- input_w3 %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  } 
  else if(shed == "FB"){
    comparison <- input_fb %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  }
  else if(shed == "ZZ"){
    comparison <- input_zz %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  }


#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
#get_eval_label(0,0)
#apply get_eval function to each model run
comparison$pred_eval <- mapply(get_eval_label, 
                                   comparison$binary, 
                                   comparison$pred_out)

# ---- STEP 3: Define helper function to extract performance metrics ----
get_metrics <- function(pred, true) {
  cm <- confusionMatrix(factor(pred), factor(true), positive = "1")
  data.frame(
    Accuracy = cm$overall["Accuracy"],
    Kappa = cm$overall["Kappa"],
    Sensitivity = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    Precision = cm$byClass["Precision"],
    F1 = cm$byClass["F1"]
  )
}

# ---- STEP 4: Summarize performance for all models ----
metrics_df <- tibble(get_metrics(comparison$pred_out, comparison$binary)) %>% 
  mutate(shed = shed,
         method = method)
#output <- list(metrics = metrics_df, sequence = ZZ_cheap)
#make data long for final plot
return(metrics_df)
}

produce_metrics(ZZ_cheap, "ZZ", "cheapest_insertion")
produce_metrics(W3_cheap, "W3", "cheapest_insertion")

```
```{r accuracy-function-testing-every-method}
methods
#function to determine a chain solution
ZZ_cheap <- chain_solution(ZZ_IDs, "ZZ", methods = "cheapest_insertion")



#testing every method
for(i in 1:length(methods)){
  chain <- chain_solution(ZZ_IDs, "ZZ", methods = methods[i])
  
  output <- produce_metrics(chain, "ZZ", methods[i])
  
  if(i == 1) many_chains <- output
  if(i > 1) many_chains <- rbind(many_chains, output)
}

all_methods_acc <- many_chains
```


```{r accuracy-W3}
w3_acc <- rbind(produce_metrics(W3_cheap, "W3", "cheapest_insertion"),
      produce_metrics(W3_pk_seq, "W3", "pk"),
      produce_metrics(convert_to_IDseq(W3_random), "W3", "random"),
      produce_metrics(w3_twi_sequence, "W3", "twi"),
      produce_metrics(convert_to_IDseq(new_seq_w3), "W3", "new"),
      produce_metrics(convert_to_IDseq(opt_routes_w3), "W3", "con")

      )
kable(w3_acc, digits = 2)

```
```{r accuracy-FB}
fb_acc <- rbind(produce_metrics(FB_cheap, "FB", "cheapest_insertion"),
      produce_metrics(FB_pk_seq, "FB", "pk"),
      produce_metrics(convert_to_IDseq(FB_random), "FB", "random"),
      produce_metrics(fb_twi_sequence, "FB", "twi"),
      produce_metrics(convert_to_IDseq(new_seq_fb), "FB", "new"),
      produce_metrics(convert_to_IDseq(opt_routes_fb), "FB", "con")
      )
kable(fb_acc, digits = 2)

```
```{r accuracy-ZZ}
zz_acc <- rbind(produce_metrics(ZZ_cheap, "ZZ", "cheapest_insertion"),
      produce_metrics(ZZ_pk_seq, "ZZ", "pk"),
      produce_metrics(convert_to_IDseq(ZZ_random), "ZZ", "random"),
      produce_metrics(zz_twi_sequence, "ZZ", "twi"),
      produce_metrics(convert_to_IDseq(new_seq_zz), "ZZ", "new"),
      produce_metrics(convert_to_IDseq(opt_routes_zz), "ZZ", "con")
      )

kable(zz_acc, digits = 2)

kable(rbind(w3_acc, fb_acc, zz_acc), digits = 2)

```

# Unused Figures
## UNUSED: Accuracy of all models versus pk accuracy
```{r W3}
#trying to plot accuracy vs accuracy
df_long2 <- df_summary_time_w3 %>%
  select(datetime, cheap_accuracy, pk_accuracy, twi_accuracy, new_accuracy, con_accuracy) %>%
  tidyr::pivot_longer(cols = c(twi_accuracy, new_accuracy, con_accuracy), 
                      names_to = "model", values_to = "accuracy") %>% 
    inner_join(w3_limbs, by = "datetime") %>% 
  mutate(shed = "W3")

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")

df_long2 %>% 
  ggplot()+
  geom_point(aes(x = pk_accuracy, 
                 y = accuracy, 
                 size = Q_mm_day, color = model),
             alpha = 0.5)+
  scale_color_manual(values = five_colors)+
  geom_abline()+
  theme_minimal()+
  lims(y = c(0, 1),
       x = c(0, 1))+
  facet_grid(~model)
```
```{r FB}
#trying to plot accuracy vs accuracy

#left join

df_long2_fb <- df_summary_time_fb %>%
  select(datetime, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy") %>% 
    inner_join(fb_limbs, by = "datetime")%>% 
  mutate(shed = "FB")

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")

df_long2_fb %>% 
  ggplot()+
  geom_point(aes(x = pk_accuracy, 
                 y = accuracy, 
                 size = roll_mean, shape = event_type, color = model),
             alpha = 0.5)+
  scale_color_manual(values = c("#bb4430", "#231f20", "#f3dfa2"))+
  geom_abline()+
  theme_minimal()+
  lims(y = c(0, 1),
       x = c(0, 1))+
  facet_grid(event_type~model)
```
```{r ZZ}
#trying to plot accuracy vs accuracy

#left join

df_long2_zz <- df_summary_time_zz %>%
  select(datetime, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy") %>% 
    inner_join(zz_limbs, by = "datetime")%>% 
  mutate(shed = "ZZ")

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")

df_long2_zz %>% 
  ggplot()+
  geom_point(aes(x = pk_accuracy, 
                 y = accuracy, 
                 size = roll_mean, shape = event_type, color = model),
             alpha = 0.5)+
  scale_color_manual(values = c("#bb4430", "#231f20", "#f3dfa2"))+
  geom_abline()+
  theme_minimal()+
  lims(y = c(0, 1),
       x = c(0, 1))+
  facet_grid(event_type~model)
```
Combine all of these, since they are not that different... breakup by watershed
```{r}
rbind(
df_long2 %>% select(datetime, pk_accuracy, model, accuracy, event_type, shed),
df_long2_fb %>% select(datetime, pk_accuracy, model, accuracy, event_type, shed),
df_long2_zz %>% select(datetime, pk_accuracy, model, accuracy, event_type, shed)) %>% 
  ggplot()+
  geom_point(aes(x = pk_accuracy, 
                 y = accuracy, shape = event_type, color = shed),
             alpha = 0.5,
             size = 2)+
  scale_color_manual(values = shed_colors)+
  geom_abline()+
  theme_minimal()+
  lims(y = c(0, 1),
       x = c(0, 1))+
  facet_grid(~model)


```


## UNUSED: Tile plot
```{r W3}

range01 <- function(x){(x-min(x))/(max(x)-min(x))}

W3_twi_seq <- w3_twi_sequence %>% 
  mutate(sequence = seq(1, length(ID), 1))

w3_topo_norm <- w3_topo %>% 
  group_by(name) %>% 
  mutate(norm_value = range01(value))

#tile plot, original vision
rbind(W3_cheap %>% mutate(model = "cheapest"),
      W3_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(w3_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  drop_na() %>% 
  ggplot() +
  geom_tile(aes(x = sequence, y = name, fill = norm_value))+
  scale_fill_gradient(low = "white", 
                      high = "black")+
  facet_grid(rows = "model")

#will it look better as a line?
#w3_explain <- 
rbind(W3_cheap %>% mutate(model = "cheapest"),
      W3_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(w3_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  drop_na() %>%
  mutate(shed = "W3") %>% 
  ggplot(aes(x = sequence, y = norm_value, color = name)) +
  geom_smooth(method = 'lm', se = FALSE)+
  geom_point()+
  facet_grid(name~model, scales = "free_x")+
  theme_classic()
```
```{r}
# Perform regressions and get significance
rbind(W3_cheap %>% mutate(model = "cheapest"),
      W3_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(w3_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  as_tibble() %>% 
group_by(model) %>% 
  do(model = lm(sequence ~ norm_value, data = .)) #%>%
  mutate(p_value = summary(model)$coefficients["x", "Pr(>|t|)"]) #%>%
  mutate(significant = p_value < 0.05)

W3_pk_seq %>% mutate(model = "cheapest") %>% 
  left_join(w3_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  as_tibble() %>% 
  filter(name == "uaa") %>% 
  lm(sequence ~ norm_value, data = .) %>% summary()
group_by(name) %>% 
  summarise(summary(lm(sequence ~ norm_value, data = .))$r.squared)
  do(model = lm(sequence ~ norm_value, data = .)) %>% print()
  mutate(p_value = summary(model)$coefficients["x", "Pr(>|t|)"]) #%>%
  mutate(significant = p_value < 0.05)
  
  
  
  
models <- data %>%
  group_by(group) %>%
  do(model = lm(y ~ x, data = .)) %>%
  mutate(p_value = summary(model)$coefficients["x", "Pr(>|t|)"]) %>%
  mutate(significant = p_value < 0.05)

# Merge significance information back to the original data
data_with_significance <- left_join(data, models %>% select(group, significant), by = "group")

# Plotting
ggplot(data_with_significance, aes(x = x, y = y, color = group)) +
  geom_point() +
  geom_smooth(data = . %>% filter(significant), method = "lm", se = FALSE) + # Only plot significant lines
  labs(title = "Trend Lines for Significant Relationships Only",
       subtitle = "Based on p-value < 0.05 for slope",
       x = "X-axis", y = "Y-axis") +
  theme_minimal()
```


```{r FB}

FB_twi_seq <- fb_twi_sequence %>% 
  mutate(sequence = seq(1, length(ID), 1))

fb_topo_norm <- fb_topo %>% 
  group_by(name) %>% 
  mutate(norm_value = range01(value))

#tile plot, original vision
rbind(FB_cheap %>% mutate(model = "cheapest"),
      FB_pk_seq %>% mutate(model = "pk")) %>%
  left_join(fb_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  drop_na() %>% 
  ggplot() +
  geom_tile(aes(x = sequence, y = name, fill = norm_value))+
  scale_fill_gradient(low = "white", 
                      high = "black")+
  facet_grid(rows = "model")

#will it look better as a line?
fb_explain <- rbind(FB_cheap %>% mutate(model = "cheapest"),
      FB_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(fb_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  mutate(shed = "FB") %>% 
  drop_na()
  ggplot() +
  geom_smooth(aes(x = sequence, y = norm_value, color = name))+
  facet_grid(~model, scales = "free_x")+
  theme_classic()
  
rbind(FB_cheap %>% mutate(model = "cheapest"),
      FB_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(w3_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  drop_na() %>%
  mutate(shed = "FB") %>% 
  ggplot(aes(x = sequence, y = norm_value, color = name)) +
  geom_smooth(method = 'lm', se = FALSE)+
  geom_point()+
  facet_grid(name~model, scales = "free_x")+
  theme_classic()
```
```{r ZZ}

ZZ_twi_seq <- zz_twi_sequence %>% 
  mutate(sequence = seq(1, length(ID), 1))

zz_topo_norm <- zz_topo %>% 
  group_by(name) %>% 
  mutate(norm_value = range01(value))

#tile plot, original vision
rbind(ZZ_cheap %>% mutate(model = "cheapest"),
      ZZ_pk_seq %>% mutate(model = "pk")) %>%
  left_join(zz_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  drop_na() %>% 
  ggplot() +
  geom_tile(aes(x = sequence, y = name, fill = norm_value))+
  scale_fill_gradient(low = "white", 
                      high = "black")+
  facet_grid(rows = "model")

#will it look better as a line?
zz_explain <- rbind(ZZ_cheap %>% mutate(model = "cheapest"),
      ZZ_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(zz_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  mutate(shed = "ZZ")
  drop_na() %>% 
  ggplot() +
  geom_smooth(aes(x = sequence, y = norm_value, color = name))+
  facet_grid(~model, scales = "free_x")+
  theme_classic()
```
```{r combine-as-lines}
rbind(w3_explain, fb_explain, zz_explain) %>% 
  ggplot() +
  geom_smooth(aes(x = sequence, y = norm_value, color = name), method = 'lm', se = FALSE)+
  facet_grid(model~shed, scales = "free_x")+
  theme_classic()

#lm(sequence ~ )

```



## UNUSED: Change in Q, versus accuracy
```{r dQ}
#calculate the delta Q
df_long2 %>% 
  mutate(dQ = abs(Q_mm_day - lag(Q_mm_day))) %>% 
  ggplot()+
  geom_point(aes(x = dQ, y = pk_accuracy, color = event_type))

df_long2_fb %>% 
  mutate(dQ = abs(roll_mean - lag(roll_mean))) %>% 
  ggplot()+
  geom_point(aes(x = dQ, y = pk_accuracy, color = event_type))+
  lims(x = c(0,0.75))

df_long2_zz %>% 
  mutate(dQ = abs(roll_mean - lag(roll_mean))) %>% 
  ggplot()+
  geom_point(aes(x = dQ, y = pk_accuracy, color = event_type))+
  lims(x = c(0,0.25))
```
```{r number-of-transitions}

df_long2 %>%
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_w3, by = "datetime") %>% 
  ggplot()+
  geom_point(aes(x = number_of_nodes, y = pk_accuracy, color = state_change),
             size = 2, alpha = 0.5)

df_long2_fb %>% 
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_fb, by = "datetime") %>% 
  ggplot()+
  geom_point(aes(x = number_of_nodes, y = pk_accuracy, color = state_change),
             size = 2, alpha = 0.5)

df_long2_zz %>% 
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_zz, by = "datetime") %>% 
  ggplot()+
  geom_point(aes(x = number_of_nodes, y = pk_accuracy, color = state_change),
             size = 2, alpha = 0.5)

rbind(df_long2 %>%
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_w3, by = "datetime") %>% 
        select(datetime, pk_accuracy, shed, number_of_nodes, state_change),
  df_long2_fb %>% 
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_fb, by = "datetime")%>% 
        select(datetime, pk_accuracy, shed, number_of_nodes, state_change),
  df_long2_zz %>% 
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_zz, by = "datetime") %>% 
        select(datetime, pk_accuracy, shed, number_of_nodes, state_change)) %>% 
  ggplot()+
  geom_point(aes(x = number_of_nodes, y = pk_accuracy, color = state_change),
             size = 2, alpha = 0.5)+
  facet_wrap(~shed)
```
# Testing many random sequences
What if we test 100 different cheapest_insertions?
```{r 10-cheapest-ZZ}
many_cheap <- rep("cheapest_insertion", 1000)
for(i in 1:length(many_cheap)){
  chain <- chain_solution(ZZ_IDs, "ZZ", methods = many_cheap[i])
  
  output <- produce_metrics(chain, "ZZ", many_cheap[i]) %>% 
    mutate(chain = list(chain$ID))
  
  if(i == 1) many_chains <- output
  if(i > 1) many_chains <- rbind(many_chains, output)
}

many_chains
many_chains -> many_chainsZZ

hist(many_chains$Accuracy)

many_cheap <- rep("cheapest_insertion", 3)
for(i in 1:length(many_cheap)){
  chain <- chain_solution(W3_IDs, "W3", methods = many_cheap[i])
  
  output <- produce_metrics(chain, "W3", many_cheap[i])
  
  if(i == 1) many_chains <- output
  if(i > 1) many_chains <- rbind(many_chains, output)
}

many_chains

```

## more random sequence stuff
I have run like 500 random sequences for ZZ, now can I calc some kind of statistic that is how often the same nodes are in the same place in the sequence? Or maybe how often they are in the sequence in relation to another?

```{r}
tibble("seqs" = unlist(many_chainsZZ$chain[1:500]),
               "pos" = rep(seq(1, 24, 1), 500)) %>% 
  group_by(seqs) %>% 
  summarize(mean_pos = mean(pos),
            sd = sd(pos),
            min = min(pos),
            max = max(pos),
            range = max(pos) - min(pos)) %>% 
  arrange(mean_pos) %>% View()

seqq <- tibble("seqs" = unlist(many_chainsZZ$chain[1:500]),
               "pos" = rep(seq(1, 24, 1), 500)) %>% 
  group_by(seqs) %>% 
  summarize(mean_pos = mean(pos)) %>% 
  arrange(mean_pos)

average_sequence <- seqq$seqs

routes_zz_test <- tibble("up" = average_sequence,
                    "down" = lag(average_sequence)) %>% drop_na() %>% 
  select(up, down)

test <- calc_props(routes_zz_test, "ZZ")
```


# Bonus Figure: comparing wetting versus drying
```{r differentiated-states-functions}
#chunk to trouble shoot non-functioning instances to figure out why they are not working
#instance not working:
#run_scenario(routes_w3_pk, "pk", "W3", "daily")

routes <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

input <- rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
        filter(wshed == "W3", mins %in% c(0, 30)) %>%
      #filter(wshed == "W3", hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary) #%>% 
  #filter(datetime < stop24 & datetime > start24)


all_transitions <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  output$wetting <- NA
  output$drying <- NA
  return(output)
}
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
#if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")
sequence_df$t1_up <- as.numeric(substr(sequence_df$windows, 1, 1))
sequence_df$t1_down <- as.numeric(substr(sequence_df$windows, 2, 2))
sequence_df$t2_up <- as.numeric(substr(sequence_df$windows, 3, 3))
sequence_df$t2_down <- as.numeric(substr(sequence_df$windows, 4, 4))

sequence_df$sum_t1 <- sequence_df$t1_up + sequence_df$t1_down
sequence_df$sum_t2 <- sequence_df$t2_up + sequence_df$t2_down
sequence_df$direction <- "drying"
sequence_df$direction[sequence_df$sum_t1 < sequence_df$sum_t2] <- "wetting"
#new fixed code, should not drop non-hierarchical values
total_state_changes <- sequence_df %>% group_by(direction) %>% summarise(totals = sum(Freq))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- sequence_df %>% 
  filter(windows %in% supports) %>% group_by(direction) %>% summarise(hierarchical = sum(Freq)) 

sub <- total_state_changes %>% left_join(hierarchical_changes, by = c("direction")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))

#old code

# total_state_changes <- sequence_df %>% group_by(direction) %>% summarise(totals = sum(Freq))
# 
# supports <- c("0001","0111","1101", "0100")
# sub <- filter(sequence_df, windows %in% supports) %>%
#   group_by(direction) %>% summarise(hierarchicals = sum(Freq)) %>% 
#   left_join(total_state_changes, by = "direction") %>% 
#   mutate(prop = hierarchicals/totals)
#output$points <- sum(sub$Frequency)
output$drying <- sub$prop[1]
output$wetting <- sub$prop[2]
#write some way to score the sequence_df
#award one point for one of these configs:


#sub <- filter(sequence_df, Sequence %in% supports)

#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(output)}
}

#test function
all_transitions("r_23", "r_6", input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_wd <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- all_transitions(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

iterate_groups_wd("r_23", "r_6", input, minutes(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_wd <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- iterate_groups_wd(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    # group_by(up, down) %>%
    # summarise(total = sum(total),
    #           points = sum(points)) %>% 
    # mutate(prop = points/total)
  return(final_output)
}

#calculate_hierarchy_wd(routes, input, minutes(30))
#calculate_hierarchy(routes, input, days(1))


#make a function to loop through the four possible timesteps, and combine the output just for ease of applying this many different variations
#fantastic 4 determines the input on its own
fantastic_four_wd <- function(routes, shed){
  theFour <- c("30mins", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_wd(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}


fantastic_four_wd(routes_w3, "W3")

#run calc_support for all sheds and timesteps for relative position
all_position_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                      fantastic_four_wd(routes_fb, "FB"),
                      fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Proportion of Time Flowing")

sults_so_far <- all_position_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying)) %>% 
  pivot_longer(cols = starts_with("avg"),
               names_to = "direction",
               values_to = "prop")

sults_so_far$direction[sults_so_far$direction == "avg_wet_prop"] <- "wetting"
sults_so_far$direction[sults_so_far$direction == "avg_dry_prop"] <- "drying"

#plot
sults_so_far %>% 
  #filter(timescale %in% c(possible_times[t])) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = direction, color = direction), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  labs(
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = c("#FFA400", "#93C2F1"),name = "Direction")+
  scale_color_manual(values = c("#FFA400", "#93C2F1"), name = "Direction")+
  ylim(c(0, 4))+
  xlim(c(0,1))+
  facet_grid(shed~timescale)


sults_so_far %>% 
  filter(shed == "W3", timescale == "30mins") %>% 
  group_by(direction) %>% 
  summarise(mean(prop))

#for each scenario create the routes, inputs, and specify the timestep

calc_props <- function(routes, shed){
  full_combos <- fantastic_four_wd(routes, shed)
total_state_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    group_by(up, down, timescale, shed) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down, timescale, shed) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down", "shed", "timescale")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
return(un_split)
}
```

```{r}

routes <- W3_cheap %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)


all_tsp_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                      fantastic_four_wd(routes_fb, "FB"),
                      fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "TSP")

sults_so_far_tsp <- all_tsp_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying)) %>% 
  pivot_longer(cols = starts_with("avg"),
               names_to = "direction",
               values_to = "prop")

sults_so_far_tsp$direction[sults_so_far_tsp$direction == "avg_wet_prop"] <- "wetting"
sults_so_far_tsp$direction[sults_so_far_tsp$direction == "avg_dry_prop"] <- "drying"

#plot
sults_so_far_tsp %>% 
  #filter(timescale %in% c(possible_times[t])) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = direction, color = direction), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  labs(
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = c("#FFA400", "#93C2F1"),name = "Direction")+
  scale_color_manual(values = c("#FFA400", "#93C2F1"), name = "Direction")+
  ylim(c(0, 4))+
  xlim(c(0,1))+
  facet_grid(shed~timescale)
```


```{r proportion-of-time-flowing-analysis}
routes_w3 <- pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- pks_fb %>%
    filter(ID %in% FB_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- pks_zz %>%
    filter(ID %in% ZZ_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

# actually determining how often the nodes follow proportion of time flowing sequence
all_pk <- rbind(calc_props(routes_w3, "W3"),
                calc_props(routes_fb, "FB"),
                calc_props(routes_zz, "ZZ")) %>% 
  mutate("method" = "Flow Permanence")
```


Exploring more ways to try and find a sequence
```{r}
# Example data: binary matrix (time x nodes)
set.seed(123)
X <- matrix(rbinom(1000, 1, 0.3), ncol = 5)  # 1000 timesteps, 5 nodes
colnames(X) <- paste0("Node", 1:5)

# Conditional probability: P(Node j = 1 at t+1 | Node i = 1 at t)
cond_prob <- function(i, j, lag = 1) {
  xi <- X[1:(nrow(X)-lag), i]
  xj <- X[(1+lag):nrow(X), j]
  mean(xj[xi == 1])
}

cond_prob(1, 2)  # P(Node2(t+1) = 1 | Node1(t) = 1)

```
```{r}
#calculating cross correlation
ccf(X[,1], X[,2], lag.max = 10, main = "Cross-correlation Node1 vs Node2")

```
```{r}
p_load(arulesSequences)

# Convert to transactions: each time step = sequence of active nodes
events <- apply(X, 1, function(row) which(row == 1))
seqdata <- do.call(rbind, lapply(seq_along(events), function(t) {
  data.frame(
    sequenceID = t,
    eventID = 1,
    size = length(events[[t]]),
    items = paste0("Node", events[[t]])
  )
}))

seqs <- as(seqdata, "transactions")
rules <- cspade(seqs, parameter = list(support = 0.01, maxlen = 3))
inspect(rules[1:10])

```
```{r}
p_load(depmixS4)

# Fit HMM with 2 hidden states to one nodes series
mod <- depmix(response = Node1 ~ 1, data = data.frame(Node1 = X[,1]), nstates = 2, family = binomial())
fit <- fit(mod)
summary(fit)

```

```{r}
data_mat <- 
input_w3 %>% 
  dplyr::select(datetime, ID, binary) %>% 
  mutate(binary = as.numeric(binary)) %>% 
  pivot_wider(names_from = ID, values_from = binary) %>% 
  dplyr::select(-datetime) %>% 
  drop_na() %>% 
  data.matrix()

ccf(data_mat[,2], data_mat[,5], lag.max = 1500, main = "Cross-correlation Node1 vs Node2")

```

Train an LTSM
```{r}
p_load(keras)

# Install TensorFlow backend if you dont have it already
keras::install_keras() 


set.seed(123)
# Example data: 1000 timesteps, 5 nodes
X <- matrix(rbinom(5000, 1, 0.3), ncol = 5)

timesteps <- nrow(X)
nodes <- ncol(X)

# Parameters
seq_len <- 10  # length of input sequence
n_samples <- timesteps - seq_len

```

