---
title: "ComparingSequences"
format: html
editor_options: 
  chunk_output_type: console
---

5/27/25

New clean script for analysis determining ideal sequences for each watershed, and comparing them to sequences that are random, and dictated by proportion of time flowing, and topography.  

Last markdown document was almost 10,000 lines, and getting unwieldy to navigate.  

Explanation of graph theory stuff: finding the best path using the traveling salesman's problem. Usually this provides a cycle, but by making a dummy node at the end and then removing it we can trick the algorithm to produce a very efficient 1-way path.  

Not sure if I should include the dummy or not; FB is a lot better with a dummy, but W3 and ZZ are a lot better without dummies at the end.

```{r setup}
#loading packages
library(pacman)
p_load(tidyverse, terra, tidyterra, whitebox, scales, wesanderson, caret, plotly,ggnewscale, sf, rgeoboundaries, elevatr, patchwork, ggspatial, zoo, igraph, TSP, ggnetwork, intergraph)
#whitebox::install_whitebox()

#reading in final format data for summer 23
data_23 <- read_csv("./DataForMary/HB_stic.csv")
#reading in final format data for summer 24
data_24 <- read_csv("./summer2024/STICS2024.csv")

data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

bind23 <- data_23 %>% 
  select(datetime, ID, wshed, binary, mins)
bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)
```
#Preparing inputs
```{r prepare-inputs}
#create input that only uses sensors that were deployed during both deployments
#need to make a list of sensors deployed in both campaigns for each watershed
#W3
w3_deployed24 <- unique(filter(data_24, wshed == "W3")$number)
w3_deployed23 <- unique(filter(data_23, wshed == "W3")$ID)
W3_IDs <- intersect(w3_deployed24, w3_deployed23)
#FB
fb_deployed24 <- unique(filter(data_24, wshed == "FB")$number)
fb_deployed23 <- unique(filter(data_23, wshed == "FB")$ID)
FB_IDs <- intersect(fb_deployed24, fb_deployed23)
#ZZ
zz_deployed24 <- unique(filter(data_24, wshed == "ZZ")$number)
zz_deployed23 <- unique(filter(data_23, wshed == "ZZ")$ID)
ZZ_IDs <- intersect(zz_deployed24, zz_deployed23)


#detach("package:fitdistrplus")
#detach("package:MASS")


bind24 <- data_24 %>% 
  select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)%>% 
  filter(mins %in% c(0, 30))


bind23 <- data_23 %>% 
  filter(ID %in% intersect(bind24$ID, data_23$ID)) %>% 
  select(datetime, ID, wshed, binary, mins) %>% 
  filter(mins %in% c(0, 30))

#filter by each watershed, then recombine at the end
input_w3 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3") %>% 
  filter(ID %in% W3_IDs)
input_fb <- rbind(bind23, bind24) %>%
  filter(wshed == "FB") %>% 
  filter(ID %in% FB_IDs)
input_zz <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ") %>% 
  filter(ID %in% ZZ_IDs)

input_all <- rbind(input_w3, input_fb, input_zz)
write_csv(input_w3, "calc_support_inputs_w3.csv")
write_csv(input_fb, "calc_support_inputs_fb.csv")
write_csv(input_zz, "calc_support_inputs_zz.csv")

      
```

```{r define-functions}
calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  sequence_df <- data.frame("Sequence" = NA, 
                            "Frequency" = NA,
                            "up" = up,
                            "down" = down)
  return(sequence_df)
} 
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")


sub <- filter(sequence_df, Sequence %in% supports)
output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)}
}

#test function

#calc_support_combos("r_18", "r_11", input_test)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_combos <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support_combos(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

#iterate_groups("r_13", "r_19", input, min(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_combos <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- iterate_groups_combos(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down, Sequence) %>%
    summarise(Frequency = sum(Frequency))
  return(final_output)
}

fantastic_four_combos <- function(routes, shed){
  theFour <- c("30mins", "hourly", "4hr", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "hourly"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(1)
  } 
  else if(timescale == "4hr"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(0,4,8,12,16,20,24), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- hours(4)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_combos(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}

calc_props <- function(routes, shed){
  full_combos <- fantastic_four_combos(routes, shed)
total_state_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    group_by(up, down, timescale, shed) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down, timescale, shed) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down", "shed", "timescale")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
return(un_split)
}
```

#Set up for analysis
```{r W3}
input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- W3_IDs

#create empty list to hold repeated node IDs
zzz <- length(combos)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}
#create data frame with all possible combinations
W3_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#Run suite of functions to determine proportion of time that state changes follow the parent -> child relationship
W3_all_combos <- calc_props(W3_combos_routes, "W3")
```

```{r FB}
#set routes for all 3 watersheds
input <- rbind(bind23, bind24) %>%
  filter(wshed == "FB", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- FB_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)

#run calc_support for all sheds and timesteps for relative position
FB_all_combos <- calc_props(all_combos_routes, "FB")
```

```{r ZZ}
#set routes for all 3 watersheds

input <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
ZZ_all_combos <- calc_props(all_combos_routes, "ZZ")
```

For each watershed, find the best chain using 
```{r W3}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

# methods <- c("random",
#   "cheapest_insertion")

#set up edges and distance matrix, convert to TSP object
edges_W3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()

keyz <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))

# Convert to TSP object

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  atsp <- as.ATSP(edges_W3)

    atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], two_opt = TRUE)
    #tour <- solve_TSP(tour, method = "two_opt")

  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "W3") %>% 
    mutate("method" = methods[i])

    if(i == 1) all_chain_outputs <- chain_output
    if(i > 1) all_chain_outputs <- rbind(all_chain_outputs, chain_output)
}

all_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

```

```{r FB-analysis}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

#methods <- c("random", "cheapest_insertion")
keyz <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edges_FB <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  atsp <- as.ATSP(edges_FB)
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i])#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "FB") %>% 
    mutate("method" = methods[i])

    if(i == 1) FB_chain_outputs <- chain_output
    if(i > 1) FB_chain_outputs <- rbind(FB_chain_outputs, chain_output)
}


#plots for committee meeting
FB_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

```

```{r ZZ-analysis}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

#methods <- c("random", "cheapest_insertion")
keyz <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edges_ZZ <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edges_ZZ)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(ZZ_IDs) + 1)#, start = 25)#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "ZZ") %>% 
    mutate("method" = methods[i])

    if(i == 1) ZZ_chain_outputs <- chain_output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, chain_output)
}


#plots for committee meeting
ZZ_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)


atsp <- as.ATSP(edges_ZZ)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[4], start = length(ZZ_IDs) + 1)#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  #path <- as.integer(tour)
  path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])



# Create a chain graph from the path
edges <- cbind(path[-length(path)], path[-1])
chain_graph <- graph_from_edgelist(edges, directed = TRUE)
plot(chain_graph,
       layout = layout_nicely(chain_graph),
     edge.arrow.size = 0.25,
  vertex.size = 10,
  vertex.label.cex = 0.75)

ggnet <- fortify(chain_graph, layout = igraph::layout_nicely(chain_graph))
ggplot(ggnet, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 8) +
  # geom_nodetext(aes(label =LETTERS[ path ]),
  #               fontface = "bold", color = "white", size = 3) +
  theme_blank()
```

For final plots for tomorrow, keep arbitrary insertion and cheapest insertion
```{r generalized-function}
#define all possible methods
#removed 3 methods
methods = c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")
#methods <- c("random", "cheapest_insertion")

general_graph <- function(combos, shed, methods = c("random", "cheapest_insertion")){
#combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
allcombosIn <- calc_props(all_combos_routes, shed)

#methods <- c("random", "cheapest_insertion")
keyz <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edgesIn <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edgesIn)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(combos) + 1, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, shed) %>% 
    mutate("method" = methods[i])

    if(i == 1) ZZ_chain_outputs <- chain_output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, chain_output)
}
return(ZZ_chain_outputs)
}

ZZ_graph_solution <- general_graph(ZZ_IDs, "ZZ")

ZZ_graph_solution %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)
```

