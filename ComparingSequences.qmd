---
title: "ComparingSequences"
format: html
editor_options: 
  chunk_output_type: console
---

5/27/25
New clean script for analysis determining ideal sequences for each watershed, and comparing them to sequences that are random, and dictated by proportion of time flowing, and topography.  

Last markdown document was almost 10,000 lines, and getting unwieldy to navigate.  

Explanation of graph theory stuff: finding the best path using the traveling salesman's problem. Usually this provides a cycle, but by making a dummy node at the end and then removing it we can trick the algorithm to produce a very efficient 1-way path.  

Not sure if I should include the dummy or not; FB is a lot better with a dummy, but W3 and ZZ are a lot better without dummies at the end.

6/30/25
Formatted, should contain everything needed for current analysis. Have not checked for dependencies in other scripts. 

# Setup and Preparing inputs
```{r setup}
#loading packages
library(pacman)
p_load(tidyverse, terra, tidyterra, whitebox, scales, wesanderson, caret, plotly,ggnewscale, sf, elevatr, patchwork, ggspatial, zoo, igraph, TSP, ggnetwork, knitr, #intergraph, ggpubr, #purrr, spatialEco, 
       hydroEvents,ggbreak,
       maptools)
#whitebox::install_whitebox()


#p_load(conflicted)
#    conflict_prefer("select", "dplyr")
#        conflict_prefer("filter", "dplyr")



#reading in final format data for summer 23
data_23 <- read_csv("./DataForMary/HB_stic.csv")
#reading in final format data for summer 24
data_24 <- read_csv("./summer2024/STICS2024.csv")

data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

bind23 <- data_23 %>% 
  select(datetime, ID, wshed, binary, mins)
bind24 <- data_24 %>% 
  dplyr::select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)
```

Preparing the STIC dataset as inputs for everything I am doing.  
```{r prepare-inputs}
#create input that only uses sensors that were deployed during both deployments
#need to make a list of sensors deployed in both campaigns for each watershed
#W3
w3_deployed24 <- unique(filter(data_24, wshed == "W3")$number)
w3_deployed23 <- unique(filter(data_23, wshed == "W3")$ID)
W3_IDs <- intersect(w3_deployed24, w3_deployed23)
#FB
fb_deployed24 <- unique(filter(data_24, wshed == "FB")$number)
fb_deployed23 <- unique(filter(data_23, wshed == "FB")$ID)
FB_IDs <- intersect(fb_deployed24, fb_deployed23)
#ZZ
zz_deployed24 <- unique(filter(data_24, wshed == "ZZ")$number)
zz_deployed23 <- unique(filter(data_23, wshed == "ZZ")$ID)
ZZ_IDs <- intersect(zz_deployed24, zz_deployed23)


#detach("package:fitdistrplus")
#detach("package:MASS")


bind24 <- data_24 %>% 
  dplyr::select(datetime, number, wshed, binary, mins) %>% 
  rename("ID" = number)%>% 
  filter(mins %in% c(0, 30))


bind23 <- data_23 %>% 
  filter(ID %in% intersect(bind24$ID, data_23$ID)) %>% 
  dplyr::select(datetime, ID, wshed, binary, mins) %>% 
  filter(mins %in% c(0, 30))

#filter by each watershed, then recombine at the end
input_w3 <- rbind(bind23, bind24) %>%
  filter(wshed == "W3") %>% 
  filter(ID %in% W3_IDs) %>% 
  drop_na()
input_fb <- rbind(bind23, bind24) %>%
  filter(wshed == "FB") %>% 
  filter(ID %in% FB_IDs)
input_zz <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ") %>% 
  filter(ID %in% ZZ_IDs)

input_all <- rbind(input_w3, input_fb, input_zz)
write_csv(input_w3, "calc_support_inputs_w3.csv")
write_csv(input_fb, "calc_support_inputs_fb.csv")
write_csv(input_zz, "calc_support_inputs_zz.csv")

      
```

Define functions that calculate proportion of time that nodes wet and dry sequentially.  
```{r define-functions}
calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  sequence_df <- data.frame("Sequence" = NA, 
                            "Frequency" = NA,
                            "up" = up,
                            "down" = down)
  return(sequence_df)
} 
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
supports <- c("0001","0111","1101", "0100")


sub <- filter(sequence_df, Sequence %in% supports)
output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)}
}

#test function

#calc_support_combos("r_18", "r_11", input_test)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_combos <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support_combos(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

#iterate_groups("r_13", "r_19", input, min(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_combos <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- iterate_groups_combos(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down, Sequence) %>%
    summarise(Frequency = sum(Frequency))
  return(final_output)
}

#fantastic four function has been modified, to do average daily state. Removed hourly and 4 hr time blocks
fantastic_four_combos <- function(routes, shed){
  theFour <- c("30mins", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed) %>% 
      mutate(
             "day" = day(datetime),
             "month" = month(datetime),
             "year" = year(datetime)) %>% 
      group_by(day, month, year, ID) %>%
      summarise(avg_state = mean(binary)) %>% 
      ungroup() %>% 
      mutate(avg_state = round(avg_state)) %>% 
      rename("binary" = avg_state) %>% 
      mutate("datetime" = ymd_hms(paste0(year,"-",month,"-",day," ","00:00:00"))) %>% 
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
          arrange(datetime) %>% 
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_combos(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}

# function to determine proportion of transitions that state changes follow all parent child relationships
calc_props <- function(routes, shed){
  full_combos <- fantastic_four_combos(routes, shed)
total_state_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    group_by(up, down, timescale, shed) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down, timescale, shed) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down", "shed", "timescale")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
return(un_split)
}
```
Make them ID/sequence format
```{r helper-functions}
#write wrapper functions to convert between the two forms easily
#routes_w3 <- 
  pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down) %>% 
  convert_to_IDseq()

#convert from up/down to ID/seq
convert_to_IDseq <- function(input){ 
  #expecting input that is a df of up/down structure

  #make list in order of parent-child sequence
  IDs <- c(input$down[1], input$up)
  #make sequence ID column
  sequence <- seq(1, length(input$up) + 1, 1)
 
  convert <- tibble(ID = IDs,
           sequence = sequence)
  
return(convert)
}
  

#convert from ID/seq to up/down
input <- W3_cheap

  input %>% 
    mutate(up = ID,
         down = lag(up)) %>% 
  select(up, down) %>% 
  drop_na()
  
convert_to_updown <- function(input){
  #up is child and down is parent
  convert <- 
  input %>% 
    mutate(up = ID,
         down = lag(up)) %>% 
  select(up, down) %>% 
  drop_na()
  
  return(convert)
}
```

## Discharge and Stage
### W3 Discharge
```{r prepare-discharge-W3}
#from TestingFrameworks script

#read in discharge from W3-- input to Carrie's model, discharge in L/s
#q <- read_csv("https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-hbr.1.17&entityid=efc477b3ef1bb3dd8b9355c9115cd849")
#write.csv(q, "HB_5minQ.csv")
q <- read_csv("HB_5minQ.csv")

#input discharge needs to be in mm/day?
#reference to understand difference between daily mean and instantaneous streamflow:
#https://hydrofunctions.readthedocs.io/en/master/notebooks/DailyMean_vs_Instant.html

#creating minute column, used to filter out higher temporal resolution measurements for plotting
data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

#find the range of dates that I need discharge for
start <- min(data_23$datetime)
stop <- max(data_23$datetime)

#filtering discharge down to the range of dates
q_23 <- q %>% 
  filter(DATETIME > start & DATETIME < stop) %>% 
  #convert to mm/day.
  #converting instantaneous streamflow to mm/day by taking measurement, and scaling   it up as if that was the discharge for the whole day. It is not, it is just at that   moment, but should fix any units/order of magnitude issues
  mutate("Q_mm_day" = Discharge_ls * 0.001 * 86400 / 420000 * 1000) 
q_23$mins <- minute(q_23$DATETIME)

#removing times that are not coincident with STIC observations
q_23_f <- filter(q_23, mins %in% c(0, 30))

ggplot(q_23_f, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#also read in provisional 2024 data
q_24 <- read_csv("w3_discharge_24.csv")

#find the range of dates that I need discharge for
start24 <- ymd_hms("2024-05-15 00:00:00 UTC")
stop24 <- max(data_24$datetime)

#filtering discharge down to the range of dates
q_24 <- q_24 %>% 
  mutate(datetime = mdy_hm(datetime)) %>% 
  filter(datetime > start24 & datetime < stop24)  
q_24$mins <- minute(q_24$datetime)

#removing times that are not coincident with STIC observations
q_24_f <- filter(q_24, mins %in% c(0, 30))

ggplot(q_24_f, aes(x  = datetime, y = Q_mmperday))+
  geom_line()+
  labs(title = "Discharge from W3, May to July 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#get discharge ready to bind to my other data
q_23_bind <- 
  q_23_f %>% 
  select(DATETIME, Q_mm_day) %>% 
  rename("datetime" = DATETIME
         )

q_24_bind <- 
  q_24_f %>% 
  select(datetime, Q_mmperday) %>% 
  rename("Q_mm_day" = Q_mmperday)
```
  
Chunk straight from mapsForStoryboard, of my current best attempt to extract rising and falling limbs from discharge record for W3. Plan to use to compare how the different methods work during rising, falling, and baseflow.
```{r identifying-events}
start <- ymd_hms("2023-7-20 00:00:00")
stop <- ymd_hms("2023-10-22 00:00:00")


q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#calculate baseflow
#old values greyed
#bf = baseflowB(q_23_plotting$Q_mm_day, alpha = 0.980)
bf = baseflowB(q_23_plotting$Q_mm_day, alpha = 0.925)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 0.5, min.diff = 30)
#plot the events
plotEvents(data = q_23_plotting$Q_mm_day, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = q_23_plotting$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm
PoT_res$srt

q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  arrange(datetime) %>% 
  unique() %>% 
  mutate(lab = as.numeric(row_number())) %>% 
  mutate(group = data.table::rleid(lab, cols = PoT_res$srt))

ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))

ranges[26,2] <- 5762
#trying fuzzy join method?
p_load(fuzzyjoin)
id_events <- q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  mutate(ID = row_number()) |> fuzzy_left_join(ranges, by = c(ID = "starts", ID = "stops"),
                              match_fun = list(`>=`, `<=`)) |> 
   mutate(event_ID = event_ID) |> 
   select(-starts, -stops)

id_events %>% 
  ggplot()+
  geom_point(aes(x = datetime, y = Q_mm_day, color = event_ID))

id_events <- q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  mutate(ID = row_number()) |> fuzzy_left_join(ranges, by = c(ID = "starts", ID = "stops"),
                              match_fun = list(`>=`, `<=`)) |> 
   mutate(event_ID = event_ID) |> 
   select(-starts, -stops)

id_events %>% 
  ggplot()+
  geom_point(aes(x = datetime, y = Q_mm_day, color = event_ID))


```

```{r W3-2023}
# Example: discharge dataframe with datetime and discharge
discharge_df <- q_23_plotting %>% 
  select(datetime, Q_mm_day)

# Copy of your event table
events <- limbs(data = q_23_plotting$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

#save output for 2023
discharge_df_23 <- discharge_df


discharge_df %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_type))+
  geom_point()

discharge_df %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_id))+
  geom_point()

```
```{r W3-2024}
#identifying events
bf = baseflowB(q_24_bind$Q_mm_day, alpha = 0.925)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(q_24_bind$Q_mm_day - bf$bf, threshold = 0.25, min.diff = 30)
#plot the events
plotEvents(data = q_24_bind$Q_mm_day, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = q_24_bind$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- q_24_bind

# Copy of your event table
events <- limbs(data = q_24_bind$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

discharge_df_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_type))+
  geom_point()

w3_limbs <- rbind(discharge_df_24, discharge_df_23)

```

Doing a thing where I define a sequence based on the activation order during events
```{r}
events

discharge_df <- q_23_plotting %>% 
  select(datetime, Q_mm_day)

discharge_df$event_id <- NA_integer_

for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:events[i+1, ]$ris.srt] <- i  # Event ID for rising+falling
}

discharge_df %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = as.character(event_id)))+
  geom_point()



#now, take this new dataframe and for each event find the start of flow
ttt <- 
input_w3 %>%
  inner_join(discharge_df,  by = c("datetime")) %>% 
    filter(binary == 1) %>% 
    group_by(ID, event_id) %>% 
    summarise(start_of_flow = min(datetime)) %>% 
    arrange(start_of_flow)

#write a for loop to go through event ids, calculate the sequence for each event, and then find an "average" sequence
for(i in 1:length(unique(ttt$event_id))){
  event <- unique(ttt$event_id)[i]
  
  sequenced_event <- filter(ttt, event_id == event) %>% 
    arrange(start_of_flow) %>% 
      rowid_to_column("Sequence")
  
  if(i == 1) all_events <- sequenced_event
  if(i > 1) all_events <- rbind(all_events, sequenced_event)
}
  mutate(Sequence = )
  group_by(ID) %>% 
  summarise(mean(Sequence))
```


Combine the model results above, and the identified events
```{r}
comparison %>% 
  drop_na() %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  group_by(event_type, code) %>% 
  summarise(mean = mean(count)) %>% 
  ggplot()+
  geom_bar(aes(x = event_type, y = mean, fill = code), stat = "identity")

comparison %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  filter(code == "ommission") %>% 
  ggplot()+
  geom_line(aes(x = datetime, y = Q_mm_day, color = count))

#for the graph theory solution, the commission and ommission are symmetrical most of the time... I guess this makes sense if one is out of order, it causes another to be in order, like when I did the lagging method to calculate this
comparison %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  pivot_wider(names_from = code, values_from = count) %>% 
  mutate(diff = commission - ommission) %>% 
  filter(diff != 0) %>% View()


```
### Stage for FB and ZZ
Process salt dilutions
```{r process-dilutions}
source("calcQ.R")
q_combined <- rbind(
calcQ("./salt_dilutions/FB/FB_8_1") %>% mutate("shed" = "FB"),
#calcQ("./salt_dilutions/FB/FB_6_23_fail") %>% mutate("shed" = "FB"),
calcQ("./salt_dilutions/ZZ/ZZ_8_1")%>% mutate("shed" = "ZZ"),
calcQ("./salt_dilutions/ZZ/ZZ_7_1")%>% mutate("shed" = "ZZ"),
#calcQ("./salt_dilutions/ZZ/ZZ_6_23") %>% mutate("shed" = "ZZ"),#different timestep, after fixing gave negative number
calcQ("./salt_dilutions/ZZ/ZZ_6_21")%>% mutate("shed" = "ZZ"),

#no k
calcQ_noK("./salt_dilutions/FB/FB_7_1", mean(c(0.0001120716, 2.621858e-06))) %>% mutate("shed" = "FB"),
calcQ_noK("./salt_dilutions/FB/FB_6_23_Success", mean(c(0.0001120716, 2.621858e-06))) %>% mutate("shed" = "FB"),

#missing FB 8_5, no k and no upstream logger
# ZZ_8_5, no k and no upstream logger
calcQ_noK_oneL("./salt_dilutions/FB/FB_8_5", mean(c(0.0001120716, 2.621858e-06))) %>% mutate("shed" = "FB"),
calcQ_noK_oneL("./salt_dilutions/ZZ/ZZ_8_5", mean(c(9.453437e-05, 4.017994e-06,
                                                    3.962093e-05, 9.693616e-05)))%>% mutate("shed" = "ZZ")
)
```

```{r FB-2023}
FB_air <- read_csv("./PressureTransducers_11_14_23/FB_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water <- read_csv("./PressureTransducers_11_14_23/FB_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  ggplot(aes(x = DATETIME, y = stage_cm))+
  geom_line()+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")

#exclude windows where they were not deployed
FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(stage_cm < 5)

#7/22/2023 00:00:00
#9/20/23 00 - 2023-09-22 00:00:00
#2023-11-12 00:30:00

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>%
  ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")

processed_fb_stage_23 <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  drop_na() %>%  mutate(Q = roll_mean * fb_stage_Q_model$coefficients[2] +  fb_stage_Q_model$coefficients[1])


processed_fb_stage %>% 
  ggplot(aes(x = DATETIME, y = Q))+
  geom_line()+
  theme_classic()

#output processed salt dilutions
# q_combined %>% filter(shed == "FB") %>% 
#   mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
#   rename(DATETIME = datetime) %>% 
#   left_join(processed_fb_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_fb_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_fb_stage$roll_mean - bf$bf, threshold = 0.4, min.diff = 20)
#plot the events
plotEvents(data = processed_fb_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_fb_stage

# Copy of your event table
events <- limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_df_23 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()


```
```{r FB-2024}
#Read in measurements from 2024

fb_air_25 <-
  rbind(read_csv("./pducers_summer25/fb_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)))

FB_air <- read_csv("./PressureTransducers_summer24/FB_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)) %>% 
  rbind(fb_air_25)

fb_water_25 <- 
  read_csv("./pducers_summer25/fb_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))


FB_water <- read_csv("./PressureTransducers_summer24/FB_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)) %>% 
  rbind(fb_water_25)

processed_fb_stage <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    #filter(DATETIME <= ymd_hms("2024-08-06 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()

#View(processed_fb_stage)

processed_fb_stage %>% 
  filter(DATETIME == ("2024-08-01 11:00:00"))

# processed salt dilutions
dilutions <- q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")
dilutions

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701,
         #apply a rolling mean to get rid of temperature artifacts
         roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+
  
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")+
  geom_point(data = dilutions,
             aes(x = DATETIME, y = stage_cm),
             color = "blue")
dilutions %>% 
  ggplot(aes(x = roll_mean, y = discharge))+
  geom_smooth(method = 'lm', se = FALSE, color = "grey")+
  geom_point()+
  labs(x = "Stage (cm)", y = "Discharge (L/s)",
       title = "FB Stage/Q")+
  #geom_text(aes(label = DATETIME), nudge_x = 1)+
  theme_classic()

fb_stage_Q_model <- lm(dilutions$discharge ~ dilutions$roll_mean)
#try exponential model
model_exp <- lm(log(dilutions$discharge)~ log(dilutions$roll_mean))
summary(model_exp)


processed_fb_stage_24 <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    filter(DATETIME <= ymd_hms("2024-08-06 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  drop_na() %>% 
  mutate(Q = roll_mean * fb_stage_Q_model$coefficients[2] +  fb_stage_Q_model$coefficients[1])

#exponential equation
processed_fb_stage_24 <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    filter(DATETIME <= ymd_hms("2024-08-06 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  drop_na() %>% 
  mutate(Q = exp(log(roll_mean) * model_exp$coefficients[2] +  model_exp$coefficients[1]))

processed_fb_stage %>% 
  ggplot(aes(x = DATETIME, y = Q))+
  geom_line()+
  theme_classic()

all_fb_Q <- rbind(processed_fb_stage_23, processed_fb_stage_24)

all_fb_Q %>% 
  ggplot(aes(x = DATETIME, y = Q))+
  geom_line(na.rm = FALSE)+
  theme_classic()
#output processed salt dilutions
q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_fb_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_fb_stage$roll_mean - bf$bf, threshold = 0.2, min.diff = 20)
#plot the events
plotEvents(data = processed_fb_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_fb_stage

# Copy of your event table
events <- limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_df_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()

fb_limbs <- rbind(stage_df_24, stage_df_23) %>% 
  rename("datetime" = DATETIME)
#BIG PROBLEM
```
```{r FB-2025}
#Read in measurements from 2024

FB_air <- read_csv("./pducers_summer25/fb_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water <- read_csv("./pducers_summer25/fb_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

# processed salt dilutions
dilutions <- q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")
dilutions

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701,
         #apply a rolling mean to get rid of temperature artifacts
         roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+
  
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")+
  geom_point(data = dilutions,
             aes(x = DATETIME, y = stage_cm),
             color = "blue")





processed_fb_stage <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    filter(DATETIME <= ymd_hms("2024-07-21 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()


#output processed salt dilutions
q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_fb_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_fb_stage$roll_mean - bf$bf, threshold = 0.2, min.diff = 20)
#plot the events
plotEvents(data = processed_fb_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_fb_stage

# Copy of your event table
events <- limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_df_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()

fb_limbs <- rbind(stage_df_24, stage_df_23) %>% 
  rename("datetime" = DATETIME)
#BIG PROBLEM
```


```{r ZZ-2023}
#chunk that reads in stage, converts to proper units
#read in stage, convert to a height
ZZ_air <- read_csv("./PressureTransducers_11_14_23/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water <- read_csv("./PressureTransducers_11_14_23/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))


processed_zz_stage <- ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>%
  #filter(DATETIME <= ymd_hms("2024-07-21 10:30:00 UTC")) %>% 
   filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  drop_na()

#View(processed_zz_stage)
processed_zz_stage %>%
  ggplot()+
    geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+  theme_classic()+
  labs(title = "ZZ Stage",
       x = "",
       y = "Stage (in)")


#identifying events
bf = baseflowB(processed_zz_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_zz_stage$roll_mean - bf$bf, threshold = 0.25, min.diff = 85)
#plot the events
plotEvents(data = processed_zz_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_zz_stage

# Copy of your event table
events <- limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_23 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()


#BIG PROBLEM
```
```{r ZZ-2024}
#chunk that reads in stage, converts to proper units
#read in stage, convert to a height
ZZ_air <- read_csv("./PressureTransducers_summer24/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water <- read_csv("./PressureTransducers_summer24/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))


processed_zz_stage <- ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>%
  filter(DATETIME <= ymd_hms("2024-07-21 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()

processed_zz_stage %>% ggplot()+
    geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+  theme_classic()+
  labs(title = "ZZ Stage",
       x = "",
       y = "Stage (in)")

#output processed salt dilutions
# q_combined %>% filter(shed == "ZZ") %>% 
#   mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
#   rename(DATETIME = datetime) %>% 
#   left_join(processed_zz_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_zz_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_zz_stage$roll_mean - bf$bf, threshold = 0.25, min.diff = 85)
#plot the events
plotEvents(data = processed_zz_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_zz_stage

# Copy of your event table
events <- limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()

zz_limbs <- rbind(stage_24, stage_23)%>% 
  rename("datetime" = DATETIME)
#BIG PROBLEM

```

```{r}
#Read in measurements from 2024

zz_air_25 <-
  rbind(read_csv("./pducers_summer25/zz_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)))

ZZ_air <- read_csv("./PressureTransducers_summer24/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)) %>% 
  rbind(fb_air_25)

zz_water_25 <- 
  read_csv("./pducers_summer25/zz_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))


ZZ_water <- read_csv("./PressureTransducers_summer24/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)) %>% 
  rbind(zz_water_25)

processed_zz_stage <- ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    #filter(DATETIME <= ymd_hms("2024-08-06 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()

View(processed_zz_stage)

processed_zz_stage %>% 
  filter(DATETIME == ("2024-08-01 11:00:00"))

# processed salt dilutions
dilutions <- q_combined %>% filter(shed == "ZZ") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_zz_stage, by = "DATETIME")
dilutions

ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701,
         #apply a rolling mean to get rid of temperature artifacts
         roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+
  
  theme_classic()+
  labs(title = "ZZ Stage",
       x = "",
       y = "Stage (in)")+
  geom_point(data = dilutions,
             aes(x = DATETIME, y = stage_cm),
             color = "blue")

dilutions %>% 
  ggplot(aes(x = roll_mean, y = discharge))+
  geom_point()+
  labs(x = "Stage (cm)", y = "Discharge (L/s)",
       title = "ZZ Stage/Q")+
  theme_classic()+
  geom_smooth(method = "lm")

exponential.model <- lm(log(dilutions$discharge)~ dilutions$roll_mean)
summary(exponential.model)

linear <- lm((dilutions$discharge)~ dilutions$roll_mean)
summary(linear)


#now apply model to convert to discharge across time
processed_zz_stage %>% 
  mutate(prediction = (roll_mean^exponential.model$coefficients[2])-exponential.model$coefficients[1]) %>% 
  ggplot(aes(x = DATETIME, y = prediction))+
  geom_line()
  
```


### Number of transitions in each component of hydrograph
```{r W3}
#make companion plot that shows the number of transitions during each part of hydrograph
test <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>%
  group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

num_trans_w3 <- test %>% 
  group_by(datetime, state_change) %>% 
  summarise(number_of_nodes = length(state_change))

test %>% 
  inner_join(discharge_df, by = "datetime") %>%
  group_by(state_change, event_type) %>% 
  summarise(count = n()) %>% 
  pivot_wider(names_from = state_change, values_from = count) %>% 
  kable(format = "markdown")


```
```{r FB}
num_trans_fb <- input_fb %>% 
  filter(mins %in% c(0, 30)) %>%
  group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

num_trans_fb$state_change <- "none"
num_trans_fb$state_change[num_trans_fb$transition == -1] <- "wetting"
num_trans_fb$state_change[num_trans_fb$transition == 1] <- "drying"

num_trans_fb <- num_trans_fb %>% 
  group_by(datetime, state_change) %>% 
  summarise(number_of_nodes = length(state_change))
```
```{r ZZ}
num_trans_zz <- input_zz %>% 
  filter(mins %in% c(0, 30)) %>%
  group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

num_trans_zz$state_change <- "none"
num_trans_zz$state_change[num_trans_zz$transition == -1] <- "wetting"
num_trans_zz$state_change[num_trans_zz$transition == 1] <- "drying"

num_trans_zz <- num_trans_zz %>% 
  group_by(datetime, state_change) %>% 
  summarise(number_of_nodes = length(state_change))
```
# Graph theory sequences
## Determine all possible combinations, or the full graph
Determine all possible combinations
```{r W3}
input <- rbind(bind23, bind24) %>%
  filter(wshed == "W3", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- W3_IDs

#create empty list to hold repeated node IDs
zzz <- length(combos)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}
#create data frame with all possible combinations
W3_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#Run suite of functions to determine proportion of time that state changes follow the parent -> child relationship
W3_all_combos <- calc_props(W3_combos_routes, "W3")

#general_graph(W3_IDs, "W3", methods = c("cheapest_insertion"), two_opt = TRUE)
```



```{r FB}
#set routes for all 3 watersheds
input <- rbind(bind23, bind24) %>%
  filter(wshed == "FB", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- FB_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)

#run calc_support for all sheds and timesteps for relative position
FB_all_combos <- calc_props(all_combos_routes, "FB")
```
```{r ZZ}
#set routes for all 3 watersheds

input <- rbind(bind23, bind24) %>%
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>%
  select(datetime, binary, ID) %>%
  #mutate(ID = paste0("r_",ID)) %>% 
  pivot_wider(names_from = ID, values_from = binary)

combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
ZZ_all_combos <- calc_props(all_combos_routes, "ZZ")
```

What if I tried to show this in some way- map where each sensor was the average prop value when it is a parent?
```{r}
parents <- W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  rename(parent = down, child = up) %>% 
  group_by(parent) %>% 
  summarise(mean_parent = mean(prop)) %>% 
  rename(ID = parent)
  
childs <- W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  rename(parent = down, child = up) %>% 
  group_by(child) %>% 
  summarise(mean_child = mean(prop)) %>% 
  rename(ID = child)

left_join(parents, childs, by = "ID") %>% 
  ggplot(aes(x = mean_parent, y = mean_child))+
  geom_point()

parents <- FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  rename(parent = down, child = up) %>% 
  group_by(parent) %>% 
  summarise(mean_parent = mean(prop)) %>% 
  rename(ID = parent)
  
childs <- FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  rename(parent = down, child = up) %>% 
  group_by(child) %>% 
  summarise(mean_child = mean(prop)) %>% 
  rename(ID = child)

left_join(parents, childs, by = "ID") %>% 
  ggplot(aes(x = mean_parent, y = mean_child))+
  geom_point()
```

## Find a path through the full graph using the TSP
For each watershed, find the best chain using solutions to traveling salesman problem.  Testing every method for solving the TSP manually.  Done before developing the generalized function below.   
```{r W3}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

# methods <- c("random",
#   "cheapest_insertion")

#set up edges and distance matrix, convert to TSP object
edges_W3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()

keyz <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))

# Convert to TSP object

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  atsp <- as.ATSP(edges_W3)

    atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], two_opt = TRUE)
    #tour <- solve_TSP(tour, method = "two_opt")

  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "W3") %>% 
    mutate("method" = methods[i])

    if(i == 1) all_chain_outputs <- chain_output
    if(i > 1) all_chain_outputs <- rbind(all_chain_outputs, chain_output)
}

all_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

```

```{r fooling around}
# 8/12/25, fooling around
keyz <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))


edges_W3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()

  #concorde solver?
#downloaded concorde but could not figure out how to get it working
concorde_path("/Users/johnmorgan/Documents/concorde/TSP")

data("USCA312")

## run concorde in verbose mode (-v) with fast cuts only (-V)
solve_TSP(USCA312, method = "concorde", control = list(clo = "-v -V"))

#now try applying
atsp <- as.ATSP(edges_W3)

    atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = "concorde", as_TSP = TRUE, precision = 6, two_opt = TRUE)
  #control = list(clo = "-B")
    #tour <- solve_TSP(atsp, method = "cheapest_insertion", as_TSP = TRUE, two_opt = TRUE)

    #tour <- solve_TSP(tour, method = "two_opt")

  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
con_sol <- c(edges[,1], edges[25,2])

concon <- tibble(ID = con_sol, sequence = seq(1, length(con_sol), 1))
concon <- tibble(node_ID = con_sol, sequence = seq(1, length(con_sol), 1)) %>% 
  left_join(key2, by = "node_ID") %>%
  rename(ID = sensor_ID) %>% 
  select(ID, sequence)
  

#con_model <- calc_model_result(concon$ID, "W3") %>% rename("con" = pred_out)
produce_metrics(concon, "W3", "concorde")


#acc_0.755 <- concon

#trying another thing
C_int <- round(1e6 * edges_W3)
diag(C_int) <- max(C_int) + 1L


reformed <- reformulate_ATSP_as_TSP(atsp)

write_TSPLIB(reformed, file = "myinstance.tsp")

sol <- as.integer(scan("~/Documents/concorde/myinstance.sol", quiet = TRUE)[-1])

read_delim("~/Documents/concorde/myinstance.sol", delim = " ")

as.TOUR(sol+1)

n_orig <- 26
map_to_atsp <- function(id) {
  if (id < n_orig) {
    return(id)
  } else if (id < 2 * n_orig) {
    return(id - n_orig)
  } else {
    return(NA)  # dummy
  }
}
tour_atsp <- vapply(sol, map_to_atsp, integer(1))
tour_atsp <- tour_atsp[!is.na(tour_atsp)]         # remove dummy
tour_atsp <- tour_atsp[!duplicated(tour_atsp)]    # keep first visit only


filter_ATSP_as_TSP_dummies(as.TOUR(sol+1))

```

Trying again, but filtering out every edge with a prop value less than 50%
```{r W3}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

methods <- c(
  "cheapest_insertion")

#set up edges and distance matrix, convert to TSP object
edges_W3 <- 
  W3_all_combos %>% 
  mutate(prop = case_when(prop < 0.5 ~ NA,
                          prop >= 0.5 ~ prop)) %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()
edges_W3[is.na(edges_W3)] <- 1

keyz <- 
  W3_all_combos %>% 
    #filter(prop > 0.5) %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))

# Convert to TSP object

#FOR LOOP TO ITERATE THROUGH METHODS
#for(i in 1:length(methods)){
i <- 1
  atsp <- as.ATSP(edges_W3)

    atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], two_opt = TRUE)
    #tour <- solve_TSP(tour, method = "two_opt")

  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  path
  #convert edges to format 
  #edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  tibble(node_ID = path,
         sequence = seq(1, length(path), 1)) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(ID = sensor_ID) %>% 
  select(ID, sequence)

produce_metrics(opt_routes, "W3", "test")

```

Trying one more thing- what if the sequence was defined in order by average prop value when it is the parent?
```{r}
#new_seq <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  rename(parent = down,
         child = up) %>% 
  group_by(parent) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  arrange(desc(mean)) %>% View()
  mutate(sequence = seq(1, length(con_big), 1)) %>% 
  rename(ID = parent) %>% 
  mutate(ID = as.numeric(substr(ID, 3, 4))) %>% 
  select(ID, sequence)
  #filter(prop != 0) %>% 
  
produce_metrics(new_seq, "W3", "ordered")


w3_pk_renum <- W3_pk_seq %>% 
  filter(ID %in% (W3_IDs)) %>% 
  mutate(sequence = seq(1, 26, 1))
new_seq %>% 
  left_join(w3_pk_renum, by = "ID") %>% 
View()
```
New sequence is just as good as proportion of time flowing, but definitely a different sequence... definitely related to the number of transitions I think

Calculate new sequence for figure 5:
```{r}
new_seq <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  rename(parent = down,
         child = up) %>% 
  group_by(parent) %>% 
  summarise(mean = mean(prop),
            sd = sd(prop)) %>% 
  arrange(desc(mean)) %>% #View()
  mutate(sequence = seq(1, length(25), 1)) %>% 
  rename(ID = parent) %>% 
  mutate(ID = as.numeric(substr(ID, 3, 4))) %>% 
  select(ID, sequence)
  
opt_routes <- 
  new_seq %>% 
  mutate(down = lag(ID)) %>% 
    rename("up" = ID) %>% 
    select(up, down) %>% drop_na()
  
calc_props(opt_routes, "W3")
```


```{r FB-analysis}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

#methods <- c("random", "cheapest_insertion")
keyz <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edges_FB <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  atsp <- as.ATSP(edges_FB)
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i])#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  path <- as.integer(tour)
  path <- path[labels(tour)[path] != "dummy"]
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "FB") %>% 
    mutate("method" = methods[i])

    if(i == 1) FB_chain_outputs <- chain_output
    if(i > 1) FB_chain_outputs <- rbind(FB_chain_outputs, chain_output)
}


#plots for committee meeting
FB_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

```
```{r ZZ-analysis}
#define all possible methods
#removed 3 methods
methods <- c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")

#methods <- c("random", "cheapest_insertion")
keyz <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edges_ZZ <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edges_ZZ)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(ZZ_IDs) + 1)#, start = 25)#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, "ZZ") %>% 
    mutate("method" = methods[i])

    if(i == 1) ZZ_chain_outputs <- chain_output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, chain_output)
}


#plots for committee meeting
ZZ_chain_outputs %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       subtitle = "0011 and 1100 removed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)


atsp <- as.ATSP(edges_ZZ)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[4], start = length(ZZ_IDs) + 1)#, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
  #path <- as.integer(tour)
  path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])



# Create a chain graph from the path
edges <- cbind(path[-length(path)], path[-1])
chain_graph <- graph_from_edgelist(edges, directed = TRUE)
plot(chain_graph,
       layout = layout_nicely(chain_graph),
     edge.arrow.size = 0.25,
  vertex.size = 10,
  vertex.label.cex = 0.75)

ggnet <- fortify(chain_graph, layout = igraph::layout_nicely(chain_graph))
ggplot(ggnet, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 8) +
  # geom_nodetext(aes(label =LETTERS[ path ]),
  #               fontface = "bold", color = "white", size = 3) +
  theme_blank()
```
```{r plot-chain}
edges2 <- all_graph_solutions %>% 
  filter(shed == "W3", method == "random", timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down) %>% as.matrix()
chain_graph <- graph_from_edgelist(edges2, directed = TRUE)
plot(chain_graph)

ggmst <- fortify(chain_graph, layout = igraph::layout_as_tree(chain_graph))
ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
                                                 type = "open")) +
  geom_nodes(color = "black", size = 3) +
  geom_nodetext(aes(label = name),
                fontface = "bold", color = "white", size = 1) +
  theme_blank()
```

Make a map of the activation sequence
```{r}

```

## Concorde solver
```{r}
edges_W3 <- 
  W3_all_combos %>% 
  mutate(prop = case_when(prop < 0.5 ~ NA,
                          prop >= 0.5 ~ prop)) %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()
edges_W3[is.na(edges_W3)] <- 1
```


```{r W3}
keyz_w3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2_w3 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz_w3), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz_w3), 3, 4))), 1))

edges_W3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()



# 1. JV-transform + dummy
atsp_to_stsp <- function(cost_mat, dummy_cost = 0) {
  n <- nrow(cost_mat)
  M <- max(cost_mat) * n + 1  # big constant
  
  # JV transform: create 2n x 2n symmetric matrix
  sym_mat <- matrix(M, nrow = 2*n, ncol = 2*n)
  
  # link in->out copies (zero cost)
  for (i in 1:n) {
    sym_mat[i, n+i] <- 0
    sym_mat[n+i, i] <- 0
  }
  
  # original costs from out(i) to in(j)
  for (i in 1:n) {
    for (j in 1:n) {
      if (i != j) {
        sym_mat[n+i, j] <- cost_mat[i, j]
        sym_mat[j, n+i] <- cost_mat[i, j]
      }
    }
  }
  
  # Add dummy node
  sym_with_dummy <- matrix(M, nrow = 2*n + 1, ncol = 2*n + 1)
  sym_with_dummy[1:(2*n), 1:(2*n)] <- sym_mat
  
  dummy_idx <- 2*n + 1
  # Connect dummy to all "in" nodes with dummy_cost
  sym_with_dummy[dummy_idx, 1:n] <- dummy_cost
  sym_with_dummy[1:n, dummy_idx] <- dummy_cost
  
  # return symmetric matrix
  sym_with_dummy
}

out_matrix_w3 <- atsp_to_stsp(edges_W3)
# 2. Write TSPLIB file
write_tsplib <- function(sym_mat, file) {
  tsp_obj <- TSP(as.dist(sym_mat))
  write_TSPLIB(tsp_obj, file = file)
}

write_tsplib(out_matrix_w3, "~/Documents/concorde/W3.tsp")

#/Users/johnmorgan/Documents/concorde/TSP/concorde -o W3solution.tour /Users/johnmorgan/Documents/concorde/W3.tsp

# 3. Read Concorde .sol and convert to ATSP tour
read_concorde_sol <- function(sol_file, cost_mat) {
  n <- nrow(cost_mat)
  tour_nodes <- scan(sol_file, what = integer(), quiet = TRUE)
  
  # Remove first line if it's length
  if (length(tour_nodes) == n + 1 || length(tour_nodes) == 2*n + 1 || length(tour_nodes) == 2*n + 2) {
    # If first number equals length of tour, drop it
    if (tour_nodes[1] == length(tour_nodes) - 1) {
      tour_nodes <- tour_nodes[-1]
    }
  }
  
  # Remove dummy index if present
  dummy_idx <- 2*n + 1
  tour_nodes <- tour_nodes[tour_nodes != (dummy_idx - 1)]  # Concorde may index from 0
  
  # Map back to original ATSP nodes: out(i) -> i, in(i) -> i
  atsp_tour <- unique(((tour_nodes %% n) + 1))
  
  atsp_tour
}

con_big_w3 <- read_concorde_sol("~/Documents/concorde/TSP/W3solution.tour", edges_W3)

concon2_w3 <- tibble(node_ID = con_big_w3, sequence = seq(1, length(con_big_w3), 1)) %>% 
  left_join(key2_w3, by = "node_ID") %>%
  rename(ID = sensor_ID) %>% 
  select(ID, sequence)
  
#con_model <- calc_model_result(concon$ID, "W3") %>% rename("con" = pred_out)
produce_metrics(concon2_w3, "W3", "concorde")

opt_routes_w3 <- 
  concon2_w3 %>% 
  mutate(down = lag(ID)) %>% 
    rename("up" = ID) %>% 
    select(up, down) %>% drop_na()

# actually determining how often the nodes follow proportion of time flowing sequence
conc_props_w3 <- calc_props(opt_routes_w3, "W3")

conc_props_w3 %>% 
  filter(timescale == "30mins") %>% 
  hist(prop)

hist(conc_props_w3$prop)
```
```{r FB}
keyz_fb <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2_fb <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz_fb), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz_fb), 3, 4))), 1))

edges_FB <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()


out_matrix_fb <- atsp_to_stsp(edges_FB)

write_tsplib(out_matrix_fb, "~/Documents/concorde/FB.tsp")
#/Users/johnmorgan/Documents/concorde/TSP/concorde -o FBsolution.tour /Users/johnmorgan/Documents/concorde/FB.tsp

con_big_fb <- read_concorde_sol("~/Documents/concorde/TSP/FBsolution.tour", edges_FB)

concon_fb <- tibble(node_ID = con_big_fb, sequence = seq(1, length(con_big_fb), 1)) %>% 
  left_join(key2_fb, by = "node_ID") %>%
  rename(ID = sensor_ID) %>% 
  select(ID, sequence)
  
#con_model <- calc_model_result(concon$ID, "W3") %>% rename("con" = pred_out)
produce_metrics(concon_fb, "FB", "concorde")

opt_routes_fb <- 
  tibble(node_ID = con_big_fb) %>% 
  left_join(key2_fb, by = "node_ID") %>% 
  mutate(down = lag(sensor_ID)) %>% 
    rename("up" = sensor_ID) %>% 
    select(up, down) %>% drop_na()

# actually determining how often the nodes follow proportion of time flowing sequence
hist(calc_props(opt_routes, "FB")$prop)
conc_props_fb <- calc_props(opt_routes, "FB")
```
```{r ZZ}
keyz_zz <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2_zz <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz_zz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz_zz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB

edges_ZZ <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) %>% as.matrix() %>% unname()


out_matrix_zz <- atsp_to_stsp(edges_ZZ)

write_tsplib(out_matrix_zz, "~/Documents/concorde/ZZ.tsp")
#/Users/johnmorgan/Documents/concorde/TSP/concorde -o ZZsolution.tour /Users/johnmorgan/Documents/concorde/ZZ.tsp

con_big_zz <- read_concorde_sol("~/Documents/concorde/TSP/ZZsolution.tour", edges_ZZ)

concon_zz <- tibble(node_ID = con_big_zz, sequence = seq(1, length(con_big_zz), 1)) %>% 
  left_join(key2_zz, by = "node_ID") %>%
  rename(ID = sensor_ID) %>% 
  select(ID, sequence)
  
#con_model <- calc_model_result(concon$ID, "W3") %>% rename("con" = pred_out)
produce_metrics(concon_zz, "ZZ", "concorde")

opt_routes_zz <- 
  tibble(node_ID = con_big_zz) %>% 
  left_join(key2_zz, by = "node_ID") %>% 
  mutate(down = lag(sensor_ID)) %>% 
    rename("up" = sensor_ID) %>% 
    select(up, down) %>% drop_na()

# actually determining how often the nodes follow proportion of time flowing sequence
conc_props_zz <- calc_props(opt_routes, "ZZ")


all_conc <- rbind(conc_props_w3,
                  conc_props_fb,
                  conc_props_zz) %>% 
  mutate(method = "Concorde TSP")
```



## Defining genearlized function
```{r generalized-function-graph-solution}
#define all possible methods
#removed 3 methods
all_methods = c("nearest_insertion", "random",
  "cheapest_insertion", "farthest_insertion", "arbitrary_insertion",
  "nn", "repetitive_nn")
#methods <- c("random", "cheapest_insertion")

general_graph <- function(combos, 
                          shed, 
                          methods = c("random", "cheapest_insertion"),
                          two_opt = TRUE){
#combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
allcombosIn <- calc_props(all_combos_routes, shed)

#methods <- c("random", "cheapest_insertion")
keyz <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edgesIn <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edgesIn)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], 
                    start = length(combos) + 1, two_opt = two_opt)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  edges <- cbind(path[-length(path)], path[-1])
  
  #apply key to convert nodes to sensor IDs for algorithm
opt_routes <- 
  as.data.frame(edges) %>% 
  rename("parent" = V2, "child" = V1) %>% 
 rename(node_ID = child) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(child = sensor_ID,
         done = node_ID,
         node_ID = parent) %>% 
  left_join(key2, by = "node_ID") %>% 
  rename(done2 = node_ID,
         parent = sensor_ID) %>% 
  select(parent, child)

#prepare routes for algorithm
routes_graph <- opt_routes %>% 
  rename("up" = child,
         "down" = parent) %>% drop_na()

chain_output <- calc_props(routes_graph, shed) %>% 
    mutate("method" = methods[i])

    if(i == 1) ZZ_chain_outputs <- chain_output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, chain_output)
}
return(ZZ_chain_outputs)
}

#test general graph solution
ZZ_graph_solution <- general_graph(ZZ_IDs, "ZZ")

ZZ_graph_solution %>% 
  filter(timescale %in% c("30mins", "daily")#, hierarchy == "Flow Permanence"
         ) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(~timescale)

#separate function to output the path, for non-determinstic methods may result in a different chain than other function
#can take multiple methods
# True/false whether output includes visualization of the chain
##works on a single method to develop a chain
chain_solution <- function(combos, 
                           shed, 
                           methods = "cheapest_insertion", 
                           plot = FALSE,
                           two_opt = TRUE){
#combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
allcombosIn <- calc_props(all_combos_routes, shed)

#methods <- c("random", "cheapest_insertion")
keyz <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edgesIn <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edgesIn)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(combos) + 1, two_opt = two_opt)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  #edges <- cbind(path[-length(path)], path[-1])
   output <- data.frame("node_ID" = path) %>% 
     left_join(key2, by = "node_ID") %>% 
     mutate(method = methods[i])
  
    if(i == 1) ZZ_chain_outputs <- output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, output)
}



chains_test <- ZZ_chain_outputs
edges <- cbind(chains_test$node_ID[-length(chains_test$node_ID)], chains_test$node_ID[-1])

chain_graph <- graph_from_edgelist(edges, directed = TRUE)

V(chain_graph)$labels <- chains_test$sensor_ID

ggmst <- fortify(chain_graph, layout = igraph::layout_as_tree(chain_graph))

# if(plot == TRUE){
# ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
#   geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
#                                                  type = "open")) +
#   geom_nodes(color = "black", size = 3) +
#   geom_nodetext(aes(label = labels),
#                 fontface = "bold", color = "white", size = 1) +
#   theme_blank()
# }

order <- ggmst %>% 
  arrange(desc(y))

#unique(order$labels)

#perfecting chains function to just output a dataframe, with one column for the sequence number, and the other for sensor ID
chains_output <- data.frame("ID" = unique(order$labels),
           "sequence" = seq(1, length(unique(order$labels)), 1))

return(chains_output)

}
chain_solution(ZZ_IDs, "ZZ", plot = TRUE)


#generate chain solution for a series of methods, or the same method many times?
#NOT FINISHED
chain_solution_multiple <- function(combos, 
                           shed, 
                           methods = "cheapest_insertion", 
                           plot = FALSE){
#combos <- ZZ_IDs
zzz <- length(combos)
rep(combos, zzz)
all_list <- c()
for(z in 1:zzz){
  all_list <- c(all_list, rep(combos[z], zzz))
}

all_combos_routes <- data.frame("up" = rep(combos, zzz),
                                "down" = all_list)
#run calc_support for all sheds and timesteps for relative position
allcombosIn <- calc_props(all_combos_routes, shed)

#methods <- c("random", "cheapest_insertion")
keyz <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  mutate(prop = 1 -prop) %>%  #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    select(-up) #%>% as.matrix() %>% unname()
#create an accurate key, old one is WRONG!!!
key2 <- data.frame(sensor_ID = as.numeric(substr(colnames(keyz), 3, 4)),
                   node_ID = seq(1, length(as.numeric(substr(colnames(keyz), 3, 4))), 1))
#set up edges and distance matrix, convert to TSP object

#IF there are NAs, replace them with 1; results from when there is a node that flowed the whole time it was deployed along with another, and not deployed for part of the other one's deployment; happened most in FB
edgesIn <- 
  allcombosIn %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  #rename(labels = prop) %>% 
  #filter(prop != 0) %>% 
  mutate(prop = 1 - prop) %>% #%>% #weight will be opposite prop value
  select(up, down, prop) %>% 
    pivot_wider(names_from = down, values_from = prop) %>% 
    mutate(
    across(everything(), ~replace_na(.x, 1))
  ) %>%
    select(-up) %>% as.matrix() %>% unname()



# reformat from atsp to tsp
#tsp1 <- reformulate_ATSP_as_TSP(atsp)

#FOR LOOP TO ITERATE THROUGH METHODS
for(i in 1:length(methods)){
  # Convert to TSP object
atsp <- as.ATSP(edgesIn)
  
  atsp <- insert_dummy(atsp, label = "dummy")
  tour <- solve_TSP(atsp, method = methods[i], start = length(combos) + 1, two_opt = TRUE)
  #tour <- filter_ATSP_as_TSP_dummies(tour, atsp)

  # Extract path, removing dummy node
   path <- unname(cut_tour(tour, "dummy"))
  #convert edges to format 
  #edges <- cbind(path[-length(path)], path[-1])
   output <- data.frame("node_ID" = path) %>% 
     left_join(key2, by = "node_ID") %>% 
     mutate(method = methods[i])
  
    if(i == 1) ZZ_chain_outputs <- output
    if(i > 1) ZZ_chain_outputs <- rbind(ZZ_chain_outputs, output)
}



chains_test <- ZZ_chain_outputs
edges <- cbind(chains_test$node_ID[-length(chains_test$node_ID)], chains_test$node_ID[-1])

chain_graph <- graph_from_edgelist(edges, directed = TRUE)

V(chain_graph)$labels <- chains_test$sensor_ID

ggmst <- fortify(chain_graph, layout = igraph::layout_as_tree(chain_graph))

# if(plot == TRUE){
# ggplot(ggmst, aes(x = x, y = y, xend = xend, yend = yend)) +
#   geom_edges(color = "grey", arrow = grid::arrow(length = unit(6, "pt"),
#                                                  type = "open")) +
#   geom_nodes(color = "black", size = 3) +
#   geom_nodetext(aes(label = labels),
#                 fontface = "bold", color = "white", size = 1) +
#   theme_blank()
# }

order <- ggmst %>% 
  arrange(desc(y))

#unique(order$labels)

#perfecting chains function to just output a dataframe, with one column for the sequence number, and the other for sensor ID
chains_output <- data.frame("ID" = unique(order$labels),
           "sequence" = seq(1, length(unique(order$labels)), 1))

return(chains_output)

}
```
```{r applying-gen-func}
#creating chains

W3_cheap <- chain_solution(W3_IDs, "W3", methods = "cheapest_insertion")
W3_random <- chain_solution(W3_IDs, "W3", methods = "random")

FB_cheap <- chain_solution(FB_IDs, "FB", methods = "cheapest_insertion")
FB_random <- chain_solution(FB_IDs, "FB", methods = "random")

ZZ_cheap <- chain_solution(ZZ_IDs, "ZZ", methods = "cheapest_insertion")
ZZ_random <- chain_solution(ZZ_IDs, "ZZ", methods = "random", two_opt = FALSE)

```

## Comparing all solutions to the TSP for every watershed
```{r compare-graph-theory-solutions}
all_graph_solutions <- rbind(general_graph(W3_IDs, "W3"),
                             general_graph(FB_IDs, "FB"),
                             general_graph(ZZ_IDs, "ZZ"))

showcase <- rbind(general_graph(W3_IDs, "W3", all_methods),
                             general_graph(FB_IDs, "FB", all_methods),
                             general_graph(ZZ_IDs, "ZZ", all_methods))

showcase %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    #geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  facet_grid(shed~timescale)
```



# Defining proportion of time flowing sequences
My big graph theory solution provides the random hierarhcy and graph theory solution. All I need is the proportion of time flowing and maybe one based on topography... maybe drainage area or TWI?
```{r flow-permanence-W3}
#just summer 2023
data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
#make binary column
data_24$binary <- 1
data_24$binary[data_24$wetdry == "dry"] <- 0

pks_23 <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>% 
  #left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()
#just summer 2024
pks_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number) %>% 
  #left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()

both <- inner_join(pks_23, pks_24, by = "ID")
ggplot()+
  geom_point(data = both,aes(x = pk.x, y = pk.y))+
  labs(title = "Change in pk from '23 to '24, W3",
       x = "2023",
       y = "2024")+
  geom_abline(slope=1, intercept=0)+
  theme_classic()

#both summers
#just rbind summers 23 and 24
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_w3 <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "W3")
#write_csv(pks_w3, "pks_w3.csv")

#Define sequence by proportion of time flowing
W3_pk_seq <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_w3$ID), 1)) %>% 
  select(ID, sequence)
```
```{r flow-permanence-FB}
#both summers
#just rbind summers 23 and 24
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_fb <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "FB")

#Define proportion of time flowing sequence
#make a df with ID and the number in the sequence
FB_pk_seq <- pks_fb %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_fb$ID), 1)) %>% 
  select(ID, sequence)
```
```{r flow-permanence-ZZ}
precalc_24 <- data_24 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, number, lat, long, binary) %>% 
    group_by(number) %>% 
  rename("DATETIME" = datetime, "ID" = number)
  
pks_zz <- data_23 %>% 
    mutate(mins = minute(datetime)) %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    #slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>%
  rbind(precalc_24) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  mutate(wshed = "ZZ")

#define proportion of time flowing sequence for ZZ
ZZ_pk_seq <- pks_zz %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_zz$ID), 1)) %>% 
  select(ID, sequence)
```
```{r proportion-of-time-flowing-analysis}
routes_w3 <- pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- pks_fb %>%
    filter(ID %in% FB_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- pks_zz %>%
    filter(ID %in% ZZ_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

# actually determining how often the nodes follow proportion of time flowing sequence
all_pk <- rbind(calc_props(routes_w3, "W3"),
                calc_props(routes_fb, "FB"),
                calc_props(routes_zz, "ZZ")) %>% 
  mutate("method" = "Flow Permanence")
```

# Defining TWI sequences
```{r locs-from-arc}
#get snapped sensor locations from extracted values from arc
w3_locs <- read_csv("./STIC_uaa/w32.csv")%>% 
  select(ID, POINT_X, POINT_Y)
fb_locs <- read_csv("./STIC_uaa/fb2.csv") %>% 
  select(ID, POINT_X, POINT_Y)
zz_locs <- read_csv("./STIC_uaa/zz1.csv")%>% 
  select(ID, POINT_X, POINT_Y)
```
```{r calculate-curvature}
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)

# w3_shed <- "./w3_dems/w3_shed.tif"
# w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)
# #FB
# fb_shed <- "./fb_dems/fb_shed.tif"
# fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)
# #ZZ
# zz_shed <- "./zz_dems/zz_shed.tif"
# zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)

# fb_curve <- m1 %>% 
#   crop(fb_outline) %>% 
#   mask(fb_outline) %>% 
#   spatialEco::curvature()
# zz_curve <- m1 %>% 
#   crop(zz_outline) %>% 
#   mask(zz_outline)%>% 
#   spatialEco::curvature()
# w3_curve <- m1 %>% 
#   crop(w3_outline) %>% 
#   mask(w3_outline)%>% 
#   spatialEco::curvature()
#curve1 <- spatialEco::curvature(m1)
curve10 <- spatialEco::curvature(m10, type = "planform")

w3_curve <- extract(curve10, w3_locs[,2:3]) %>% 
  rename("curvature" = dem1m) %>% 
  pivot_longer(-ID)
fb_curve <- extract(curve10, fb_locs[,2:3]) %>% 
  rename("curvature" = dem1m) %>% 
  pivot_longer(-ID)
zz_curve <- extract(curve10, zz_locs[,2:3]) %>% 
  rename("curvature" = dem1m) %>% 
  pivot_longer(-ID)
```

```{r calculate-w3-topo}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(w3_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)


w3_uaa <- extract(rast(flowacc_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("uaa" = `X10mdem_flowacc`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

w3_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

w3_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

w3_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

w3_topo <- rbind(w3_uaa, w3_twi, w3_tpi, w3_slope, w3_curve)

```
```{r calculate-fb-topo}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(fb_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

fb_uaa <- extract(rast(flowacc_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("uaa" = `X10mdem_flowacc`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

fb_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

fb_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

fb_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

fb_topo <- rbind(fb_uaa, fb_tpi, fb_twi, fb_slope, fb_curve)
```
```{r calculate-zz-topo}
#flow accumulation/upslope drainage area at 3 m resolution calculated earlier in markdown
flowacc_output <- "./HB/1m hydro enforced DEM/dem3m_flowacc.tif"

#convert STIC data to a SpatVector data format
locs_shape <- vect(zz_locs, 
                   geom=c("POINT_X", "POINT_Y"), 
                   crs = crs(rast(flowacc_output))) #set crs to NAD 83

#calculate 10m DEM, then breach and fill
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./w3_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./w3_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./w3_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./w3_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

#flow accumulation/drainage area
flowacc_output <- "./w3_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")
#Slope
slope_output <- "./w3_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")
#calculate TPI
tpi_output <- "./w3_dems/10mdem_tpi.tif"
wbt_relative_topographic_position(
    dem = fill_output, 
    output = tpi_output, 
    filterx=11, 
    filtery=11)
#TWI
twi_output <- "./w3_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

zz_uaa <- extract(rast(flowacc_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("uaa" = `X10mdem_flowacc`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

zz_tpi <- extract(rast(tpi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("tpi" = `X10mdem_tpi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

zz_twi <- extract(rast(twi_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("twi" = `X10mdem_twi`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

zz_slope <- extract(rast(slope_output), locs_shape, ID = FALSE, bind = TRUE) %>% 
  rename("slope" = `X10mdem_slope`) %>% 
  as_tibble() %>% 
  pivot_longer(-ID)

zz_topo <- rbind(zz_uaa, zz_tpi, zz_twi, zz_slope, zz_curve)


```
```{r topographic-wetness-index}
routes_w3_twi <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID,
         "twi" = value) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

routes_fb_twi <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>% 
  rename("up" = ID,
         "twi" = value) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down)

routes_zz_twi <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("up" = ID,
         "twi" = value) %>%
  arrange(desc(twi)) %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

all_twi <- rbind(calc_props(routes_w3_twi, "W3"),
                 calc_props(routes_fb_twi, "FB"),
                 calc_props(routes_zz_twi, "ZZ")) %>% 
  mutate("method" = "Topographic Wetness Index")

```
```{r define-TWI-sequences}


#create a reference dataframe with the sensor ID, then the number in the sequence
w3_twi_sequence <- w3_twi %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("twi" = value) %>% 
  arrange(desc(twi)) %>% 
  mutate(sequence = seq(1, length(w3_twi$ID), 1)) %>% 
  select(ID, sequence)

fb_twi_sequence <- fb_twi %>% 
    filter(ID %in% FB_IDs) %>%
  rename("twi" = value) %>%
  arrange(desc(twi)) %>% 
  mutate(sequence = seq(1, length(FB_IDs), 1)) %>% 
  select(ID, sequence)

zz_twi_sequence <- zz_twi %>% 
    filter(ID %in% ZZ_IDs) %>% 
  rename("twi" = value) %>%
  arrange(desc(twi)) %>% 
  mutate(sequence = seq(1, length(ZZ_IDs), 1)) %>% 
  select(ID, sequence)

```

# Defining New Sequence- average prop value as parent node
Calculate new sequence for figure 5:
```{r combine}
new_seq_w3 <- 
  W3_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  group_by(down) %>% 
  summarise(mean = mean(prop)) %>% 
  arrange(desc(mean)) %>% 
  mutate(up = as.numeric(substr(down, 3, 4)),
         down = lag(up)) %>% 
  select(down, up) %>% 
  drop_na() 

new_seq_fb <- 
  FB_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  group_by(down) %>% 
  summarise(mean = mean(prop)) %>% 
  arrange(desc(mean)) %>% 
  mutate(up = as.numeric(substr(down, 3, 4)),
         down = lag(up)) %>% 
  select(down, up) %>% 
  drop_na()

new_seq_zz <- 
  ZZ_all_combos %>% 
  filter(timescale == "30mins") %>% 
  ungroup() %>% 
  select(up, down, prop) %>% 
  group_by(down) %>% 
  summarise(mean = mean(prop)) %>% 
  arrange(desc(mean)) %>% 
  mutate(up = as.numeric(substr(down, 3, 4)),
         down = lag(up)) %>% 
  select(down, up) %>% 
  drop_na()
  
# actually determining how often the nodes follow proportion of time flowing sequence
all_new <- rbind(calc_props(new_seq_w3, "W3"),
                calc_props(new_seq_fb, "FB"),
                calc_props(new_seq_zz, "ZZ")) %>% 
  mutate("method" = "Average Sequential Wetting")
```

Make them ID/sequence format
```{r}
#write wrapper functions to convert between the two forms easily
#routes_w3 <- 
  pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

#convert from up/down to ID/seq
convert_to_IDseq <- function(input){

  convert <-
    input %>%
    mutate(sequence = seq(1, length(input$up), 1)) %>%
    rename(ID = down) %>%
    select(ID, sequence)
return(convert)
}
  

#convert from ID/seq to up/down
input <- W3_cheap

  input %>% 
    mutate(down = "")
  mutate(up = as.numeric(substr(down, 3, 4)),
         down = lag(up)) %>% 
  select(down, up) %>% 
  drop_na()
```

# Defining random sequences

```{r}
W3_random <- 
tibble(down = sample(W3_IDs)) %>% 
  mutate(up = lag(down)) %>% 
  select(up, down) %>% 
  drop_na()

FB_random <- 
tibble(down = sample(FB_IDs)) %>% 
  mutate(up = lag(down)) %>% 
  select(up, down) %>% 
  drop_na()

ZZ_random <- 
tibble(down = sample(ZZ_IDs)) %>% 
  mutate(up = lag(down)) %>% 
  select(up, down) %>% 
  drop_na()


#create randomly generated sequences
all_random <- rbind(calc_props(W3_random, "W3"),
                    calc_props(FB_random, "FB"),
                    calc_props(ZZ_random, "ZZ")) %>% 
  mutate(method = "Random")
```


# Testing Sequences, running model
This section contains all chunks used to test the effectiveness of a given sequence.  
Model is using same Method as Botter et al. 2021, where the number of active nodes at a given timestep is known. Nodes are activated in order of the sequence until the number activated is reached. The presence or absence of flow based on the model is then compared to the actual values for each node at each timestep.  

```{r figuring-out-how}
#new method, incorporating missing nodes at different timesteps
input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  arrange((sequence)) %>% 
  # mutate("downstream" = lag(binary),
  #        "following" = downstream - binary) %>% 
  #filter(following != -1) %>% 
  filter(datetime == "2023-07-15 19:00:00") #%>% View()
   # summarise(count_non = length(following)) %>% View()


#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy
number_activated <- 
  input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  mutate(deployed = length(binary)) %>% 
  #arrange(sequence) %>% 
      #filter(binary == 1) %>% 
    summarise(number_flowing = length(binary[binary == 1]),
            deployed = list(unique(ID))) #%>% View()

number_activated
number_activated %>% mutate(all_nodes = W3_pk_seq$ID)



W3_pk_seq$ID[W3_pk_seq$ID %in% unlist(number_activated$deployed[1])]

# 2. Dataframe with timestamps and how many nodes to activate
number_activated <- number_activated %>% 
  select(-deployed)
# Fixed full node sequence
full_node_sequence <- W3_pk_seq$ID

# Sample activation schedule
activation_schedule <- number_activated

# Function to generate a single activation row with NA for missing nodes
    '%!in%' <- function(x,y)!('%in%'(x,y))

generate_activation_row <- function(datetime, n_active, available_nodes) {
  #n_active <- activation_schedule$number_flowing[1]
  #available_nodes <- unlist(activation_schedule$deployed[1])
  # Order available nodes based on full sequence
  ordered_available <- full_node_sequence[full_node_sequence %in% available_nodes]
  
    #not_deployed <- full_node_sequence[full_node_sequence %!in% (available_nodes)]


  
  # Identify the first `n_active` nodes to activate
  activated_nodes <- head(ordered_available, n_active)
  # ordered_available
  # activated_nodes
  # full_node_sequence
  output <- tibble("datetime" = datetime,
             "ID" = full_node_sequence,
             "binary" = 0) %>% 
    #mutate(binary = case_when(ID %in% ordered_available ~ 0)) %>% 
    mutate(binary = case_when(
                              ID %in% activated_nodes ~ 1,
                              ID %!in% ordered_available ~ NA,
                              TRUE ~ 0))
  return(output)
}

generate_activation_row(activation_schedule$datetime[1],
                        activation_schedule$number_flowing[1],
                        unlist(activation_schedule$deployed[1]))
# Apply the function to each row of the activation schedule

model_out <- Map(generate_activation_row, 
    datetime = activation_schedule$datetime, 
    n_active = activation_schedule$number_flowing, 
    available_nodes = activation_schedule$deployed) %>% dplyr::bind_rows() %>% 
  rename(pk_out = binary)

model_out
comparison <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(model_out, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins)
```
Make sure that the new way to model is working for sequences other than pk
```{r testing-cases}
#new method, incorporating missing nodes at different timesteps
number_activated <- 
  input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  mutate(deployed = length(binary)) %>% 
  #arrange(sequence) %>% 
      #filter(binary == 1) %>% 
    summarise(number_flowing = length(binary[binary == 1]),
            deployed = list(unique(ID))) #%>% View()

W3_pk_seq$ID[W3_pk_seq$ID %in% unlist(number_activated$deployed[1])]

# 2. Dataframe with timestamps and how many nodes to activate
number_activated <- number_activated %>% 
  select(-deployed)
# Fixed full node sequence
full_node_sequence <- W3_cheap$ID

# Sample activation schedule
activation_schedule <- number_activated

# Function to generate a single activation row with NA for missing nodes
    '%!in%' <- function(x,y)!('%in%'(x,y))

#generate_activation_row <- function(datetime, n_active, available_nodes) {
  n_active <- activation_schedule$number_flowing[1]
  available_nodes <- unlist(activation_schedule$deployed[1])
  datetime <- activation_schedule$datetime[1]
  # Order available nodes based on full sequence
  ordered_available <- full_node_sequence[full_node_sequence %in% available_nodes]
  
    #not_deployed <- full_node_sequence[full_node_sequence %!in% (available_nodes)]


  
  # Identify the first `n_active` nodes to activate
  activated_nodes <- head(ordered_available, n_active)
  ordered_available
  activated_nodes
  full_node_sequence
  output <- tibble("datetime" = datetime,
             "ID" = full_node_sequence,
             "binary" = 0) %>% 
    #mutate(binary = case_when(ID %in% ordered_available ~ 0)) %>% 
    mutate(binary = case_when(
                              ID %in% activated_nodes ~ 1,
                              ID %!in% ordered_available ~ NA,
                              TRUE ~ 0))
  return(output)
#}

generate_activation_row(activation_schedule$datetime[1],
                        activation_schedule$number_flowing[1],
                        unlist(activation_schedule$deployed[1]))
# Apply the function to each row of the activation schedule

model_out <- Map(generate_activation_row, 
    datetime = activation_schedule$datetime, 
    n_active = activation_schedule$number_flowing, 
    available_nodes = activation_schedule$deployed) %>% dplyr::bind_rows() #%>% 
  rename(pk_out = binary)

model_out
comparison <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(model_out, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins)
```

Now make a function to calc the model result using this method
```{r define calc-model-result function}
#new method, incorporating missing nodes at different timesteps
# only implemented for W3 rn
calc_model_result <- function(input_sequence, shed){

  input <- if(shed == "W3") input_w3
  else if (shed == "FB") input_fb
  else if (shed == "ZZ") input_zz
  
number_activated <- 
  input %>% 
  #left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  mutate(deployed = length(binary)) %>% 
  #arrange(sequence) %>% 
      #filter(binary == 1) %>% 
    summarise(number_flowing = length(binary[binary == 1]),
            deployed = list(unique(ID))) #%>% View()

#W3_pk_seq$ID[W3_pk_seq$ID %in% unlist(number_activated$deployed[1])]

# 2. Dataframe with timestamps and how many nodes to activate

# Fixed full node sequence
full_node_sequence <- input_sequence

# Sample activation schedule
activation_schedule <- number_activated

# Function to generate a single activation row with NA for missing nodes
    '%!in%' <- function(x,y)!('%in%'(x,y))

generate_activation_row <- function(datetime, n_active, available_nodes) {
  #n_active <- activation_schedule$number_flowing[1]
  #available_nodes <- unlist(activation_schedule$deployed[1])
  # Order available nodes based on full sequence
  ordered_available <- full_node_sequence[full_node_sequence %in% available_nodes]
  
    #not_deployed <- full_node_sequence[full_node_sequence %!in% (available_nodes)]


  
  # Identify the first `n_active` nodes to activate
  activated_nodes <- head(ordered_available, n_active)
  # ordered_available
  # activated_nodes
  # full_node_sequence
  output <- tibble("datetime" = datetime,
             "ID" = full_node_sequence,
             "binary" = 0) %>% 
    #mutate(binary = case_when(ID %in% ordered_available ~ 0)) %>% 
    mutate(binary = case_when(
                              ID %in% activated_nodes ~ 1,
                              ID %!in% ordered_available ~ NA,
                              TRUE ~ 0))
  return(output)
}

# Apply the function to each row of the activation schedule

model_out <- Map(generate_activation_row, 
    datetime = activation_schedule$datetime, 
    n_active = activation_schedule$number_flowing, 
    available_nodes = activation_schedule$deployed) %>% dplyr::bind_rows() %>% 
  rename(pred_out = binary)

return(model_out)
}

#Testing function on W3
calc_model_result(W3_pk_seq$ID, "W3")

# Helper function for determining type of error
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}

get_eval_label(0,0)

# comparison <- input_w3 %>% 
#   filter(mins %in% c(0, 30)) %>% 
#   left_join(model_out, by = c("datetime", "ID")) %>% 
#   select(-wshed, -mins)
# 
# 
# input_w3 %>% 
#   filter(mins %in% c(0, 30)) %>% 
#   left_join(cheap_model, by = c("datetime", "ID")) %>% 
#   select(-wshed, -mins) %>%
#   filter(datetime == ymd_hms("2023-08-08 04:00:00 UTC")) %>%  View()
```
Run on W3
```{r W3}


#running model
cheap_model_w3 <- calc_model_result(W3_cheap$ID, "W3") %>%   
  rename("cheapest" = pred_out)
pk_model_w3 <- calc_model_result(W3_pk_seq$ID, "W3") %>% 
  rename("pk" = pred_out)
random_model_w3 <- calc_model_result(W3_random$down, "W3") %>% 
  rename("random" = pred_out)
twi_model_w3 <- calc_model_result(w3_twi_sequence$ID, "W3") %>% 
  rename("twi" = pred_out)
new_model_w3 <- calc_model_result(new_seq_w3$down, "W3") %>% 
  rename("new" = pred_out)
con_model_w3 <- calc_model_result(opt_routes_w3$down, "W3") %>% 
  rename("concorde" = pred_out)


comparison_w3 <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model_w3, by = c("datetime", "ID")) %>% 
  left_join(pk_model_w3, by = c("datetime", "ID")) %>% 
    left_join(random_model_w3, by = c("datetime", "ID")) %>% 
  left_join(twi_model_w3, by = c("datetime", "ID")) %>% 
  left_join(new_model_w3, by = c("datetime", "ID")) %>% 
    #left_join(con_model_w3, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins) %>% 
  drop_na()

#apply get_eval function to each model run
comparison_w3$cheapest_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$cheapest)
comparison_w3$pk_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$pk)
comparison_w3$random_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$random)
comparison_w3$twi_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$twi)
comparison_w3$new_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$new)
#comparison_w3$con_eval <- mapply(get_eval_label, comparison_w3$binary, comparison_w3$concorde)



# comparison$comparison_cheap <- with(comparison, ifelse(
#   cheapest_eval == pk_eval, "equal",
#   ifelse(cheapest_eval == "correct", "cheap_better",
#          ifelse(pk_eval == "correct", "pk_better", "neither_better"))
# ))
# comparison$comparison_random <- with(comparison, ifelse(
#   random_eval == pk_eval, "equal",
#   ifelse(random_eval == "correct", "cheap_better",
#          ifelse(pk_eval == "correct", "pk_better", "neither_better"))
# ))
# comparison$comparison_twi <- with(comparison, ifelse(
#   twi == pk_eval, "equal",
#   ifelse(twi == "correct", "cheap_better",
#          ifelse(pk_eval == "correct", "pk_better", "neither_better"))
# ))

#summarize through time
df_summary_time_w3 <- comparison_w3 %>%
  group_by(datetime) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
      new_correct = sum(new_eval == "correct"),
    #con_correct = sum(con_eval == "correct"),


    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct / total#,
    #con_accuracy = con_correct / total


  )

df_long_w3 <- df_summary_time_w3 %>%
  select(datetime, contains("accuracy")) %>% 
  tidyr::pivot_longer(cols = c(cheap_accuracy, pk_accuracy, 
                               random_accuracy, twi_accuracy, new_accuracy),#, con_accuracy), 
                      names_to = "model", values_to = "accuracy")

#make data long for final plot
comparison_long_w3 <- 
  comparison_w3 %>% 
  select(datetime, ends_with("eval")) %>% 
    pivot_longer(-datetime) %>% 
  mutate(year = year(datetime))
#final plot

#plot of correct, ommission, and commission through time
#plot that works
comparison_long_w3 %>% 
  filter(#name != "random_eval",
         name != "cheapest_eval",
         name != "con_eval",
         name != "random_eval") %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  mutate(value = fct_relevel(value, c("correct", "omission", "commission"))) %>% 
  ggplot() +
  geom_histogram(aes(x = day_of_year, fill = value), binwidth = 1, na.rm = TRUE)+
  scale_fill_manual(values = c("grey", "green", "orange"))+
  facet_grid(name~year)

#histogram where I pre-calculate the total number per day of the year, and what proportion that is of the total.
##then the bars will be the same h eight
test <- comparison_long_w3 %>% 
  filter(#name != "random_eval",
         name != "cheapest_eval",
         name != "con_eval",
         name != "random_eval") %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  mutate(value = fct_relevel(value, c("correct", "omission", "commission"))) %>% 
  group_by(datetime, name) %>% 
  mutate(total = length(value)) %>% 
  group_by(datetime, name, value) %>% 
  mutate(count = length(value)) %>% unique() #%>% 


test2 <- test %>%
  ungroup() %>% 
  mutate(prop = count/total) %>% 
  mutate(value = fct_relevel(value, c("omission", "correct", "commission")))

  ggplot(data = test2) +
  geom_bar(aes(x = (day_of_year), fill = value, y = prop), na.rm = TRUE, position="fill", stat="identity", width = 1)+
  scale_fill_manual(values = c("green","grey",  "orange"))+
  facet_grid(name~year)

                 

```

```{r old-plot-presence/absence-with-discharge}

#d <- 1
#start <- ymd_hms(paste0(crossed_up$dry_date[d]," 00:00:00"))
#stop <- ymd_hms(paste0(crossed_up$wet_date[d]," 00:00:00"))


test_IDS <- input_w3 #%>% 
    filter(datetime > start & datetime < stop)

remainder <- unique(test_IDS$ID)

pks_ordered <- pks_w3 %>% 
filter(ID %in% remainder) %>% 
  arrange(desc(pk)) %>% 
  rowid_to_column("pk_order")

q_23_plotting <- q_23_f %>% 
  rbind(q_24_f) %>% 
    #filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)

q_23_plotting <- q_23_f %>% 
    #filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()


test <- input_w3 %>% 
    #filter(datetime > start & datetime < stop) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(q_23_plotting, by = "datetime") %>% 
  left_join(pks_ordered, by = "ID")
sec2 <- with(test, train_sec(pk_order, Q_mm_day))

test %>% 
  ggplot(aes(x = datetime))+
  geom_tile(aes(y = (pk_order), fill = as.character(binary)))+
      geom_line(aes(y = sec2$fwd(Q_mm_day)), color = "white")+
  ylab("Supposed activation order")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$rev(.), name = "Instantaneous Q (mm/day)")) +
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(title = "Very dry to very wet",
       subtitle = "W3, 7/20 - 7/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-07-20 00:00:00"),
                            ymd_hms("2023-07-21 00:00:00"),
                            ymd_hms("2023-07-22 00:00:00")),
                   labels = c("7/20/23",
                              "7/21/23",
                              "7/22/23"))+
  theme_classic()+
    theme(legend.position="right")

#add hydrograph to plot
crossed_up
```

Combining these two ideas
```{r successfully graphed both accuracy over time and discharge}
test <- comparison_long_w3 %>% 
  filter(#name != "random_eval",
         name != "cheapest_eval",
         name != "con_eval",
         name != "random_eval") %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  mutate(value = fct_relevel(value, c("correct", "omission", "commission"))) %>% 
  group_by(datetime, name) %>% 
  mutate(total = length(value)) %>% 
  group_by(datetime, name, value) %>% 
  mutate(count = length(value)) %>% unique() #%>% 



test2 <- test %>%
  ungroup() %>% 
  mutate(prop = count/total) %>% 
  mutate(value = fct_relevel(value, c("correct", "omission", "commission"))) %>% 
  left_join(rbind(q_23_bind, q_24_bind), by = "datetime")

sec2 <- with(test2, train_sec(prop, Q_mm_day))

#Plot with both accuracy and discharge, very cooL!
test2 %>% 
  ggplot(aes(x = day_of_year)) +
  geom_bar(aes(fill = value, y = prop), na.rm = TRUE, position="fill", stat="identity", width = 1)+
  scale_fill_manual(values = c("grey", "green", "orange"))+
  geom_line(aes(y = sec2$fwd(Q_mm_day)), color = "gray40")+
  ylab("Accuracy")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$rev(.), name = "Instantaneous Q (mm/day)")) +
  facet_grid(name~year)

test2 %>% 
  ggplot(aes(x = datetime)) +
  geom_bar(aes(fill = value, y = prop),  position="fill", stat="identity", na.rm = TRUE)+
  scale_fill_manual(values = c("grey", "green", "orange"))+
  geom_line(aes(y = sec2$fwd(Q_mm_day)), color = "gray40")+
  ylab("Accuracy")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$rev(.), name = "Instantaneous Q (mm/day)")) +
  theme_classic()+
  facet_grid(rows = "name")+
  scale_x_break(c(ymd_hms("2023-11-12 00:00:00"), ymd_hms("2024-05-20 00:00:00")))+
  scale_x_continuous(
    #breaks = as.numeric(pretty(df$date)),
    labels = function(datetime) as_datetime(datetime, tz = "UTC")
  )

# Example: Number of seconds
    seconds_since_epoch <- 1678886400

    # Convert to POSIXct using lubridate's as_datetime
    datetime_object_lub <- as_datetime(seconds_since_epoch, tz = "UTC")

    # Print the result
    print(datetime_object_lub)
```

Now add a bar/different color for when it is the best sequence!
```{r}
five_colors <- c("#d68c45",  "#247BA0", "#A30B37", "#F0C808", "#2c6e49")


#plot the best at each timestep
unique_maxi <- 
  df_long_w3 %>% 
  filter(model != "random_accuracy",
         model != "cheap_accuracy",
         model != "con_accuracy") %>% 
  group_by(datetime) %>% 
  filter(accuracy == max(accuracy)) %>% 
  mutate(dup = as.character(n() > 1)) %>% ungroup()  %>%
  mutate(unique_sol = case_when(dup == TRUE ~ "z_tie",
                                dup == FALSE ~ model)) %>%
  filter(dup == FALSE) %>% 
  mutate(name = case_when(model == "cheap_accuracy" ~ "cheap_eval",
                          model == "pk_accuracy" ~ "pk_eval",
                          model == "new_accuracy" ~ "new_eval")) #%>% 
inner_join(big_comb, by = "datetime") %>%
  ggplot(aes(x = datetime, y = Q_mm_day, color = model)) +
  #geom_point() +
  geom_point(size = 1)+
  theme_classic()+
  scale_color_manual(values = c(five_colors[1:3], "lightgrey"))+
  labs(title = "Most accurate model along hydrograph", y = "Discharge (mm/day)", x = "")+
  facet_wrap(~ model)+
    scale_x_break(c(ymd_hms("2023-11-12 00:00:00"), ymd_hms("2024-05-20 00:00:00")))
  
test2 %>% 
  left_join(unique_maxi, by = c("datetime", "name")) %>% 
  ggplot(aes(x = datetime)) +
  geom_bar(aes(fill = value, y = prop),  position="fill", stat="identity", na.rm = TRUE)+
    scale_fill_manual(values = c("grey", "green", "orange"))+
#new_scale_fill()+
  geom_point(aes(y= 0), alpha = 0.3, na.rm = TRUE)+
  geom_line(aes(y = sec2$fwd(Q_mm_day)), color = "gray40")+
  ylab("Accuracy")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$rev(.), name = "Instantaneous Q (mm/day)")) +
  facet_grid(rows = "name")

```

Combine histogram of accuracy, colored by best model, with discharge plotted as a line overtop
Gave up on this plot, although it has potential
```{r}
test2 <- 
df_acc %>%
  left_join(rbind(q_23_bind, q_24_bind), by = "datetime")

sec2 <- with(test2, train_sec(accuracy, Q_mm_day))

#Plot with both accuracy and discharge, very cooL!
test2 %>% 
  ggplot(aes(x = datetime)) +
  geom_col(aes(y = accuracy, fill = unique_sol), width = 1800, position = "identity") +
    geom_line(aes(y = sec2$fwd(Q_mm_day)), color = "black", lwd = 1.5)+
  scale_y_continuous(sec.axis = sec_axis(~sec2$rev(.), name = "Instantaneous Q (mm/day)")
                     ) +
  theme_classic() +
  labs(y = "Accuracy", x = NULL) +
  scale_fill_manual(values = c(five_colors[1:3], "lightgrey")) +
  scale_color_manual(values = c(five_colors[1:3], "lightgrey")) +
  scale_x_break(c(ymd_hms("2023-11-1200:00:00"),
                  ymd_hms("2024-05-20 00:00:00"))) +
  scale_x_datetime(
    date_labels = "%b %Y",
    date_breaks = "1 month",
    timezone = "UTC"
  ) +
  theme(
    axis.text.x.top = element_blank(),
    axis.ticks.x = element_blank(),
    axis.line.x.top = element_blank()#,
    #legend.position = "none"   # legend will come from bottom plot
  )
```

Third plot idea- color discharge by the proportion of network that is flowing

```{r}
number_activated <- 
  input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  mutate(deployed = length(binary)) %>% 
  #arrange(sequence) %>% 
      #filter(binary == 1) %>% 
    summarise(number_flowing = length(binary[binary == 1]),
            deployed = list(unique(ID)))


test2 %>% 
  left_join(number_activated, by = "datetime") %>% 
  ggplot(aes(x = day_of_year)) +
  geom_bar(aes(fill = value, y = prop), na.rm = TRUE, position="fill", stat="identity", width = 1)+
  scale_fill_manual(values = c("grey", "black", "black"))+
  geom_line(aes(y = sec2$fwd(Q_mm_day), color = number_flowing))+
  scale_color_viridis()+
  ylab("Accuracy")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$rev(.), name = "Instantaneous Q (mm/day)")) +
  facet_grid(name~year)
```


```{r FB}
#chains used in graph theory solutions
FB_cheap <- chain_solution(FB_IDs, "FB", methods = "cheapest_insertion")
#FB_random <- chain_solution(FB_IDs, "FB", methods = "random")
FB_random <- tibble(ID = sample(FB_IDs),
                    sequence = seq(1, length(FB_IDs), 1))
                    

#applying evaluation model using different sequences
cheap_model_fb <- calc_model_result(FB_cheap$ID, "FB") %>% rename("cheapest" = pred_out)
pk_model_fb <- calc_model_result(FB_pk_seq$ID, "FB") %>% rename("pk" = pred_out)
random_model_fb <- calc_model_result(FB_random$down, "FB") %>% rename("random" = pred_out)
twi_model_fb <- calc_model_result(fb_twi_sequence$ID, "FB") %>% rename("twi" = pred_out)
new_model_fb <- calc_model_result(new_seq_fb$down, "FB") %>% rename("new" = pred_out)

comparison_fb <- input_fb %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model_fb, by = c("datetime", "ID")) %>% 
  left_join(pk_model_fb, by = c("datetime", "ID")) %>% 
    left_join(random_model_fb, by = c("datetime", "ID")) %>% 
  left_join(twi_model_fb, by = c("datetime", "ID")) %>% 
  left_join(new_model_fb, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins) %>% 
  drop_na()

#apply get_eval function to each model run
comparison_fb$cheapest_eval <- mapply(get_eval_label, comparison_fb$binary, comparison_fb$cheapest)
comparison_fb$pk_eval <- mapply(get_eval_label, comparison_fb$binary, comparison_fb$pk)
comparison_fb$random_eval <- mapply(get_eval_label, comparison_fb$binary, comparison_fb$random)
comparison_fb$twi_eval <- mapply(get_eval_label, comparison_fb$binary, comparison_fb$twi)
comparison_fb$new_eval <- mapply(get_eval_label, comparison_fb$binary, comparison_fb$new)



#summarize through time
df_summary_time_fb <- comparison_fb %>%
  group_by(datetime) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct / total
  )


df_long_fb <- df_summary_time_fb %>%
  select(datetime, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy, new_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy, new_accuracy), names_to = "model", values_to = "accuracy")

ggplot(df_long_fb, aes(x = datetime, y = accuracy, color = model)) +
  geom_line() +
  labs(title = "Model Accuracy Over Time", y = "Accuracy", x = "Time")

#original way of comparing- the grid
comparison_long_fb <- 
  comparison_fb %>% 
  select(datetime, ends_with("eval")) %>% 
    pivot_longer(-datetime) %>% 
  mutate(year = year(datetime))

# filter(datetime >= ymd_hms("2023-07-24 00:00:00") & datetime <= ymd_hms("2023-08-20 00:00:00")) %>% 
comparison_long_fb %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  #group_by(datetime) %>% 
  #filter(year == 2023) %>% 
  mutate(total = as.numeric(length(value))) %>% 
  ggplot() +
  geom_histogram(aes(x = day_of_year, fill = value), binwidth = 1, na.rm = TRUE)+
  facet_grid(name~year)
```
```{r ZZ}
ZZ_cheap <- chain_solution(ZZ_IDs, "ZZ", methods = "cheapest_insertion", two_opt = FALSE)
#ZZ_random <- chain_solution(ZZ_IDs, "ZZ", methods = "random", two_opt = FALSE)
ZZ_random <- tibble(ID = sample(ZZ_IDs),
                    sequence = seq(1, length(ZZ_IDs), 1))

cheap_model_zz <- calc_model_result(ZZ_cheap$ID, "ZZ") %>% rename("cheapest" = pred_out)
pk_model_zz <- calc_model_result(ZZ_pk_seq$ID, "ZZ") %>% rename("pk" = pred_out)
random_model_zz <- calc_model_result(ZZ_random$ID, "ZZ") %>% rename("random" = pred_out)
twi_model_zz <- calc_model_result(zz_twi_sequence$ID, "ZZ") %>% rename("twi" = pred_out)
new_model_zz <- calc_model_result(new_seq_zz$down, "ZZ") %>% rename("new" = pred_out)


comparison_zz <- input_zz %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model_zz, by = c("datetime", "ID")) %>% 
  left_join(pk_model_zz, by = c("datetime", "ID")) %>% 
    left_join(random_model_zz, by = c("datetime", "ID")) %>% 
  left_join(twi_model_zz, by = c("datetime", "ID")) %>% 
  left_join(new_model_zz, by = c("datetime", "ID")) %>%
  select(-wshed, -mins) %>% 
  drop_na()

#apply get_eval function to each model run
comparison_zz$cheapest_eval <- mapply(get_eval_label, comparison_zz$binary, comparison_zz$cheapest)
comparison_zz$pk_eval <- mapply(get_eval_label, comparison_zz$binary, comparison_zz$pk)
comparison_zz$random_eval <- mapply(get_eval_label, comparison_zz$binary, comparison_zz$random)
comparison_zz$twi_eval <- mapply(get_eval_label, comparison_zz$binary, comparison_zz$twi)
comparison_zz$new_eval <- mapply(get_eval_label, comparison_zz$binary, comparison_zz$new)



#summarize through time
df_summary_time_zz <- comparison_zz %>%
  group_by(datetime) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct / total
  )


#make data long for final plot
comparison_long_zz <- 
  comparison_zz %>% 
  select(datetime, ends_with("eval")) %>% 
    pivot_longer(-datetime) %>% 
  mutate(year = year(datetime))
#final plot
comparison_long_zz %>% 
  mutate(day_of_year = yday(datetime)
  ) %>% 
  #group_by(datetime) %>% 
  #filter(year == 2023) %>% 
  mutate(total = as.numeric(length(value))) %>% 
  ggplot() +
  geom_histogram(aes(x = day_of_year, fill = value), binwidth = 1, na.rm = TRUE)+
  facet_grid(name~year)


```



# Paper Figures
## Figure 1: Maps of watersheds
```{r W3-map}
#map of watershed 3 with depth to bedrock
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
hill <- rast(hillshade_out)

#dem
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
#crop to rectangular area
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#writeRaster(crop1, "1mdemw3_cropped.tif")

#watershed boundary
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)
plot(w3_outline)
expanse(w3_outline)

#w3 network- thing I need to change
#read in shapefile of stream converted in ARC
vect_stream_path <- "./AGU24posterAnalysis/vector_stream/vector_stream.shp"
#stream as a vector
vect_stream <- vect(vect_stream_path)
plot(vect_stream)
#crop to watershed boundary
w3_stream_crop <- crop(vect_stream, w3_outline)
plot(w3_stream_crop)
#or i could use old classification

#point locations- snapped points from above chunk
w3_stic_locs_snap <- "w3_stic_locs_snap.shp"

w3_stic_locs_r <- vect(w3_stic_locs_snap) %>% 
  left_join(pks_w3, by = "ID")



#w3_stic_locs_r <- vect(w3_stic_locs_snap)
#writeVector(w3_stic_locs_r, "./seismic_map_exports/w3_stic_locs_snap.shp")


w3_net <- vect("./carrieZigZag/w3_network.shp")
#writeVector(w3_net, "./seismic_map_exports/network.shp")

plot(w3_net)
sum(perim(w3_net))

ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
     scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))

w3_map_f1 <- 
  ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("gray9", "gray48","lightgray", "white"),
                       guide = "none")+
    new_scale_fill() +
  #geom_spatraster(data = crop1, alpha = 0.5)+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "#9AC0CD", lwd = 2) +
  geom_sf(data = w3_stic_locs_r, aes(fill = pk), size = 3, pch = 21) +
  #geom_sf(data = dd, aes(color = (depth)), pch = 19, size = 3) +
  scale_fill_gradient(high = "#27408B", low = "white", guide = "none")+
  #geom_sf(data = w3_pour, colour = "black") +
   #scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
  #theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr',
                              pad_x = unit(0.4, "cm"),
                              pad_y = unit(1, "cm"))
                              
w3_map_f1
ggsave("w3_map_f1f.png", w3_map_f1 )

```
```{r FB-map}
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

#define the rectangular area that will be shown on final map
ybounds <- c(4868850,4869650)
xbounds <- c(279350, 280450)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./fb_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
fb_crop <- "./fb_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
fb_net <- vect("./carrieZigZag/FB_network.shp")

###pour point to define where the watershed boundary is
#manually type coords from windows computer
fb_pour_coords <- data.frame("easting" = 280400,
                             "northing" = 4869120)
#convert to SpatVector object
fb_pour <- vect(fb_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
fb_pour_filename <- "./fb_dems/fb_pour.shp"
#writeVector(fb_pour, fb_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
b_crop <- "./fb_dems/1mdem_crop.tif"

fb_breached <- "./fb_dems/1mdem_breach.tif"
# wbt_breach_depressions_least_cost(
#   dem = fb_crop,
#   output = fb_breached,
#   dist = 1,
#   fill = TRUE)

fb_filled <- "./fb_dems/1mdem_fill.tif"
# wbt_fill_depressions_wang_and_liu(
#   dem = fb_breached,
#   output = fb_filled
# )
#calculate flow accumulation and direction
fb_flowacc <- "./fb_dems/1mdem_fb_flowacc.tif"
# wbt_d8_flow_accumulation(input = fb_filled,
#                          output = fb_flowacc)
# plot(rast(fb_flowacc))
fb_d8pt <- "./fb_dems/1mdem_fb_d8pt.tif"
# wbt_d8_pointer(dem = fb_filled,
#                output = fb_d8pt)
# plot(rast(fb_d8pt))


#delineate streams
fb_streams <- "./fb_dems/fb_streams.tif"
# wbt_extract_streams(flow_accum = fb_flowacc,
#                     output = fb_streams,
#                     threshold = 8000)
# plot(rast(fb_streams))
# points(lcc)
#snap pour point to streams
fb_pour_snap <- "./fb_dems/fb_pour_snap.shp"
# wbt_jenson_snap_pour_points(pour_pts = fb_pour_filename,
#                             streams = fb_streams,
#                             output = fb_pour_snap,
#                             snap_dist = 10)
fb_pour_snap_read <- vect("./fb_dems/fb_pour_snap.shp")

fb_shed <- "./fb_dems/fb_shed.tif"
# wbt_watershed(d8_pntr = fb_d8pt,
#               pour_pts = fb_pour_snap,
#               output = fb_shed)

#convert raster of watershed area to vector for final mapping
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)

#get sensor locations from STIC data, format
locs_fb <- data_23 %>% 
  filter(wshed == "FB") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape_fb <- vect(locs_fb, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc_fb <- terra::project(locs_shape_fb, crs(m1)) %>% 
  left_join(pks_fb, by = "ID")


#assign destination for hillshade calculation
hillshade_out_fb <- "./fb_dems/1mdem_hillshade.tif"
# wbt_hillshade(
#   dem = fb_crop,
#   output = hillshade_out,
# )
hill_fb <- rast(hillshade_out_fb)

#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.
#fb_map <- 
  ggplot()+
  geom_spatraster(data = hill_fb)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c( "gray9", "gray48","lightgray", "white"),
                                   guide = "none")+
    new_scale_fill() +
  #geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = fb_outline, fill = NA, color = "#397367", alpha = 0.3, lwd = 2) +
  geom_sf(data = fb_net, colour = "darkslategray3", lwd = 2) +
    #geom_sf(data = lcc_fb, colour = "midnightblue", pch = 19, size = 2) +
    geom_sf(data = lcc_fb, aes(fill = pk), size = 3, pch = 21) +
  #geom_sf(data = dd, aes(color = (depth)), pch = 19, size = 3) +
  scale_fill_gradient(high = "#27408B", low = "white")+
  #geom_sf(data = fb_pour, colour = "black", pch = 8, size = 3) +
   #scale_fill_hypso_c(palette = "dem_screen", limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))
  

```
```{r ZZ-map}
#map for ZZ
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
#plot(m1)

#get sensor locations from STIC data, format
locs <- data_23 %>% 
  filter(wshed == "ZZ") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc <- terra::project(locs_shape, crs(m1)) %>% 
  left_join(pks_zz, by = "ID")

#plot(lcc)
#define the rectangular area that will be shown on final map
ybounds <- c(4866400,4867500)
xbounds <- c(277200, 277650)
#plot(m1, xlim = xbounds, ylim = ybounds)
#points(lcc)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#plot(crop1)
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./zz_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
zz_crop <- "./zz_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
zz_net <- vect("./carrieZigZag/zigzag_streams.shp")
#plot(zz_net)

###pour point to define where the watershed boundary is
#manually type coords from windows computer


zz_pour_coords <- data.frame("easting" = 277280.45,
                             "northing" = 4867436.45)
#convert to SpatVector object
zz_pour <- vect(zz_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
zz_pour_filename <- "./zz_dems/zz_pour.shp"
#writeVector(zz_pour, zz_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
zz_crop <- "./zz_dems/1mdem_crop.tif"

zz_breached <- "./zz_dems/1mdem_breach.tif"


zz_filled <- "./zz_dems/1mdem_fill.tif"
# wbt_fill_depressions_wang_and_liu(
#   dem = zz_breached,
#   output = zz_filled
# )
#calculate flow accumulation and direction
zz_flowacc <- "./zz_dems/1mdem_zz_flowacc.tif"
# wbt_d8_flow_accumulation(input = zz_filled,
#                          output = zz_flowacc)
#plot(rast(zz_flowacc))
zz_d8pt <- "./zz_dems/1mdem_zz_d8pt.tif"
# wbt_d8_pointer(dem = zz_filled,
#                output = zz_d8pt)
#plot(rast(zz_d8pt))


#delineate streams
zz_streams <- "./zz_dems/zz_streams.tif"
# wbt_extract_streams(flow_accum = zz_flowacc,
#                     output = zz_streams,
#                     threshold = 8000)
# plot(rast(zz_streams))
# points(lcc)
#snap pour point to streams
zz_pour_snap <- "./zz_dems/zz_pour_snap.shp"
# wbt_jenson_snap_pour_points(pour_pts = zz_pour_filename,
#                             streams = zz_streams,
#                             output = zz_pour_snap,
#                             snap_dist = 10)
zz_pour_snap_read <- vect("./zz_dems/zz_pour_snap.shp")
# plot(rast(zz_streams), 
#      xlim = c(280200, 280410),
#      ylim = c(4869300, 4869000))
# points(zz_pour_snap_read, pch = 1)

zz_shed <- "./zz_dems/zz_shed.tif"
# wbt_watershed(d8_pntr = zz_d8pt,
#               pour_pts = zz_pour_snap,
#               output = zz_shed)

#plot(rast(zz_shed))
#convert raster of watershed area to vector for final mapping
zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)
#plot(zz_outline)



#assign destination for hillshade calculation
hillshade_out <- "./zz_dems/1mdem_hillshade.tif"
# wbt_hillshade(
#   dem = zz_crop,
#   output = hillshade_out,
# )
hill <- rast(hillshade_out)
#plot(hill)
#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.
#zz_map <- 
  ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  #theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"),
                       guide = "none")+
    new_scale_fill() +
  #geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2) +
  geom_sf(data = zz_net, colour = "darkslategray3", lwd = 2) +
    #geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 2) +
    geom_sf(data = lcc, aes(fill = pk), size = 3, pch = 21) +
  #geom_sf(data = dd, aes(color = (depth)), pch = 19, size = 3) +
  scale_fill_continuous(high = "#27408B", low = "white",
                        labels = c("0", "0.5", "1"),
                        breaks = c(0, 0.5, 1),
                        limits = c(0, 1))+
  #geom_sf(data = zz_pour, colour = "black", pch = 8, size = 3) +
   #scale_fill_hypso_c(palette = "dem_screen", limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position = "left")+
    labs(fill = str_wrap("Proportion of time flowing", width = 10))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0.2, "cm"), 
                              pad_y = unit(1, "cm"))


```

```{r whole-Valley}
#use HB dem from previous maps, but just don't crop it, include outline of the whole valley, and the outlines of each study shed
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
plot(m1)

#old hillshade
valley_hill <- rast("./HB/1m hydro enforced DEM/wall_hillshade.tif")
#old hillshade has a different angle, emphasizes the lineaments in the east better, but new one looks niver
#see if a newly calculated one looks better
hillshade_out <- "./HB/1m hydro enforced DEM/new_hillshade.tif"
wbt_hillshade(
  dem = dem,
  output = hillshade_out,
)
valley_hill2 <- rast(hillshade_out)
plot(valley_hill2)


#get the outlines for each watershed
#W3
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)
#FB
fb_shed <- "./fb_dems/fb_shed.tif"
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)
#ZZ
zz_shed <- "./zz_dems/zz_shed.tif"
zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)



valley_plot <- ggplot()+
  geom_spatraster(data = valley_hill2, maxcell = 5e+05)+#make max cells bigger on final version
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = m1, alpha = 0.5)+
  geom_sf(data = fb_outline, fill = NA, color = "black", alpha = 0.3, lwd = 1.5) +
  geom_sf(data = w3_outline, fill = NA, color = "black", alpha = 0.3, lwd = 1.5) +
  geom_sf(data = zz_outline, fill = NA, color = "black", alpha = 0.3, lwd = 1.5) +

  #geom_sf(data = fb_net, colour = "darkslategray3") +#, lwd = 3) +
    #geom_sf(data = lcc, colour = "midnightblue") + #, pch = 19, size = 6) +
  #geom_sf(data = fb_pour, colour = "black") + #, pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen")+#, limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0.5, "cm"), 
                              pad_y = unit(0.5, "cm")) #, line_width = 3, text_cex = 5, tick_height = 20)
valley_plot
#ggsave("valley.png", plot = valley_plot, limitsize = FALSE, units = "in", bg = NULL,)

#combine 3 watershed shapes into one layer, and either label or make distinct colors

```
```{r}
HB_bounds <- "./HB/hbef_wsheds/hbef_wsheds.shp"
sheds <- vect(HB_bounds)
sheds <- terra::project(sheds, crs(hill))
plot(sheds)

hb_outline <- aggregate(sheds)


shed_colors <- c("#397367", "#FFD166", "#7E6B8F")

valley_plot <- ggplot()+
  geom_spatraster(data = valley_hill2, maxcell = 5e+05)+#make max cells bigger on final version
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  #geom_spatraster(data = m1, alpha = 0.5)+
  geom_sf(data = hb_outline, fill = NA, color = "black", alpha = 0.6, lwd = 1, lty = 2) +
  geom_sf(data = fb_outline, fill = shed_colors[1], color = shed_colors[1], alpha = 0.6, lwd = 1) +
  geom_sf(data = w3_outline, fill = shed_colors[2], color = shed_colors[2], alpha = 0.6, lwd = 1) +
  geom_sf(data = zz_outline, fill = shed_colors[3], color = shed_colors[3], alpha = 0.6, lwd = 1) +

  #geom_sf(data = fb_net, colour = "darkslategray3") +#, lwd = 3) +
    #geom_sf(data = lcc, colour = "midnightblue") + #, pch = 19, size = 6) +
  #geom_sf(data = fb_pour, colour = "black") + #, pch = 8, size = 3) +
   #scale_fill_hypso_c(palette = "dem_screen")+#, limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(0.5, "cm"))
valley_plot

ggsave("valley.png", plot = valley_plot, limitsize = FALSE, units = "in", bg = NULL,)

```


## Figure 2: Vignettes of network wetting and drying
Chunk below copied from mapsForStoryboard.qmd, from section where I was determining times when the network went from fully dry to fully wet.
```{r W3-flowing-states}
 # Function factory for secondary axis transforms
train_sec <- function(primary, secondary, na.rm = TRUE) {
  # Thanks Henry Holm for including the na.rm argument!
  from <- range(secondary, na.rm = na.rm)
  to   <- range(primary, na.rm = na.rm)
  # Forward transform for the data
  forward <- function(x) {
    rescale(x, from = from, to = to)
  }
  # Reverse transform for the secondary axis
  reverse <- function(x) {
    rescale(x, from = to, to = from)
  }
  list(fwd = forward, rev = reverse)
}

start <- ymd_hms("2023-10-20 00:00:00")
# stop <- ymd_hms("2023-10-22 00:00:00")
stop <- ymd_hms("2023-10-24 00:00:00")

#add precipitation
#add hydrograph to plot
q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

    



#precip <- 
  read_csv("./HB/dailyWatershedPrecip1956-2024.csv") %>% 
    filter(watershed == "W3") %>% 
        filter(DATE >= start & DATE <= stop) %>% 
    ggplot()+
    geom_bar(aes(x = DATE, y = Precip), stat = "identity")

precip1 <- read_csv("./HB/HBEF_W3precipitation_15min.csv") %>% 
    filter(DateTime >= start & DateTime <= stop) %>% 
    mutate(day = day(DateTime),
           hour = hour(DateTime)) %>% 
    group_by(day, hour) %>% 
    reframe(hourly_precip = sum(precip), across()) %>% 
    mutate(mins = minute(DateTime)) %>% 
    filter(mins == 0) %>%
  rename("datetime" = DateTime) %>% 
    left_join(q_23_plotting, by = "datetime") %>% 
  select(datetime, day, hour, hourly_precip, Q_mm_day)


sec3 <- with(precip1, train_sec(hourly_precip, Q_mm_day))


water <- precip1 %>% 
    ggplot(aes(x = datetime))+
    geom_col(aes(y = hourly_precip))+
    geom_line(aes(y = sec3$fwd(Q_mm_day)), colour = "blue") +
    scale_y_continuous("Hourly Precipitation (mm)",
                       sec.axis = sec_axis(~sec3$rev(.),
                       name = "Discharge (mm/day)"))+
    labs(x = "",
         y = "Precipitation (mm)")+
      theme_classic()+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y = element_text(angle = 0),
        axis.title.y.right = element_text(angle = 0)
        )


test_IDS <- input_w3 %>% 
    filter(datetime > start & datetime < stop)

remainder <- test_IDS$ID

pks_ordered <- pks_w3 %>% 
filter(ID %in% remainder) %>% 
  arrange(desc(pk)) %>% 
  rowid_to_column("pk_order")

test <- input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
sec2 <- with(test, train_sec(pk_order, pk))

test <- 
  input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
    mutate(day = day(datetime),
           hour = hour(datetime)) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(precip1, by = c("day", "hour")) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
  #filter(mins.x == 0)
test2 <- test %>% 
  select(datetime.x, binary, hourly_precip, pk_order)
sec2 <- with(test2, train_sec(pk_order, hourly_precip))

binary <- c("#DB995A",
            "#586BA4"
  )


tiles <- 
test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = pk_order, fill = as.character(binary)))+
  scale_y_continuous("Supposed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                    values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23",
                              "10/23/23",
                              "10/24/23"))+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 0))

(water)/tiles + plot_layout(heights = c(1, 5))


test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = (pk_order), fill = as.character(binary)))+
  geom_col(aes(y = sec2$fwd(hourly_precip)))+
  ylab("Supposed activation order")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$fwd(.), name = "Instantaneous Q (mm/day)")) +
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(title = "Very dry to very wet",
       subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23",
                              "10/23/23",
                              "10/24/23"))+
  theme_classic()+
    theme(legend.position="right")


#different order, not based on Pk but the actual sequence that they activate in
#determine what the activation order is
pks_ordered <- 
input_w3 %>% 
    filter(ID %in% remainder) %>% 
    filter(datetime > start & datetime < stop,
           binary == 1) %>% 
    group_by(ID) %>% 
    summarise(start_of_flow = min(datetime)) %>% 
    arrange(start_of_flow) %>% 
  rowid_to_column("pk_order") %>% 
  select(ID, pk_order) %>% unique()

test <- input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
sec2 <- with(test, train_sec(pk_order, pk))

test <- 
  input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
    mutate(day = day(datetime),
           hour = hour(datetime)) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(precip1, by = c("day", "hour")) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
  #filter(mins.x == 0)
test2 <- test %>% 
  select(datetime.x, binary, hourly_precip, pk_order)
sec2 <- with(test2, train_sec(pk_order, hourly_precip))

binary <- c("#DB995A",
            "#586BA4"
  )


tiles <- 
test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = pk_order, fill = as.character(binary)))+
  scale_y_continuous("Supposed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                    values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23",
                              "10/23/23",
                              "10/24/23"))+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 0))

(water)/tiles + plot_layout(heights = c(1, 5))
```
```{r}
pks_ordered <- rename(pks_ordered, sequence = pk_order)
calc_model_result(pks_ordered$ID, "W3")
produce_metrics(pks_ordered, "W3", "one_event")
```

Plot it one more time, but maybe with sequence dictated by graph theory solution?
```{r}
pks_ordered <- 
W3_cheap %>% 
      filter(ID %in% remainder) %>% 
  rename("pk_order" = sequence)

test <- 
  input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
    mutate(day = day(datetime),
           hour = hour(datetime)) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(precip1, by = c("day", "hour")) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
  #filter(mins.x == 0)
test2 <- test %>% 
  select(datetime.x, binary, hourly_precip, pk_order)
sec2 <- with(test2, train_sec(pk_order, hourly_precip))

binary <- c("#DB995A",
            "#586BA4"
  )


tiles <- 
test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = pk_order, fill = as.character(binary)))+
  scale_y_continuous("Supposed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                    values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23",
                              "10/23/23",
                              "10/24/23"))+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 0))

(water)/tiles + plot_layout(heights = c(1, 5))
```

```{r W3-map-panels}
#create a template for maps, then plot the flowing state at different timesteps on the template
#map of watershed 3 with depth to bedrock
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
#hill <- rast(hillshade_out)

#dem
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
#crop to rectangular area
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#writeRaster(crop1, "1mdemw3_cropped.tif")

#watershed boundary
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)

#w3 network- thing I need to change
#read in shapefile of stream converted in ARC
vect_stream_path <- "./AGU24posterAnalysis/vector_stream/vector_stream.shp"
#stream as a vector
vect_stream <- vect(vect_stream_path)
#plot(vect_stream)
#crop to watershed boundary
w3_stream_crop <- crop(vect_stream, w3_outline)
#plot(w3_stream_crop)
#or i could use old classification

#point locations- snapped points from above chunk
w3_stic_locs_snap <- "w3_stic_locs_snap.shp"

w3_stic_locs_r <- vect(w3_stic_locs_snap) %>% 
  left_join(pks_w3, by = "ID")



w3_stic_locs_r <- vect(w3_stic_locs_snap)
#writeVector(w3_stic_locs_r, "./seismic_map_exports/w3_stic_locs_snap.shp")


w3_net <- vect("./carrieZigZag/w3_network.shp")
#writeVector(w3_net, "./seismic_map_exports/network.shp")

#plot(w3_net)


#test <- 
ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "darkslategray3", lwd = 2) +
  geom_sf(data = w3_stic_locs_r, colour = "midnightblue", size = 2) +
  #geom_sf(data = dd, aes(color = (depth)), pch = 19, size = 3) +
  scale_color_gradient(low = "black", high = "white")+
  #geom_sf(data = w3_pour, colour = "black") +
   scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))
  
# determine the flowing state
doi <- c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00"))
  
states <- input_w3 %>% 
    filter(datetime %in% doi[5])

#w3_stic_locs_r %>% left_join(states, by = "ID")

ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
   
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(binary)), size = 4, pch = 21) +
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0, "cm"),
                              pad_y = unit(1, "cm"))

```


## Figure 5: Distributions of proportion of transitions that follow a sequence
Distributions of prop values, with dashed lines for median, faceted by timescale and watershed, colored by method for determining sequence
```{r combine-and-plot}
four_colors <- c("#231f20", "#bb4430", "#7ebdc2", "#f3dfa2")

rbind(all_graph_solutions,
      all_pk,
      all_twi) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    #geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Distributions of Proportion of time Sequence Followed",
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = four_colors,
                     name = "Method")+
  scale_color_manual(values = four_colors,
                     name = "Method")+
  facet_grid(shed~timescale)
```
```{r combine-and-plot-two}



#fixing randomly generated sequence
fixed_combined <- 
rbind(all_pk,
      all_twi, 
      all_new,
      #all_conc,
      all_random
      ) 

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")
five_colors <- c("#d68c45",  "#247BA0", "#A30B37", "#F0C808", "#2c6e49")
#fixing names for legend, and factor levels

fixed_combined %>% 
  ungroup() %>% 
  # mutate(method = case_when(method == "random" ~ "Random",
  #                           method == "Flow Permanence" ~ "Proportion of Time Flowing",
  #                           method == "cheapest_insertion" ~ "TSP Solution",
  #                           method == "Topographic Wetness Index" ~ "Topographic Wetness Index")) %>%
  mutate(timescale = case_when(timescale == "30mins" ~ "30 Minute Frequency",
                               timescale == "daily" ~ "Daily Average Flow State")) %>%
  mutate(shed = fct_relevel(shed,
                              c("W3", "FB", "ZZ"))) %>%
  # mutate(method = fct_relevel(method,
  #                             c("TSP Solution",  "Proportion of Time Flowing", "Random",
  #                                "Topographic Wetness Index"))) %>%
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    stat_central_tendency(aes(color = method), type = "median", linetype = "dashed")+
    #geom_density(alpha = 0.5, lty = 3)+
     # geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_classic2()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Proportion of Transitions where Sequence is Followed",
       x = "Proportion of transitions that are sequential",
       y = "Density")+
  scale_fill_manual(values = four_colors,
                     name = "Method")+
  scale_color_manual(values = four_colors,
                     name = "Method")+
  scale_linetype_manual(values = 2, name = "Median")+
  scale_x_continuous(labels = c("0", "0.25", "0.5", "0.75", "1"), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(panel.spacing = unit(1, "lines"))+
  facet_grid(shed~timescale)
```

```{r combine-and-plot-two}
#fixing randomly generated sequence
fixed_combined2 <- 
rbind(general_graph(W3_IDs, "W3", methods = c("random"), two_opt = FALSE),
      general_graph(W3_IDs, "W3", methods = c("cheapest_insertion"), two_opt = TRUE),
      all_pk,
      all_twi,
      mutate(calc_props(opt_routes, "W3"), method = "concorde")
      ) %>% 
  filter(shed == "W3")

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2", "green")

#fixing names for legend, and factor levels

fixed_combined2 %>% 
  ungroup() %>% 
  # mutate(method = case_when(method == "random" ~ "Random",
  #                           method == "Flow Permanence" ~ "Proportion of Time Flowing",
  #                           method == "cheapest_insertion" ~ "TSP Solution",
  #                           method == "Topographic Wetness Index" ~ "Topographic Wetness Index")) %>% 
  # mutate(timescale = case_when(timescale == "30mins" ~ "30 Minute Frequency",
  #                              timescale == "daily" ~ "Daily Average Flow State")) %>% 
  # mutate(method = fct_relevel(method, 
  #                             c("TSP Solution",  "Proportion of Time Flowing", "Random",
  #                                "Topographic Wetness Index", "concorde"))) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(color = method, fill = method), alpha = 0.5)+
    stat_central_tendency(aes(color = method), type = "median", linetype = "dashed")+
    #geom_density(alpha = 0.5, lty = 3)+
     # geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_classic2()+
  #ylim(c(0, 6.25))+
  xlim(c(0,1))+
  labs(title = "Proportion of Transitions where Sequence is Followed",
       x = "Proportion of transitions that are sequential",
       y = "Density")+
  scale_fill_manual(values = four_colors,
                     name = "Method")+
  scale_color_manual(values = four_colors,
                     name = "Method")+
  scale_linetype_manual(values = 2, name = "Median")+
  scale_x_continuous(labels = c("0", "0.25", "0.5", "0.75", "1"), expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))+
  theme(panel.spacing = unit(1, "lines"))+
  facet_grid(shed~timescale)

fixefixefixed_combined %>% 
  filter(timescale == "30mins", 
         shed == "W3",
         method %in% c("cheapest_insertion", "Flow Permanence")) %>% print(n = 50)
```

Can also calculate proportion of sequential behavior through time! Comes out looking better than modeling using Botter & Durighetto

```{r calculate-how-many-follow-sequence}
#sequential <- 
  input_w3 %>% 
  left_join(pks_w3, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed.x, wshed.y, mins, datetime)) %>% 
  arrange(desc(pk)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>%
  filter(datetime == ymd_hms("2023-07-03 17:00:00")) %>% print(n = 25)
  filter(following != -1) %>% 
    summarise(count_non = length(following))

total <- input_w3 %>% 
  left_join(pks_w3, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed.x, wshed.y, mins, datetime)) %>% 
  arrange(desc(pk)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
    summarise(count_all = length(following))

final <- left_join(sequential, total, by = "datetime") %>% 
  mutate(prop = count_non/count_all)

final %>% 
  mutate(diff = count_all - count_non) %>% 
  summarise(sum(diff),
            sum(count_all))

```

Old method to calculate how well sequences worked:
Once sequence is defined, I can compare to the actual flow data
```{r}
#sequential <- 
input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  arrange((sequence)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  #filter(following != -1) %>% 
  filter(datetime == "2023-07-15 19:00:00") %>% View()
   # summarise(count_non = length(following)) %>% View()
    
input_w3 %>% 
  left_join(W3_pk_seq, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins)) %>% 
  arrange((sequence)) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  #filter(following != -1) %>% 
  filter(datetime == "2023-07-15 19:00:00") %>% View()
    summarise(count_non = length(following))

#make a df with ID and the number in the sequence
W3_pk_seq <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, length(pks_w3$ID), 1)) %>% 
  select(ID, sequence)
  

total <- input_w3 %>% 
  left_join(W3_cheap, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins, datetime)) %>% 
  arrange(sequence) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
    summarise(count_all = length(following))

final <- left_join(sequential, total, by = "datetime") %>% 
  mutate(prop = count_non/count_all)

#write a function to calculate this in 1 quick step
calc_time <- function(sequence_df, shed, seq_label){
  
  if(shed == "W3"){
    input_f <- input_w3
  } else if(shed == "FB"){
        input_f <- input_fb
  } else if(shed == "ZZ"){
        input_f <- input_zz
  }
  
sequential <- input_f %>% 
  left_join(sequence_df, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins, datetime)) %>% 
  arrange(sequence) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
  filter(following != -1) %>% 
    summarise(count_non = length(following))

total <- input_f %>% 
  left_join(sequence_df, by = "ID") %>%
  group_by(datetime) %>% 
  select(-c(wshed, mins, datetime)) %>% 
  arrange(sequence) %>% 
  mutate("downstream" = lag(binary),
         "following" = downstream - binary) %>% 
    summarise(count_all = length(following))

final <- left_join(sequential, total, by = "datetime") %>% 
  mutate(prop = count_non/count_all) %>% 
  mutate(shed = shed)

output <- q_24_bind %>% 
  inner_join(final, by = "datetime") %>% 
  mutate(seq_label = seq_label)

return(output)
}

calc_time(W3_cheap, "W3", "cheapest")
```
```{r plot-along-discharge}

rbind(
calc_time(W3_cheap, "W3", "cheapest"),
calc_time(W3_random, "W3", "random"),
calc_time(w3_twi_sequence, "W3", "twi"),
calc_time(W3_pk_seq, "W3", "pk")
) %>% 
  ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line(aes(color = prop))+
  labs(title = "Discharge from W3, 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()+
  scale_color_continuous(type = "viridis",
                             limits = c(0,1))+
                       theme(rect = element_rect(fill = "transparent", color = NA))+
  facet_grid(seq_label~shed)

```
```{r difference-plot}
pk_props_join <- calc_time(W3_pk_seq, "W3", "pk") %>% 
  select(datetime, prop) %>% 
  rename("pk_prop" = prop)

rbind(
calc_time(W3_cheap, "W3", "cheapest"),
calc_time(W3_random, "W3", "random"),
calc_time(w3_twi_sequence, "W3", "twi"),
calc_time(W3_pk_seq, "W3", "pk")
) %>% 
  left_join(pk_props_join, by = "datetime") %>% 
  filter(seq_label != "pk") %>% 
  mutate(diff = prop - pk_prop) %>% 
  ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line(aes(color = diff))+
  labs(title = "Difference between Flow permanence sequence and others, 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()+
  scale_color_gradient2(low = "red", 
    mid = "white", 
    high = "blue", 
    midpoint = 0,
    limits = c(-0.3, 0.3))+
                       theme(rect = element_rect(fill = "transparent", color = NA))+
  facet_grid(seq_label~shed)
```

New idea: bar graphs, where the height of the bar represents the proportion of sequential transitions
```{r}
#calculate the total number of timesteps, and how many contain a transition in state?


test <- input_w3 %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) #%>% 
  #filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change))

length(test$datetime)

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change)) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count, fill = state_change))+
  geom_histogram(stat = "identity")

#changing_dates <- 
  test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "none") #%>% select(datetime) %>% unique() %>% 
  mutate(status = "changing")

`%not_in%` <- purrr::negate(`%in%`)

#bar of total timesteps
test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates) %>% 
  group_by(status) %>% 
  summarise(count = length(status)) %>% 
  mutate(shed = "W3") %>% 
  ggplot(aes(x = shed, y = count, fill = status))+
  geom_histogram(stat = "identity")
  
test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "none") %>% unique() %>% 
  group_by(state_change) %>% 
  summarise(timesteps = length(datetime))

test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
  summarise(length(datetime))

test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "wetting",
         state_change != "drying") %>% unique() %>% 
  group_by(state_change) %>% 
  summarise(timesteps = length(datetime))

summarise(how_many = length(unique(state_change))) %>% 
  ungroup() %>% 
  filter(how_many > 1)
```

```{r define-functions}
filtered_input <- input_w3 %>%
      filter(mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
#calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
i <- 1
child <- paste0("r_",routes_w3$up[i])
parent <- paste0("r_",routes_w3$down[i])
input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(child, parent)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      #filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()

      #View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

# if(check <= 2){
#   sequence_df <- data.frame("Sequence" = NA, 
#                             "Frequency" = NA,
#                             "up" = up,
#                             "down" = down)
#   return(sequence_df)
# } 
# else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
#output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
#supports <- c("0001","0111","1101", "0100")


#sub <- filter(sequence_df, Sequence %in% supports)
#output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)
}


#test function

calc_support_combos("r_18", "r_11", input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
#iterate_groups_combos <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

i <- 4
up <- paste0("r_",routes_w3$up[i])
down <- paste0("r_",routes_w3$down[i])
timestep <- minutes(30)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
    print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support_combos(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

iterate_groups_combos("r_13", "r_19", input, minutes(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_combos <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  #out <- iterate_groups_combos(up, down, input, timestep)
    out <- calc_support_combos(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down, Sequence) %>%
    summarise(Frequency = sum(Frequency))
  return(final_output)
}
test <- calculate_hierarchy_combos(routes_w3, filtered_input, minutes(30))

test %>% 
  group_by(Sequence) %>% 
  summarise(sum(Frequency))

supports <- c("0001","0111","1101", "0100")

test %>% ungroup() %>% 
  filter(Sequence %in% supports) %>% 
  summarise(sum(Frequency))

test %>% ungroup() %>% 
  #filter(Sequence %in% supports) %>% 
  summarise(sum(Frequency))

#create a key for wetting and drying
wetting <- c("0001", "0010", "0011", "0111", "1011")
drying <- c("0100", "1000", "1100", "1101", "1110")
no_net_change <- c("0000", "0101", "0110", "1001", "1010", "1111")
#create key for sequential and non
sequential <- c("0001", "0100", "0111", "1101")
nonsequential <- c("0010", "1000", "1011", "1110")
undeterminable <- c("0011","0110", "1001","1100")
no_change2 <- c("0000", "1111", "0101", "1010")

test2 <- test %>% 
  #mutate(Sequence = as.numeric(Sequence)) %>% 
  mutate(status = case_when(Sequence %in% wetting ~ "wetting",
                            Sequence %in% drying ~ "drying",
                            Sequence %in% no_net_change ~ "no_net_change"),
         category = case_when(Sequence %in% sequential ~ "sequential",
                              Sequence %in% nonsequential ~ "nonsequential",
                              Sequence %in% undeterminable ~ "undeterminable",
                              Sequence %in% no_change2 ~ "no_change")) %>% 
  group_by(status, category) %>% 
  summarise(count = sum(Frequency))

test2 %>% ungroup() %>% 
  group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count, fill = category))+
  geom_histogram(stat = "identity")

#second plot- what proportion of transitions are sequential or not
#ultimately one of these for each sequence?
test2 %>% ungroup() %>% 
  filter(category != "no_change") %>% 
  group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count, fill = category))+
  geom_histogram(stat = "identity")


#fantastic four function has been modified, to do average daily state. Removed hourly and 4 hr time blocks
fantastic_four_combos <- function(routes, shed){
  theFour <- c("30mins", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed) %>% 
      mutate(
             "day" = day(datetime),
             "month" = month(datetime),
             "year" = year(datetime)) %>% 
      group_by(day, month, year, ID) %>%
      summarise(avg_state = mean(binary)) %>% 
      ungroup() %>% 
      mutate(avg_state = round(avg_state)) %>% 
      rename("binary" = avg_state) %>% 
      mutate("datetime" = ymd_hms(paste0(year,"-",month,"-",day," ","00:00:00"))) %>% 
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
          arrange(datetime) %>% 
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_combos(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}


fantastic_four_combos(routes_w3, "W3")
# function to determine proportion of transitions that state changes follow all parent child relationships
calc_props_silly <- function(routes, shed){
  full_combos <- routes
total_state_changes <- full_combos %>% 
    filter(Sequence %not_in% c(undeterminable, no_change2)) %>% 
    group_by(up, down) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- full_combos %>% 
    filter(Sequence %not_in% c(undeterminable, no_change2)) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
return(un_split)
}

calc_props_silly(test, "W3")
```

Final first graph
```{r W3}
#final first graph
test <- input_w3 %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) #%>% 
  #filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change))

length(test$datetime)

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change)) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count, fill = state_change))+
  geom_histogram(stat = "identity")

changing_dates <- 
  test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "none") %>% select(datetime) %>% unique() %>% 
  mutate(status = "changing")
  
  length(changing_dates$datetime)/48
  length(unique(input_w3$datetime))/48

`%not_in%` <- purrr::negate(`%in%`)

w3_wetness_changing <- test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates)

#bar of total timesteps
w3_bar_ready <- test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates) %>% 
  group_by(status) %>% 
  summarise(count = length(status)) %>% 
  mutate(shed = "W3")
  #ungroup() %>% 

w3_bar_ready

w3_bar_ready %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = status))+
  geom_histogram(stat = "identity")

#one more thing, looking at the accuracy during times of transition
make_change <- test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates)

df_summary_time_w3 %>% 
  pivot_longer(-datetime) %>% 
left_join(make_change, by = "datetime") %>% 
  group_by(name, status) %>% 
  summarise(average_acc = mean(value),
            sd(value))
```
```{r FB}
#final first graph
test <- input_fb %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) #%>% 
  #filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change))

length(test$datetime)

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change)) %>% 
  mutate(shed = "FB" ) %>% 
  ggplot(aes(x = shed, y = count, fill = state_change))+
  geom_histogram(stat = "identity")

changing_dates <- 
  test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "none") %>% select(datetime) %>% unique() %>% 
  mutate(status = "changing")
  
#  length(changing_dates$datetime)/48
#  length(unique(input_w3$datetime))/48

#bar of total timesteps
fb_bar_ready <- test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates) %>% 
  group_by(status) %>% 
  summarise(count = length(status)) %>% 
  mutate(shed = "FB")

fb_bar_ready %>% 
  #ungroup() %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = status))+
  geom_histogram(stat = "identity")
```
```{r ZZ}
#final first graph
test <- input_zz %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) #%>% 
  #filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change))

length(test$datetime)

test %>% ungroup() %>% 
  group_by(state_change) %>% 
  summarise(count = length(state_change)) %>% 
  mutate(shed = "ZZ" ) %>% 
  ggplot(aes(x = shed, y = count, fill = state_change))+
  geom_histogram(stat = "identity")

changing_dates <- 
  test %>% 
  ungroup() %>% 
  select(datetime, state_change) %>% 
  group_by(datetime) %>% 
  filter(state_change != "none") %>% select(datetime) %>% unique() %>% 
  mutate(status = "changing")
  
#  length(changing_dates$datetime)/48
#  length(unique(input_w3$datetime))/48

#bar of total timesteps
zz_bar_ready <- test %>% 
  ungroup() %>% 
  select(datetime) %>% unique() %>% 
filter(datetime %not_in% changing_dates$datetime) %>% 
  mutate(status = "no change") %>% 
  bind_rows(changing_dates) %>% 
  group_by(status) %>% 
  summarise(count = length(status)) %>% 
  mutate(shed = "ZZ")

zz_bar_ready %>% 
  #ungroup() %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = status))+
  geom_histogram(stat = "identity")
```

Final second graph
```{r}
filtered_input <- input_w3 %>%
      filter(mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
calc_support_combos <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 1
# child <- paste0("r_",routes_w3$up[i])
# parent <- paste0("r_",routes_w3$down[i])
# input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(child, parent)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      #filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()

      #View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

# if(check <= 2){
#   sequence_df <- data.frame("Sequence" = NA, 
#                             "Frequency" = NA,
#                             "up" = up,
#                             "down" = down)
#   return(sequence_df)
# } 
# else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")


sequence_df$up <- up
sequence_df$down <- down
#output$total <- sum(sequence_df$Frequency)
#write some way to score the sequence_df
#award one point for one of these configs:
#supports <- c("0001","0111","1101", "0100")


#sub <- filter(sequence_df, Sequence %in% supports)
#output$points <- sum(sub$Frequency)


#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(sequence_df)
}


#test function

#calc_support_combos("r_18", "r_11", input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_combos <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes_w3$up[i])
# down <- paste0("r_",routes_w3$down[i])
# timestep <- minutes(30)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
    print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- calc_support_combos(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

#iterate_groups_combos("r_13", "r_19", input, minutes(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_combos <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  #out <- iterate_groups_combos(up, down, input, timestep)
    out <- calc_support_combos(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    group_by(up, down, Sequence) %>%
    summarise(Frequency = sum(Frequency))
  return(final_output)
}

test <- calculate_hierarchy_combos(routes_w3, filtered_input, minutes(30))
calc_props_silly(test, "W3")$prop %>% hist()


#create a key for wetting and drying
wetting <- c("0001", "0010", "0011", "0111", "1011")
drying <- c("0100", "1000", "1100", "1101", "1110")
no_net_change <- c("0000", "0101", "0110", "1001", "1010", "1111")
#create key for sequential and non
sequential <- c("0001", "0100", "0111", "1101")
nonsequential <- c("0010", "1000", "1011", "1110")
undeterminable <- c("0011","0110", "1001","1100")
no_change2 <- c("0000", "1111", "0101", "1010")

test2 <- test %>% 
  #mutate(Sequence = as.numeric(Sequence)) %>% 
  mutate(status = case_when(Sequence %in% wetting ~ "wetting",
                            Sequence %in% drying ~ "drying",
                            Sequence %in% no_net_change ~ "no_net_change"),
         category = case_when(Sequence %in% sequential ~ "sequential",
                              Sequence %in% nonsequential ~ "nonsequential",
                              Sequence %in% undeterminable ~ "undeterminable",
                              Sequence %in% no_change2 ~ "no_change")) %>% 
  group_by(status, category) %>% 
  summarise(count = sum(Frequency))

# test2 %>% ungroup() %>% 
#   group_by(category) %>% 
#   mutate(shed = "W3" ) %>% 
#   ggplot(aes(x = shed, y = count, fill = category))+
#   geom_histogram(stat = "identity")

#second plot- what proportion of transitions are sequential or not
#ultimately one of these for each sequence?
test2 %>% ungroup() %>% 
  filter(category != "no_change") %>% 
  #group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = category))+
  geom_histogram(stat = "identity")


#do it again, but for twi
routes_w3_twi

hist_prep <- function(routes) {
  test <- calculate_hierarchy_combos(routes, filtered_input, minutes(30))


#create a key for wetting and drying
wetting <- c("0001", "0010", "0011", "0111", "1011")
drying <- c("0100", "1000", "1100", "1101", "1110")
no_net_change <- c("0000", "0101", "0110", "1001", "1010", "1111")
#create key for sequential and non
sequential <- c("0001", "0100", "0111", "1101")
nonsequential <- c("0010", "1000", "1011", "1110")
undeterminable <- c("0011","0110", "1001","1100")
no_change2 <- c("0000", "1111", "0101", "1010")

test2 <- test %>% 
  #mutate(Sequence = as.numeric(Sequence)) %>% 
  mutate(status = case_when(Sequence %in% wetting ~ "wetting",
                            Sequence %in% drying ~ "drying",
                            Sequence %in% no_net_change ~ "no_net_change"),
         category = case_when(Sequence %in% sequential ~ "sequential",
                              Sequence %in% nonsequential ~ "nonsequential",
                              Sequence %in% undeterminable ~ "undeterminable",
                              Sequence %in% no_change2 ~ "no_change")) %>% 
  group_by(status, category) %>% 
  summarise(count = sum(Frequency))

return(test2)
}

hist_prep(routes_w3_twi)

hist_prep(routes_w3_twi) %>%
  ungroup() %>% 
  filter(category != "no_change") %>% 
  #group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = category))+
  geom_histogram(stat = "identity")

calc_props_silly(calculate_hierarchy_combos(routes_w3_twi, filtered_input, minutes(30)), "W3")$prop %>% hist()

#again for graph theory
W3_cheap
W3_random

routes_w3_cheap <- W3_cheap %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID)  %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

hist_prep(routes_w3_cheap) %>%
  ungroup() %>% 
  filter(category != "no_change") %>% 
  #group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = category))+
  geom_histogram(stat = "identity")

calc_props_silly(calculate_hierarchy_combos(routes_w3_cheap, filtered_input, minutes(30)), "W3")$prop %>% hist()


#again for random sequence
routes_w3_random <- W3_random %>% 
    filter(ID %in% W3_IDs) %>% 
  rename("up" = ID)  %>% 
  mutate(down = lag(up)) %>% 
   drop_na() %>% 
  select(up, down) 

hist_prep(routes_w3_random) %>%
  ungroup() %>% 
  filter(category != "no_change") %>% 
  #group_by(category) %>% 
  mutate(shed = "W3" ) %>% 
  ggplot(aes(x = shed, y = count/sum(count), fill = category))+
  geom_histogram(stat = "identity")
```


### New graph
Plot a grid, of sequential wetting and drying, "the full graph"
```{r}
W3_all_combos %>% 
  ggplot(aes(x = down, y = up, fill = prop))+
    geom_tile(color="white") +
    scale_fill_viridis_c(limits=c(0,1)) +
    labs(x="Parent", y="Child", fill="P(i before j)", title="Event precedence matrix") +
    theme_minimal()

FB_all_combos %>% 
  ggplot(aes(x = down, y = up, fill = prop))+
    geom_tile(color="white") +
    scale_fill_viridis_c(limits=c(0,1)) +
    labs(x="Parent", y="Child", fill="P(i before j)", title="Event precedence matrix") +
    theme_minimal()

ZZ_all_combos %>% 
  ggplot(aes(x = down, y = up, fill = prop))+
    geom_tile(color="white") +
    scale_fill_viridis_c(limits=c(0,1)) +
    labs(x="Parent", y="Child", fill="P(i before j)", title="Event precedence matrix") +
    theme_minimal()

```
Make these plots again, but order them by proportion of time flowing sequence

```{r}
w3_pk_seq2 <- 
  pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, 26, 1))

#w3_grid <- 
  W3_all_combos %>% 
    filter(prop > 0.5) %>% 
  mutate(up = as.numeric(substr(up, 3,4)),
         down = as.numeric(substr(down, 3,4))) %>% 
  filter(timescale == "30mins") %>% 
  select(up, down, prop) %>% 
    ungroup() %>% 
  rename(ID = up) %>% 
  left_join(w3_pk_seq2, by = "ID") %>% 
  rename(child = ID,
         ID = down,
         child_sequence = sequence) %>% 
  left_join(w3_pk_seq2, by = "ID") %>% 
  rename(parent = ID, 
         parent_sequence = sequence) %>% 
  select(child, parent, child_sequence, parent_sequence, prop) %>% 
  ggplot(aes(x = parent_sequence, y = child_sequence, fill = prop))+
    geom_tile(color="white") +
    scale_fill_viridis_c(limits=c(0,1)) +
    labs(x="Parent", y="Child", fill="Prop", title="W3 sequential behavior matrix") +
    theme_minimal()+
  theme(legend.position = "none")
  
####### FB
fb_pk_seq2 <- 
  pks_fb %>% 
    filter(ID %in% FB_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, 24, 1))

fb_grid <- FB_all_combos %>% 
  mutate(up = as.numeric(substr(up, 3,4)),
         down = as.numeric(substr(down, 3,4))) %>% 
  filter(timescale == "30mins") %>% 
  select(up, down, prop) %>% 
    ungroup() %>% 
  rename(ID = up) %>% 
  left_join(fb_pk_seq2, by = "ID") %>% 
  rename(child = ID,
         ID = down,
         child_sequence = sequence) %>% 
  left_join(fb_pk_seq2, by = "ID") %>% 
  rename(parent = ID, 
         parent_sequence = sequence) %>% 
  select(child, parent, child_sequence, parent_sequence, prop) %>% 
  ggplot(aes(x = parent_sequence, y = child_sequence, fill = prop))+
    geom_tile(color="white") +
    scale_fill_viridis_c(limits=c(0,1)) +
    labs(x="Parent", y="Child", fill="Prop", title="FB sequential behavior matrix") +
    theme_minimal()+
  theme(legend.position = "none")


###### ZZ
zz_pk_seq2 <- 
  pks_zz %>% 
    filter(ID %in% ZZ_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(sequence = seq(1, 24, 1))

zz_grid <- ZZ_all_combos %>% 
  mutate(up = as.numeric(substr(up, 3,4)),
         down = as.numeric(substr(down, 3,4))) %>% 
  filter(timescale == "30mins") %>% 
  select(up, down, prop) %>% 
    ungroup() %>% 
  rename(ID = up) %>% 
  left_join(zz_pk_seq2, by = "ID") %>% 
  rename(child = ID,
         ID = down,
         child_sequence = sequence) %>% 
  left_join(zz_pk_seq2, by = "ID") %>% 
  rename(parent = ID, 
         parent_sequence = sequence) %>% 
  select(child, parent, child_sequence, parent_sequence, prop) %>% 
  ggplot(aes(x = parent_sequence, y = child_sequence, fill = prop))+
    geom_tile(color="white") +
    scale_fill_viridis_c(limits=c(0,1)) +
    labs(x="Parent", y="Child", fill="Prop", title="ZZ sequential behavior matrix") +
    theme_minimal()

w3_grid + fb_grid + zz_grid
```

## Figure 6: Chains
Plot that will show multiple chains beside each other, and highlight where they are different
```{r}
renum_w3_pk <- W3_pk_seq %>% 
  mutate(method = "Proportion of Time Flowing", x = 1) %>% 
  filter(ID %in% new_seq_w3$down) %>% 
  mutate(sequence = seq(1, 25, 1))

color_key <- renum_w3_pk %>% 
  select(ID, sequence) %>% 
  rename(color_ID = sequence)
  
renum_w3_pk <- renum_w3_pk %>% left_join(color_key, by = "ID")

renum_w3_twi <- w3_twi_sequence %>% 
  mutate(method = "Topographic Wetness Index", x = 3) %>% 
  filter(ID %in% new_seq_w3$down) %>% 
  mutate(sequence = seq(1, 25, 1)) %>% 
  left_join(color_key, by = "ID")

renum_w3_new <- convert_to_IDseq(new_seq_w3) %>% 
  mutate(method = "Average Sequential Wetting", x = 2) %>% 
  left_join(color_key, by = "ID")

#Make dataframe to contain the locations of points
rbind(#mutate(W3_cheap, method = "cheapest_insertion",x = 1),
      renum_w3_pk,
      #mutate(convert_to_IDseq(W3_random), method = "random", x = 3),
       renum_w3_new,
      renum_w3_twi
     
      #mutate(convert_to_IDseq(opt_routes_w3), method = "con", x = 6)
      ) %>% 
  mutate(#method = str_wrap(method, 10),
         method = fct_relevel(method, c("Proportion of Time Flowing",
                                "Average Sequential Wetting",
                                "Topographic Wetness Index"))
         ) %>% 
  ggplot()+
  geom_point(aes(x = method, y = sequence, color = color_ID), size = 5)+
  geom_text(aes(x = method, y = sequence, label = ID), color = "white", size = 2)+
  scale_x_discrete(labels = function(method) str_wrap(method, width = 10))+
  scale_color_viridis_c(guide = "none")+
  theme_classic()+
  labs(y = "Sequence", x = "")
```


## Figure 7: Hydrograph, colored by which sequence works the best
NEED TO FIX COLORS
```{r}
five_colors <- c("#d68c45",  "#247BA0", "#A30B37", "#F0C808", "#2c6e49")


#plot the best at each timestep
#p_load(ggbreak)
df <- df_long_w3 %>% 
  filter(model != "random_accuracy",
         model != "cheap_accuracy",
         model != "con_accuracy") %>% 
  group_by(datetime) %>% 
  filter(accuracy == max(accuracy)) %>% 
  mutate(dup = as.character(n() > 1)) %>% ungroup()  %>%
  mutate(unique_sol = case_when(dup == TRUE ~ "z_tie",
                                dup == FALSE ~ model)) %>%
inner_join(big_comb, by = "datetime") #%>%

df_acc <- df %>%
  dplyr::group_by(datetime) %>%
  dplyr::summarise(
    accuracy = unique(accuracy),
    unique_sol = unique(unique_sol),
    .groups = "drop"
  )


ggplot(df, aes(x = datetime, y = Q_mm_day, color = unique_sol)) +
  geom_point(size = 1) +
  theme_classic() +
  scale_fill_manual(values = c(five_colors[1:3], "lightgrey")) +
  scale_color_manual(values = c(five_colors[1:3], "lightgrey")) +
  labs(
    #title = "Most accurate model along hydrograph",
    y = "Discharge (mm/day)",
    x = ""
  ) +
  scale_x_break(c(ymd_hms("2023-11-12 00:00:00"),
                  ymd_hms("2024-05-20 00:00:00"))) +
  scale_x_datetime(
    date_labels = "%b %Y",   # or "%b %Y" for shorter labels
    date_breaks = "1 month",    # adjust to taste
    timezone = "UTC"
  )+ theme(#legend.position = "bottom",
           axis.ticks.x.top = element_blank(),
        axis.text.x.top  = element_blank(),
        axis.line.x.top = element_blank())




p2 <- ggplot(df_acc, aes(x = datetime, y = accuracy, fill = unique_sol)) +
  geom_col(width = 1800, position = "identity") +
  theme_classic() +
  labs(y = "Accuracy", x = NULL) +
  scale_fill_manual(values = c(five_colors[1:3], "lightgrey")) +
  scale_color_manual(values = c(five_colors[1:3], "lightgrey")) +
  scale_x_break(c(ymd_hms("2023-11-1200:00:00"),
                  ymd_hms("2024-05-20 00:00:00"))) +
  scale_x_datetime(
    date_labels = "%b %Y",
    date_breaks = "1 month",
    timezone = "UTC"
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.line.x.top = element_blank()#,
    #legend.position = "none"   # legend will come from bottom plot
  )

      

# stack top (accuracy bars) and bottom (hydrograph)
(free(p2, type = "label") / p1) + plot_layout(heights = c(1, 3) , guides = "collect")

(p2 / p1) +
  plot_layout(heights = c(1, 3), guides = "collect") &
  theme(legend.position = "right")

######
####### Plot that has a venn diagram of colors to show which ties are which;
#almost all of the time the ties are between pk and new, almost no ties are with TWI
df_long_w3 %>% 
  filter(model != "random_accuracy",
         model != "cheap_accuracy",
         model != "con_accuracy") %>% 
  group_by(datetime) %>% 
  filter(accuracy == max(accuracy)) %>% 
  summarise(
    accuracy = unique(accuracy),  # should be only one per datetime
    models = paste(sort(unique(model)), collapse = ", "),
    n_models = n()
  ) %>%
  ungroup() %>% 
  mutate(color = case_when(models == "new_accuracy" ~ "red",
                           models == "pk_accuracy" ~ "blue",
                           models == "twi_accuracy" ~ "yellow",
                           models == "new_accuracy, pk_accuracy" ~ "purple",
                           models == "new_accuracy, twi_accuracy" ~ "orange",
                           models == "pk_accuracy, twi_accuracy" ~ "green",
                           models == "new_accuracy, pk_accuracy, twi_accuracy" ~ "black"))%>%
inner_join(big_comb, by = "datetime") %>%
  ggplot(aes(x = datetime, y = Q_mm_day, color = color)) +
  #geom_point() +
  geom_point(size = 1)+
  theme_classic()+
scale_color_identity()+
  labs(title = "Most accurate model along hydrograph", y = "Discharge (mm/day)", x = "")+
 # facet_wrap(~ unique_sol)+
    scale_x_break(c(ymd_hms("2023-11-12 00:00:00"), ymd_hms("2024-05-20 00:00:00")))

```

New version, where the difference in accuracy has to be greater than some threshold
```{r}


#plot the best at each timestep
#df <- 
  df_long_w3 %>% 
  filter(model != "random_accuracy",
         model != "cheap_accuracy",
         model != "con_accuracy",
         model != "twi_accuracy") %>% 
  group_by(datetime) %>% 
  mutate(max_accuracy = max(accuracy),
         diff_in_acc = max_accuracy - accuracy) %>% 
  filter(diff_in_acc >= 0.05) #%>%
  mutate(dup = as.character(n() > 1)) %>% ungroup()  %>%
  mutate(unique_sol = case_when(dup == TRUE ~ "z_tie",
                                dup == FALSE ~ model)) %>%
inner_join(big_comb, by = "datetime") #%>%
  
  library(dplyr)
threshold <- 0.05
#df_best <- 
  df_long_w3 %>% 
  filter(model != "random_accuracy",
         model != "cheap_accuracy",
         model != "con_accuracy")  %>% #View()
    # your 0.05 rule
  group_by(datetime) %>%
  summarise(
    top_acc   = max(accuracy, na.rm = TRUE),
    n_top     = sum(accuracy >= top_acc - 1e-12),            # how many hit the top
    top_model = model[which.max(accuracy)][1],
    accs      = list(sort(unique(accuracy), decreasing = TRUE)),
    .groups   = "drop"
  ) %>%
  rowwise() %>%
  mutate(
    second_acc = ifelse(length(accs) >= 2, accs[2], -Inf),   # second distinct max
    margin     = top_acc - second_acc,
    winner     = ifelse(n_top == 1 & margin >= threshold, top_model, "tie")
  ) %>%
  ungroup() %>%
  select(datetime, winner, max_acc = top_acc) %>% 
    inner_join(big_comb, by = "datetime") %>%
ggplot(aes(x = datetime, y = log(Q_mm_day), color = winner)) +
  geom_point(size = 1) +
  theme_classic() +
  scale_fill_manual(values = c("#d68c45", "#247BA0", "lightgrey",  "#2c6e49")) +
  scale_color_manual(values = c("#d68c45", "#247BA0", "lightgrey",  "#2c6e49")) +
  labs(
    title = "Model at least 5% more accurate than other models, W3, Log Y",
    y = "Discharge (mm/day)",
    x = ""
  ) +
  scale_x_break(c(ymd_hms("2023-11-12 00:00:00"),
                  ymd_hms("2024-05-20 00:00:00"))) +
  scale_x_datetime(
    date_labels = "%b %Y",   # or "%b %Y" for shorter labels
    date_breaks = "1 month",    # adjust to taste
    timezone = "UTC"
  )+ theme(#legend.position = "bottom",
           axis.ticks.x.top = element_blank(),
        axis.text.x.top  = element_blank(),
        axis.line.x.top = element_blank())

```



FB
```{r}
all_fb_Q <- all_fb_Q %>% 
  rename(datetime = DATETIME,
         Q_mm_day = Q)

  threshold <- 0.05
#df_best <- 
  df_long_fb %>% 
  filter(model != "random_accuracy",
         model != "cheap_accuracy",
         model != "con_accuracy")  %>% #View()
    # your 0.05 rule
  group_by(datetime) %>%
  summarise(
    top_acc   = max(accuracy, na.rm = TRUE),
    n_top     = sum(accuracy >= top_acc - 1e-12),            # how many hit the top
    top_model = model[which.max(accuracy)][1],
    accs      = list(sort(unique(accuracy), decreasing = TRUE)),
    .groups   = "drop"
  ) %>%
  rowwise() %>%
  mutate(
    second_acc = ifelse(length(accs) >= 2, accs[2], -Inf),   # second distinct max
    margin     = top_acc - second_acc,
    winner     = ifelse(n_top == 1 & margin >= threshold, top_model, "tie")
  ) %>%
  ungroup() %>%
  select(datetime, winner, max_acc = top_acc) %>% 
    inner_join(all_fb_Q, by = "datetime") %>%
ggplot(aes(x = datetime, y = (Q_mm_day), color = winner)) +
  geom_point(size = 1) +
  theme_classic() +
  scale_fill_manual(values = c("#d68c45", "#247BA0", "lightgrey",  "#2c6e49")) +
  scale_color_manual(values = c("#d68c45", "#247BA0", "lightgrey",  "#2c6e49")) +
  labs(
    title = "Model at least 5% more accurate than other models, FB",
    y = "Discharge (L/s)",
    x = ""
  ) +
  scale_x_break(c(ymd_hms("2023-11-12 00:00:00"),
                  ymd_hms("2024-05-20 00:00:00"))) +
  scale_x_datetime(
    date_labels = "%b %Y",   # or "%b %Y" for shorter labels
    date_breaks = "1 month",    # adjust to taste
    timezone = "UTC"
  )+ theme(#legend.position = "bottom",
           axis.ticks.x.top = element_blank(),
        axis.text.x.top  = element_blank(),
        axis.line.x.top = element_blank())
  
# to convert from L/s to mm/day, I need to know the watershed area of FB
```


## Figure 8: Maps of where sequences work in space
Set of map panels where symbol color = flowing state, outline color or shape indicating correctly predicted or not

For these plots I need:
- actual flowing state
- map template
- predicted state based on a sequence

Run lines 2815 and beyond
```{r model_predictions}
cheap_model <- calc_model_result(W3_cheap$ID) %>% rename("cheapest" = pred_binary)
pk_model <- calc_model_result(W3_pk_seq$ID) %>% rename("pk" = pred_binary)
random_model <- calc_model_result(W3_random$ID) %>% rename("random" = pred_binary)
twi_model <- calc_model_result(w3_twi_sequence$ID) %>% rename("twi" = pred_binary)

comparison <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>% 
  left_join(cheap_model, by = c("datetime", "ID")) %>% 
  left_join(pk_model, by = c("datetime", "ID")) %>% 
    left_join(random_model, by = c("datetime", "ID")) %>% 
  left_join(twi_model, by = c("datetime", "ID")) %>% 
  select(-wshed, -mins) %>% 
  drop_na()

#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
get_eval_label(0,0)
#apply get_eval function to each model run
comparison$cheapest_eval <- mapply(get_eval_label, comparison$binary, comparison$cheapest)
comparison$pk_eval <- mapply(get_eval_label, comparison$binary, comparison$pk)
comparison$random_eval <- mapply(get_eval_label, comparison$binary, comparison$random)
comparison$twi_eval <- mapply(get_eval_label, comparison$binary, comparison$twi)

```
```{r sample-based-on-discharge}
#select random timesteps evenly distributed along flow duration curve
q_23_bind <- 
  q_23_f %>% 
  select(DATETIME, Q_mm_day) %>% 
  rename("datetime" = DATETIME
         )

q_24_bind <- 
  q_24_f %>% 
  select(datetime, Q_mmperday) %>% 
  rename("Q_mm_day" = Q_mmperday)

big_comb <- rbind(q_23_bind, q_24_bind) #%>% 
  inner_join(comparison, by = "datetime")
```
### W3
From earlier, contains map template and actual flowing state
```{r W3-map-panels}
#create a template for maps, then plot the flowing state at different timesteps on the template
#map of watershed 3 with depth to bedrock
hillshade_out <- "./w3_dems/1mdem_hillshade.tif"
hill <- rast(hillshade_out)

#dem
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

ybounds <- c(4870350,4871350)
xbounds <- c(281350, 282150)
#crop to rectangular area
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#writeRaster(crop1, "1mdemw3_cropped.tif")

#watershed boundary
w3_shed <- "./w3_dems/w3_shed.tif"
w3_outline <- as.polygons(rast(w3_shed), extent=FALSE)

#w3 network- thing I need to change
#read in shapefile of stream converted in ARC
vect_stream_path <- "./AGU24posterAnalysis/vector_stream/vector_stream.shp"
#stream as a vector
vect_stream <- vect(vect_stream_path)
#plot(vect_stream)
#crop to watershed boundary
w3_stream_crop <- crop(vect_stream, w3_outline)
#plot(w3_stream_crop)
#or i could use old classification

#point locations- snapped points from above chunk
w3_stic_locs_snap <- "w3_stic_locs_snap.shp"

w3_stic_locs_r <- vect(w3_stic_locs_snap) %>% 
  left_join(pks_w3, by = "ID")



w3_stic_locs_r <- vect(w3_stic_locs_snap)
#writeVector(w3_stic_locs_r, "./seismic_map_exports/w3_stic_locs_snap.shp")


w3_net <- vect("./carrieZigZag/w3_network.shp")
#writeVector(w3_net, "./seismic_map_exports/network.shp")

#plot(w3_net)


#test <- 
ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "darkslategray3", lwd = 2) +
  geom_sf(data = w3_stic_locs_r, colour = "midnightblue", size = 2) +
  #geom_sf(data = dd, aes(color = (depth)), pch = 19, size = 3) +
  scale_color_gradient(low = "black", high = "white")+
  #geom_sf(data = w3_pour, colour = "black") +
   scale_fill_hypso_c(palette = "dem_screen" , limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))
  
# determine the flowing state
doi <- c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00"),
                            ymd_hms("2023-10-23 00:00:00"),
                            ymd_hms("2023-10-24 00:00:00"))
#instead of big_comb, need something that has the flowing state of and model accuracy
states <- big_comb %>% 
    filter(datetime %in% doi[1])

states <- comparison_w3 %>% 
  filter(datetime %in% doi[1])
#w3_stic_locs_r %>% left_join(states, by = "ID")

#figure without scale bar and legends
ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(binary),
            color = cheapest_eval), size = 4, pch = 21, stroke = 2) +
  scale_color_manual(values = c("orange", "black", "limegreen"),
                     name = "")+
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position = "")
  # ggspatial::annotation_scale(location = 'tr', pad_x = unit(0, "cm"),
  #                             pad_y = unit(1, "cm"))
#figure with scale bar and legends
ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(binary),
            color = cheapest_eval), size = 4, pch = 21) +
  scale_color_manual(values = c("orange", "black", "limegreen"),
                     name = "")+
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0, "cm"),
                              pad_y = unit(1, "cm"))

```
```{r}
#make the same thing as above, but pivot longer and then plot using facets
#states <- 
states <- comparison_w3 %>% 
    select(-c(cheapest, random)) %>% 
    filter(datetime %in% doi) %>% 
  pivot_longer(cols = c(pk, twi),
               values_to = "prediction", 
               names_to = "sequence")

states$eval <- mapply(get_eval_label, states$binary, states$prediction)

ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 1)+
  geom_sf(data = w3_net, colour = "grey", lwd = 0.8) +
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(prediction),
            color = eval), size = 2, stroke = 1, pch = 21) +
  scale_color_manual(values = c("green", "black", "orange"),
                     name = "")+
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position = "")+
  facet_grid(sequence~datetime)

```

```{r}
#take map panels from above, and plot for times of interest, where one sequence works better than another... why?
threshold <- 0.05
#df_best <- 
t <-   df_long_w3 %>% 
  filter(model != "random_accuracy",
          #model != "new_accuracy",
          model != "twi_accuracy",
         model != "cheap_accuracy",
         model != "con_accuracy"
         )  %>% #View()
    # your 0.05 rule
  group_by(datetime) %>%
  summarise(
    top_acc   = max(accuracy, na.rm = TRUE),
    n_top     = sum(accuracy >= top_acc - 1e-12),            # how many hit the top
    top_model = model[which.max(accuracy)][1],
    accs      = list(sort(unique(accuracy), decreasing = TRUE)),
    .groups   = "drop"
  ) %>%
  rowwise() %>%
  mutate(
    second_acc = ifelse(length(accs) >= 2, accs[2], -Inf),   # second distinct max
    margin     = top_acc - second_acc,
    winner     = ifelse(n_top == 1 & margin >= threshold, top_model, "tie")
  ) %>%
  ungroup() %>%
  select(datetime, winner, max_acc = top_acc) %>% 
    inner_join(big_comb, by = "datetime") %>%
ggplot(aes(x = datetime, y = (Q_mm_day), color = winner)) +
  geom_point(size = 1) +
  theme_classic() +
  scale_fill_manual(values = c("#d68c45", "#247BA0", "lightgrey",  "#2c6e49")) +
  scale_color_manual(values = c("#d68c45", "#247BA0","lightgrey",  "#2c6e49")) +
  labs(
    title = "Model at least 5% more accurate than other models, W3",
    y = "Discharge (mm/day)",
    x = ""
  ) +
 scale_x_break(c(ymd_hms("2023-11-12 00:00:00"),
                ymd_hms("2024-05-20 00:00:00"))) +
  scale_x_datetime(
    date_labels = "%b %Y",   # or "%b %Y" for shorter labels
    date_breaks = "1 month",    # adjust to taste
    timezone = "UTC"
  )+ theme(#legend.position = "bottom",
           axis.ticks.x.top = element_blank(),
        axis.text.x.top  = element_blank(),
        axis.line.x.top = element_blank())

t
# determine the flowing state
doi <- c(ymd_hms("2023-09-03 10:00:00"),
                            ymd_hms("2023-09-04 10:00:00"),
                            ymd_hms("2023-09-05 10:00:00"))#,
                            #ymd_hms("2023-10-23 00:00:00"),
                            #ymd_hms("2023-10-24 00:00:00"))
doi <- c(ymd_hms("2023-11-04 07:00:00"),
                            ymd_hms("2023-11-04 20:00:00"),
                            ymd_hms("2023-11-04 21:00:00"),
                            ymd_hms("2023-11-04 22:00:00"),
                            ymd_hms("2023-11-05 04:00:00"))
doi <- c(ymd_hms("2024-06-22 01:30:00"),
                            ymd_hms("2024-06-22 05:30:00"),
                            ymd_hms("2024-06-22 09:30:00"),
                            ymd_hms("2024-06-22 13:30:00"),
                            ymd_hms("2024-06-22 17:30:00"))

t + xlim(doi[1], doi[5])

states <- comparison_w3 %>% 
    select(-c(cheapest, random)) %>% 
    filter(datetime %in% doi) %>% 
  pivot_longer(cols = c(pk, twi, new),
               values_to = "prediction", 
               names_to = "sequence")

states$eval <- mapply(get_eval_label, states$binary, states$prediction)

ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 1)+
  geom_sf(data = w3_net, colour = "grey", lwd = 0.8) +
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(prediction)
            ,
            color = eval
            ), size = 2, stroke = 1, pch = 21) +
  scale_color_manual(values = c("green", "black", "orange"),
                     name = "")+
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position = "")+
  facet_grid(sequence~datetime)

```


```{r}
#make a mini bar plot to show the flowing accuracy at each time step
maps <- ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 1)+
  geom_sf(data = w3_net, colour = "grey", lwd = 0.8) +
geom_sf(data = w3_stic_locs_r %>% inner_join(states, by = "ID"), 
        aes(fill = as.character(prediction),
            color = eval), size = 2, stroke = 1, pch = 21) +
  scale_color_manual(values = c("green", "black", "orange"),
                     name = "")+
  scale_fill_manual(values = c("white", "black"),
                    labels = c("dry", "flowing"),
                    name = "")+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position = "")
# +
#   facet_grid(sequence~datetime)

bars <- ggplot(filter(states, datetime == doi[1]), aes(y = datetime, fill = eval)) + 
  geom_bar()+
    theme_void()+
  scale_fill_manual(values = c("green", "black", "orange"))+
  theme(
        legend.position = "")
# +
#   facet_grid(sequence~datetime)

(maps/(bars)) +
  plot_layout(widths = c(1, 1), heights = unit(c(12, 1), c('cm', 'null')))


```
Map where the points are colored by accuracy, and faceted by hydrograph component and sequence
```{r W3-map}
#summarize through time
ID_acc <- comparison_w3 %>%
    inner_join(discharge_df, by = "datetime") %>% 
  group_by(ID, event_type) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total
  ) %>%
  select(ID, event_type, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy,pk_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy")


#figure with scale bar and legends
ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(ID_acc, by = "ID"), 
        aes(fill = accuracy, shape = event_type), size = 4) +
  # scale_color_manual(values = c("orange", "black", "limegreen"),
  #                    name = "")+
  scale_fill_continuous(type = "viridis",
                             limits = c(0,1))+
  scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(0, "cm"),
                              pad_y = unit(1, "cm"))+
  facet_grid(event_type ~ model)

#another version, where the points maintain their colors from earlier, but accuracy is illustrated using size or alpha
ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(ID_acc, by = "ID"), 
        aes(alpha = accuracy, shape = event_type, fill = model), size = 4) +
  scale_fill_manual(values = four_colors,
                     name = "")+
  scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid(event_type ~ model)

## Plot that is just a single row
ID_acc_w3 <- comparison_w3 %>%
    #inner_join(fb_limbs, by = "datetime") %>% 
  group_by(ID) %>%
  summarize(
    #cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
    total = n(),
    #cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct/total
  ) %>%
  select(ID, new_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(new_accuracy,pk_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy")

ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(ID_acc_w3, by = "ID"), 
        aes(alpha = accuracy, fill = model), size = 4, pch = 21) +
  scale_fill_manual(values = four_colors,
                     name = "")+
  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid( ~ model)
```

```{r W3-one-row}
ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 2)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(ID_acc_w3, by = "ID"), 
        aes(fill = accuracy), size = 4, pch = 21) +
  scale_fill_gradientn(colors = c("white","lightgrey","darkgrey", "black"))+
  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid( ~ model)
```

### FB
Make map template:
```{r FB-map}
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)

#define the rectangular area that will be shown on final map
ybounds <- c(4868850,4869650)
xbounds <- c(279350, 280450)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
#points(lcc)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./fb_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
fb_crop <- "./fb_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
fb_net <- vect("./carrieZigZag/FB_network.shp")

###pour point to define where the watershed boundary is
#manually type coords from windows computer
fb_pour_coords <- data.frame("easting" = 280400,
                             "northing" = 4869120)
#convert to SpatVector object
fb_pour <- vect(fb_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
fb_pour_filename <- "./fb_dems/fb_pour.shp"
#writeVector(fb_pour, fb_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
b_crop <- "./fb_dems/1mdem_crop.tif"

fb_breached <- "./fb_dems/1mdem_breach.tif"
# wbt_breach_depressions_least_cost(
#   dem = fb_crop,
#   output = fb_breached,
#   dist = 1,
#   fill = TRUE)

fb_filled <- "./fb_dems/1mdem_fill.tif"
# wbt_fill_depressions_wang_and_liu(
#   dem = fb_breached,
#   output = fb_filled
# )
#calculate flow accumulation and direction
fb_flowacc <- "./fb_dems/1mdem_fb_flowacc.tif"
# wbt_d8_flow_accumulation(input = fb_filled,
#                          output = fb_flowacc)
# plot(rast(fb_flowacc))
fb_d8pt <- "./fb_dems/1mdem_fb_d8pt.tif"
# wbt_d8_pointer(dem = fb_filled,
#                output = fb_d8pt)
# plot(rast(fb_d8pt))


#delineate streams
fb_streams <- "./fb_dems/fb_streams.tif"
# wbt_extract_streams(flow_accum = fb_flowacc,
#                     output = fb_streams,
#                     threshold = 8000)
# plot(rast(fb_streams))
# points(lcc)
#snap pour point to streams
fb_pour_snap <- "./fb_dems/fb_pour_snap.shp"
# wbt_jenson_snap_pour_points(pour_pts = fb_pour_filename,
#                             streams = fb_streams,
#                             output = fb_pour_snap,
#                             snap_dist = 10)
fb_pour_snap_read <- vect("./fb_dems/fb_pour_snap.shp")

fb_shed <- "./fb_dems/fb_shed.tif"
# wbt_watershed(d8_pntr = fb_d8pt,
#               pour_pts = fb_pour_snap,
#               output = fb_shed)

#convert raster of watershed area to vector for final mapping
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)

#get sensor locations from STIC data, format
locs_fb <- data_23 %>% 
  filter(wshed == "FB") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape_fb <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc_fb <- terra::project(locs_shape_fb, crs(m1))

#assign destination for hillshade calculation
hillshade_out <- "./fb_dems/1mdem_hillshade.tif"
# wbt_hillshade(
#   dem = fb_crop,
#   output = hillshade_out,
# )
hill <- rast(hillshade_out)

#also clip and mask shapefiles
fb_outline <- as.polygons(rast(fb_shed), extent=FALSE)
fb_net <- vect("./carrieZigZag/FB_network.shp")
plot(ext(fb_outline))

plot(maptools::elide(sf::st_as_sf(fb_outline), 90), add = TRUE)
s <- sf::st_as_sf(v)

plot(ext(fb_net), add = TRUE)
plot(fb_net, add = TRUE)
plot(fb_outline, add = TRUE)
plot(centroids(vect(ext(fb_net))))
unname(ext(fb_outline)[1:4])

polygon_cropped <- crop(fb_net, unname(ext(fb_outline)[1:4]))
plot(ext(polygon_cropped), add = TRUE, col = "red")

fb_net <- terra::crop(fb_net, ext(fb_outline))

plot(ext(fb_net), add = TRUE, col = "red")


rescale()

plot(ext(fb_net))
plot(fb_net, add = TRUE)
#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.

  
#copy and pasted from W3 chunk
  #summarize through time
ID_acc_fb <- comparison_fb %>%
    inner_join(fb_limbs, by = "datetime") %>% 
  group_by(ID, event_type) %>%
  summarize(
    cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    total = n(),
    cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total
  ) %>%
  select(ID, event_type, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy,pk_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy")

#figure with scale bar and legends

#another version, where the points maintain their colors from earlier, but accuracy is illustrated using size or alpha
ggplot()+
  theme_void()+
  geom_sf(data = fb_outline, fill = NA, color = "#397367", alpha = 0.3, lwd = 2)+
  geom_sf(data = fb_net, colour = "grey", lwd = 1) +
geom_sf(data = locs_shape %>% inner_join(ID_acc_fb, by = "ID"), 
        aes(alpha = accuracy, shape = event_type, fill = model), size = 4) +
  scale_fill_manual(values = four_colors,
                     name = "")+
  scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid(event_type ~ model)


## Plot that is just a single row
ID_acc_fb <- comparison_fb %>%
    #inner_join(fb_limbs, by = "datetime") %>% 
  group_by(ID) %>%
  summarize(
    #cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
    total = n(),
    #cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct / total
  ) %>%
  select(ID, pk_accuracy, random_accuracy, twi_accuracy, new_accuracy) %>%
  tidyr::pivot_longer(cols = c(pk_accuracy, random_accuracy, twi_accuracy, new_accuracy), 
                      names_to = "model", values_to = "accuracy")

ggplot()+
  theme_void()+
  geom_sf(data = fb_outline, fill = NA, color = "#397367", alpha = 0.3, lwd = 2)+
  geom_sf(data = fb_net, colour = "grey", lwd = 1) +
geom_sf(data = locs_shape %>% inner_join(ID_acc_fb, by = "ID"), 
        aes(alpha = accuracy, fill = model), size = 4, pch = 21) +
  scale_fill_manual(values = four_colors,
                     name = "")+
  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid( ~ model)

```
```{r final-map-one-row}

#terra::spin(fb_outline, 90, 279933.5, 4869173)
ggplot()+
  theme_void()+
  geom_sf(data = terra::spin(fb_outline, 90, 279933.5, 4869173), fill = NA, color = "#397367", alpha = 0.3, lwd = 2)+
  geom_sf(data = terra::spin(fb_net, 90, 279933.5, 4869173), colour = "grey", lwd = 1) +
geom_sf(data =  terra::spin(lcc_fb, 90, 279933.5, 4869173) %>% inner_join(ID_acc_fb, by = "ID"),
        aes(fill = accuracy), size = 4, pch = 21) +
     scale_fill_gradient(low = "white", high = "black")+

  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA)) +
  facet_grid( ~ model)
```


### ZZ
```{r ZZ-map}
#map for ZZ
#read in DEM of whole valley, 1m resolution
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
#plot(m1)

#get sensor locations from STIC data, format
locs_zz <- data_23 %>% 
  filter(wshed == "ZZ") %>% 
  select(ID, lat, long) %>% 
  unique()
#convert STIC data to a SpatVector data format
locs_shape_zz <- vect(locs_zz, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
#plot(locs_shape)
#reproject coordinates from WGS84 to NAD83 19N, which is the projection of raster
lcc_zz <- terra::project(locs_shape_zz, crs(m1))
#plot(lcc)
#define the rectangular area that will be shown on final map
ybounds <- c(4866400,4867500)
xbounds <- c(277200, 277650)
#plot(m1, xlim = xbounds, ylim = ybounds)
#points(lcc)

#create a SpatExtent from a vector (length=4; order=xmin, xmax, ymin, ymax)
crop1 <- crop(m1, ext(c(xbounds, ybounds)))
#plot(crop1)
#save cropped 1m dem to reduce processing time below, and gurantee that everything has the same extent
#writeRaster(crop1, "./zz_dems/1mdem_crop.tif", overwrite = TRUE)
#read in cropped dem
zz_crop <- "./zz_dems/1mdem_crop.tif"

#read in shapefile of stream network shape from ARC file on windows computer
zz_net <- vect("./carrieZigZag/zigzag_streams.shp")
#plot(zz_net)

###pour point to define where the watershed boundary is
#manually type coords from windows computer


zz_pour_coords <- data.frame("easting" = 277280.45,
                             "northing" = 4867436.45)
#convert to SpatVector object
zz_pour <- vect(zz_pour_coords,
                geom = c("easting", "northing"),
                   crs = crs(m1))
#snap pour point to make sure it lies on flowlines
#fb_pour <- snap(fb_pour, fb_net, tol = 1)

#save to file for use in whitebox functions
zz_pour_filename <- "./zz_dems/zz_pour.shp"
#writeVector(zz_pour, zz_pour_filename, overwrite=TRUE)

####delineate watershed and keep watershed boundary
#breach and fill I guess
zz_crop <- "./zz_dems/1mdem_crop.tif"

zz_breached <- "./zz_dems/1mdem_breach.tif"


zz_filled <- "./zz_dems/1mdem_fill.tif"
# wbt_fill_depressions_wang_and_liu(
#   dem = zz_breached,
#   output = zz_filled
# )
#calculate flow accumulation and direction
zz_flowacc <- "./zz_dems/1mdem_zz_flowacc.tif"
# wbt_d8_flow_accumulation(input = zz_filled,
#                          output = zz_flowacc)
#plot(rast(zz_flowacc))
zz_d8pt <- "./zz_dems/1mdem_zz_d8pt.tif"
# wbt_d8_pointer(dem = zz_filled,
#                output = zz_d8pt)
#plot(rast(zz_d8pt))


#delineate streams
zz_streams <- "./zz_dems/zz_streams.tif"
# wbt_extract_streams(flow_accum = zz_flowacc,
#                     output = zz_streams,
#                     threshold = 8000)
# plot(rast(zz_streams))
# points(lcc)
#snap pour point to streams
zz_pour_snap <- "./zz_dems/zz_pour_snap.shp"
# wbt_jenson_snap_pour_points(pour_pts = zz_pour_filename,
#                             streams = zz_streams,
#                             output = zz_pour_snap,
#                             snap_dist = 10)
zz_pour_snap_read <- vect("./zz_dems/zz_pour_snap.shp")
# plot(rast(zz_streams), 
#      xlim = c(280200, 280410),
#      ylim = c(4869300, 4869000))
# points(zz_pour_snap_read, pch = 1)

zz_shed <- "./zz_dems/zz_shed.tif"
# wbt_watershed(d8_pntr = zz_d8pt,
#               pour_pts = zz_pour_snap,
#               output = zz_shed)

#plot(rast(zz_shed))
#convert raster of watershed area to vector for final mapping
zz_outline <- as.polygons(rast(zz_shed), extent=FALSE)
#plot(zz_outline)



#assign destination for hillshade calculation
hillshade_out <- "./zz_dems/1mdem_hillshade.tif"
# wbt_hillshade(
#   dem = zz_crop,
#   output = hillshade_out,
# )
hill <- rast(hillshade_out)
#plot(hill)

centroids(vect(ext(zz_net)))

#final plot with cropped hillshade and dem, STIC locations, watershed boundary, and stream network.
#zz_map <- 
  ggplot()+
  geom_spatraster(data = hill)+
  theme_void()+
  theme(legend.position = "")+
  scale_fill_gradientn(colors = c("black", "gray9", "gray48","lightgray", "white"))+
    new_scale_fill() +
  geom_spatraster(data = crop1, alpha = 0.5)+
    geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2) +
  geom_sf(data = zz_net, colour = "darkslategray3", lwd = 2) +
    geom_sf(data = lcc, colour = "midnightblue", pch = 19, size = 2) +
  #geom_sf(data = zz_pour, colour = "black", pch = 8, size = 3) +
   scale_fill_hypso_c(palette = "dem_screen", limits = c(200, 1000))+
  theme(rect = element_rect(fill = "transparent", color = NA))+
  ggspatial::annotation_scale(location = 'tr', pad_x = unit(1, "cm"), 
                              pad_y = unit(1, "cm"))

#copy and pasted from W3 chunk
  #summarize through time
# ID_acc_zz <- comparison_zz %>%
#     inner_join(zz_limbs, by = "datetime") %>% 
#   group_by(ID, event_type) %>%
#   summarize(
#     cheap_correct = sum(cheapest_eval == "correct"),
#     pk_correct = sum(pk_eval == "correct"),
#     random_correct = sum(random_eval == "correct"),
#     twi_correct = sum(twi_eval == "correct"),
#     total = n(),
#     cheap_accuracy = cheap_correct / total,
#     pk_accuracy = pk_correct / total,
#     random_accuracy = random_correct / total,
#     twi_accuracy = twi_correct / total
#   ) %>%
#   select(ID, event_type, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
#   tidyr::pivot_longer(cols = c(cheap_accuracy,pk_accuracy, random_accuracy, twi_accuracy), 
#                       names_to = "model", values_to = "accuracy")

#figure with scale bar and legends

#another version, where the points maintain their colors from earlier, but accuracy is illustrated using size or alpha
# ggplot()+
#   theme_void()+
#   geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2)+
#   geom_sf(data = zz_net, colour = "grey", lwd = 1) +
# geom_sf(data = locs_shape %>% inner_join(ID_acc_zz, by = "ID"), 
#         aes(alpha = accuracy, shape = event_type, fill = model), size = 4) +
#   scale_fill_manual(values = four_colors,
#                      name = "")+
#   scale_shape_manual(values = c(21, 24, 22))+
#   theme(rect = element_rect(fill = "transparent", color = NA))+#,
#         #legend.position = "")+
#   facet_grid(event_type ~ model)


## Plot that is just a single row
ID_acc_zz <- comparison_zz %>%
    #inner_join(fb_limbs, by = "datetime") %>% 
  group_by(ID) %>%
  summarize(
    #cheap_correct = sum(cheapest_eval == "correct"),
    pk_correct = sum(pk_eval == "correct"),
    random_correct = sum(random_eval == "correct"),
    twi_correct = sum(twi_eval == "correct"),
    new_correct = sum(new_eval == "correct"),
    total = n(),
    #cheap_accuracy = cheap_correct / total,
    pk_accuracy = pk_correct / total,
    random_accuracy = random_correct / total,
    twi_accuracy = twi_correct / total,
    new_accuracy = new_correct / total
  ) %>%
  select(ID,  pk_accuracy, random_accuracy, twi_accuracy, new_accuracy) %>%
  tidyr::pivot_longer(cols = c(pk_accuracy, random_accuracy, twi_accuracy, new_accuracy), 
                      names_to = "model", values_to = "accuracy")

ggplot()+
  theme_void()+
  geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2)+
  geom_sf(data = zz_net, colour = "grey", lwd = 1) +
geom_sf(data = locs_shape %>% inner_join(ID_acc_zz, by = "ID"), 
        aes(alpha = accuracy, fill = model), size = 4, pch = 21) +
  scale_fill_manual(values = four_colors,
                     name = "")+
  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid( ~ model)

```
```{r ZZ-one-row}
ggplot()+
  theme_void()+
  geom_sf(data = zz_outline, fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2)+
  geom_sf(data = zz_net, colour = "grey", lwd = 1) +
geom_sf(data = locs_shape %>% inner_join(ID_acc_zz, by = "ID"), 
        aes(fill = accuracy), size = 4, pch = 21) +
    scale_fill_gradient(low = "white", high = "black")+

  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA))+#,
        #legend.position = "")+
  facet_grid( ~ model)

ggplot()+
  theme_void()+
  geom_sf(data = terra::spin(zz_outline, 180, 277401.8, 4867071), fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 2)+
  geom_sf(data = terra::spin(zz_net, 180, 277401.8, 4867071), colour = "grey", lwd = 1) +
geom_sf(data =  terra::spin(lcc_zz, 180, 277401.8, 4867071) %>% inner_join(ID_acc_fb, by = "ID"),
        aes(fill = accuracy), size = 4, pch = 21) +
     scale_fill_gradient(low = "white", high = "black")+

  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA)) +
  facet_grid( ~ model)
```
### Combine all
```{r final-combined-plot}
#combine all of the spun plots from before
#W3
W3_rot <- ggplot()+
  theme_void()+
  geom_sf(data = w3_outline, fill = NA, color = "#FFD166", alpha = 0.3, lwd = 1)+
  geom_sf(data = w3_net, colour = "grey", lwd = 1) +
geom_sf(data = w3_stic_locs_r %>% inner_join(ID_acc_w3, by = "ID"), 
        aes(fill = accuracy), size = 3, pch = 21) +
  scale_fill_gradientn(colors = c("white","lightgrey","darkgrey", "black"))+
  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position = "none",
        strip.text.x = element_blank())+
  facet_grid( ~ model)
W3_rot
#FB
FB_rot <- ggplot()+
  theme_void()+
  geom_sf(data = terra::spin(fb_outline, 90, 279933.5, 4869173), fill = NA, color = "#397367", alpha = 0.3, lwd = 1)+
  geom_sf(data = terra::spin(fb_net, 90, 279933.5, 4869173), colour = "grey", lwd = 1) +
geom_sf(data =  terra::spin(lcc_fb, 90, 279933.5, 4869173) %>% inner_join(ID_acc_fb, by = "ID"),
        aes(fill = accuracy), size = 3, pch = 21) +
  scale_fill_gradientn(colors = c("white","lightgrey","darkgrey", "black"))+

  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position="none",
        strip.text.x = element_blank()) +
  facet_grid( ~ model)
FB_rot
#zz
ZZ_rot <- ggplot()+
  theme_void()+
  geom_sf(data = terra::spin(zz_outline, 180, 277401.8, 4867071), fill = NA, color = "#7E6B8F", alpha = 0.3, lwd = 1)+
  geom_sf(data = terra::spin(zz_net, 180, 277401.8, 4867071), colour = "grey", lwd = 1) +
geom_sf(data =  terra::spin(lcc_zz, 180, 277401.8, 4867071) %>% inner_join(ID_acc_fb, by = "ID"),
        aes(fill = accuracy), size = 3, pch = 21) +
       scale_fill_gradientn(colors = c("white","lightgrey","darkgrey", "black"),
                         limits = c(0,1),
                         labels = c("0", "0.25", "0.5", "0.75", "1"),
                         name = "Accuracy")+

  #scale_shape_manual(values = c(21, 24, 22))+
  theme(rect = element_rect(fill = "transparent", color = NA),
        legend.position="bottom",
        panel.spacing = unit(5, "lines"),
        strip.text.x = element_blank()) +
  facet_grid( ~ model) 
ZZ_rot
#combine all plots
W3_rot / FB_rot / (ZZ_rot)

#In canva will have to add north arrows and the scale bar
```


## Table 1: Topographic Metrics
Drainage densities, drainage area, elevation range, measured expansion coefficient
```{r drainage-area}
#determine watershed area
area_w3 <- expanse(w3_outline)/(1000 * 1000) #km^2
```

drainage density is just stream length (m) divided by area of shed (m^2)
```{r drainage-density}
## W3
#determine total stream length
length_w3 <- sum(perim(w3_net))/1000 #m
#determine watershed area
area_w3 <- expanse(w3_outline)/(1000 * 1000) #km^2

length_w3 / area_w3
#end up with a drainage density a lot larger than what Carrie reported and calculated

## FB
length_fb <- sum(perim(fb_net))/1000 #m
#determine watershed area
area_w3 <- expanse(w3_outline)/(1000 * 1000) #m^2

length_w3 / area_w3
## ZZ
```

```{r}
p_load(rempsyc)
```


## Table 2: Accuracy of models
Create a function that will compare a chain output and it's sequential wetting model to the actual observations.
chunk to calculate accuracy metrics and confusion matrices
```{r figuring-out-accuracy}
# Load required packages
p_load(caret)

# ---- STEP 1: Assume your data is already loaded into `df` ----
# df should have columns: binary, cheapest, pk, random, twi
df <- comparison

# ---- STEP 2: Create confusion matrices ----
conf_cheapest <- table(Predicted = df$cheapest, Actual = df$binary)
conf_pk       <- table(Predicted = df$pk,       Actual = df$binary)
conf_random   <- table(Predicted = df$random,   Actual = df$binary)
conf_twi      <- table(Predicted = df$twi,      Actual = df$binary)

# ---- STEP 3: Define helper function to extract performance metrics ----
get_metrics <- function(pred, true) {
  cm <- confusionMatrix(factor(pred), factor(true), positive = "1")
  data.frame(
    Accuracy = cm$overall["Accuracy"],
    Kappa = cm$overall["Kappa"],
    Sensitivity = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    Precision = cm$byClass["Precision"],
    F1 = cm$byClass["F1"]
  )
}

# ---- STEP 4: Summarize performance for all models ----
metrics_df <- rbind(
  cheapest = get_metrics(df$cheapest, df$binary),
  pk       = get_metrics(df$pk, df$binary),
  random   = get_metrics(df$random, df$binary),
  twi      = get_metrics(df$twi, df$binary)
)

# Add model name as a column
metrics_df <- metrics_df %>% 
  mutate(model = rownames(metrics_df)) %>%
  select(model, everything())

# Print the performance summary
kable(metrics_df, digits = 2)

# ---- STEP 5: Define function to plot confusion matrix ----
plot_conf_mat <- function(cm, title = "Confusion Matrix") {
  cm_df <- as.data.frame(cm)
  ggplot(cm_df, aes(x = Actual, y = Predicted)) +
    geom_tile(aes(fill = Freq), color = "white") +
    geom_text(aes(label = Freq)) +
    scale_fill_gradient(low = "white", high = "steelblue", limits = c(0, 100000)) +
    labs(title = title, fill = "Count") +
    theme_minimal()
}

# ---- STEP 6: Plot confusion matrices for each model ----
cheap_plot <- plot_conf_mat(conf_cheapest, title = "Cheapest") + theme(legend.position = "none")
pk_plot <- plot_conf_mat(conf_pk,       title = "PK")+ theme(legend.position = "none")
random_plot <- plot_conf_mat(conf_random,   title = "Random")+ theme(legend.position = "none")
twi_plot <- plot_conf_mat(conf_twi,      title = "TWI")

(cheap_plot + pk_plot) / (random_plot + twi_plot)
```

Can probably delete the chunk below, and keep one with pred_out instead
```{r produce-accuracy-metrics}
#function that will take a chain solution, and calculate accuracy metrics
produce_metrics <- function(chain, shed, method){
# chain <- ZZ_cheap
# shed <- "ZZ"
# method = "cheapest_insertion"
# calculate the number of flowing nodes at each timestep
  if(shed == "W3"){
    number_activated <-
      input_w3 %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  } 
  else if(shed == "FB"){
    number_activated <-
      input_fb %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  }
  else if(shed == "ZZ"){
    number_activated <-
      input_zz %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  }

#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy

  model_result <- calc_model_result(chain$ID, shed)

#combine all model results
  if(shed == "W3"){
    comparison <- input_w3 %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  } 
  else if(shed == "FB"){
    comparison <- input_fb %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  }
  else if(shed == "ZZ"){
    comparison <- input_zz %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  }


#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
#get_eval_label(0,0)
#apply get_eval function to each model run
comparison$pred_eval <- mapply(get_eval_label, 
                                   comparison$binary, 
                                   comparison$pred_out)

# ---- STEP 3: Define helper function to extract performance metrics ----
get_metrics <- function(pred, true) {
  cm <- confusionMatrix(factor(pred), factor(true), positive = "1")
  data.frame(
    Accuracy = cm$overall["Accuracy"],
    Kappa = cm$overall["Kappa"],
    Sensitivity = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    Precision = cm$byClass["Precision"],
    F1 = cm$byClass["F1"]
  )
}

# ---- STEP 4: Summarize performance for all models ----
metrics_df <- tibble(get_metrics(comparison$pred_out, comparison$binary)) %>% 
  mutate(shed = shed,
         method = method)
#output <- list(metrics = metrics_df, sequence = ZZ_cheap)
#make data long for final plot
return(metrics_df)
}

produce_metrics2 <- function(chain, shed, method){
# chain <- ZZ_cheap
# shed <- "ZZ"
# method = "cheapest_insertion"
# calculate the number of flowing nodes at each timestep
  if(shed == "W3"){
    number_activated <-
      input_w3 %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  } 
  else if(shed == "FB"){
    number_activated <-
      input_fb %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  }
  else if(shed == "ZZ"){
    number_activated <-
      input_zz %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  }

#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy

  model_result <- calc_model_result(chain$ID, shed)

#combine all model results
  if(shed == "W3"){
    comparison <- input_w3 %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  } 
  else if(shed == "FB"){
    comparison <- input_fb %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  }
  else if(shed == "ZZ"){
    comparison <- input_zz %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  }


#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
#get_eval_label(0,0)
#apply get_eval function to each model run
comparison$pred_eval <- mapply(get_eval_label, 
                                   comparison$binary, 
                                   comparison$pred_out)


return(comparison)
}


produce_metrics2(ZZ_cheap, "ZZ", "cheapest_insertion")
produce_metrics(W3_cheap, "W3", "cheapest_insertion")

#function to calculate accuracy just when system wetness is changing
w3_wetness_changing

#second produce metrics, but only for times when there are changes in state in nodes
produce_metrics2 <- function(chain, shed, method){
# chain <- ZZ_cheap
# shed <- "ZZ"
# method = "cheapest_insertion"
# calculate the number of flowing nodes at each timestep
  if(shed == "W3"){
    number_activated <-
      input_w3 %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  } 
  else if(shed == "FB"){
    number_activated <-
      input_fb %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  }
  else if(shed == "ZZ"){
    number_activated <-
      input_zz %>%
      group_by(datetime) %>%
      select(-c(wshed, mins)) %>%
      filter(binary == 1) %>%
      summarise(number_flowing = length(binary))
  }

#make a dataframe where each row is a date, with a list of the active or inactive nodes according to a hierarchy

  model_result <- calc_model_result(chain$ID, shed)

`%not_in%` <- purrr::negate(`%in%`)  
#combine all model results
  if(shed == "W3"){
    comparison <- input_w3 %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      left_join(w3_wetness_changing, by = "datetime") %>% 
      filter(status == "changing") %>% 
      select(-wshed, -mins) %>%
      drop_na()
  } 
  else if(shed == "FB"){
    comparison <- input_fb %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  }
  else if(shed == "ZZ"){
    comparison <- input_zz %>%
      filter(mins %in% c(0, 30)) %>%
      left_join(model_result, by = c("datetime", "ID")) %>%
      select(-wshed, -mins) %>%
      drop_na()
  }


#chat gpt method to compare them
get_eval_label <- function(true, pred) {
  if (true == 1 && pred == 1) return("correct")
  if (true == 0 && pred == 0) return("correct")
  if (true == 1 && pred == 0) return("omission")
  if (true == 0 && pred == 1) return("commission")
}
#get_eval_label(0,0)
#apply get_eval function to each model run
comparison$pred_eval <- mapply(get_eval_label, 
                                   comparison$binary, 
                                   comparison$pred_out)

# ---- STEP 3: Define helper function to extract performance metrics ----
get_metrics <- function(pred, true) {
  cm <- confusionMatrix(factor(pred), factor(true), positive = "1")
  data.frame(
    Accuracy = cm$overall["Accuracy"],
    Kappa = cm$overall["Kappa"],
    Sensitivity = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    Precision = cm$byClass["Precision"],
    F1 = cm$byClass["F1"]
  )
}

# ---- STEP 4: Summarize performance for all models ----
metrics_df <- tibble(get_metrics(comparison$pred_out, comparison$binary)) %>% 
  mutate(shed = shed,
         method = method)
#output <- list(metrics = metrics_df, sequence = ZZ_cheap)
#make data long for final plot
return(metrics_df)
}

# THERE IS NOT A BIG DIFFERENCE IN accuracy of the models when system wetness is changing I think
produce_metrics(W3_cheap, "W3", "cheapest_insertion")
produce_metrics2(W3_cheap, "W3", "cheapest_insertion")


produce_metrics(W3_pk_seq, "W3", "pk")
produce_metrics2(W3_pk_seq, "W3", "pk")

```
```{r accuracy-function-testing-every-method}
methods
#function to determine a chain solution
ZZ_cheap <- chain_solution(ZZ_IDs, "ZZ", methods = "cheapest_insertion")



#testing every method
for(i in 1:length(methods)){
  chain <- chain_solution(ZZ_IDs, "ZZ", methods = methods[i])
  
  output <- produce_metrics(chain, "ZZ", methods[i])
  
  if(i == 1) many_chains <- output
  if(i > 1) many_chains <- rbind(many_chains, output)
}

all_methods_acc <- many_chains
```


```{r accuracy-W3}
w3_acc <- rbind(#produce_metrics(W3_cheap, "W3", "cheapest_insertion"),
      produce_metrics(W3_pk_seq, "W3", "pk"),
      #produce_metrics(convert_to_IDseq(W3_random), "W3", "random"),
      produce_metrics(w3_twi_sequence, "W3", "twi"),
      produce_metrics(convert_to_IDseq(new_seq_w3), "W3", "new")#,
      #produce_metrics(convert_to_IDseq(opt_routes_w3), "W3", "con")

      )
kable(w3_acc, digits = 2)

p_load(rempsyc)

nice_table(
  w3_acc
  )
)

```
```{r accuracy-FB}
fb_acc <- rbind(produce_metrics(FB_cheap, "FB", "cheapest_insertion"),
      produce_metrics(FB_pk_seq, "FB", "pk"),
      produce_metrics(convert_to_IDseq(FB_random), "FB", "random"),
      produce_metrics(fb_twi_sequence, "FB", "twi"),
      produce_metrics(convert_to_IDseq(new_seq_fb), "FB", "new"),
      produce_metrics(convert_to_IDseq(opt_routes_fb), "FB", "con")
      )
kable(fb_acc, digits = 2)

```
```{r accuracy-ZZ}
zz_acc <- rbind(produce_metrics(ZZ_cheap, "ZZ", "cheapest_insertion"),
      produce_metrics(ZZ_pk_seq, "ZZ", "pk"),
      produce_metrics(convert_to_IDseq(ZZ_random), "ZZ", "random"),
      produce_metrics(zz_twi_sequence, "ZZ", "twi"),
      produce_metrics(convert_to_IDseq(new_seq_zz), "ZZ", "new"),
      produce_metrics(convert_to_IDseq(opt_routes_zz), "ZZ", "con")
      )

kable(zz_acc, digits = 2)

kable(rbind(w3_acc, fb_acc, zz_acc), digits = 2)

```

# Unused Figures
## UNUSED: Accuracy of all models versus pk accuracy
```{r W3}
#trying to plot accuracy vs accuracy
df_long2 <- df_summary_time_w3 %>%
  select(datetime, cheap_accuracy, pk_accuracy, twi_accuracy, new_accuracy, con_accuracy) %>%
  tidyr::pivot_longer(cols = c(twi_accuracy, new_accuracy, con_accuracy), 
                      names_to = "model", values_to = "accuracy") %>% 
    inner_join(w3_limbs, by = "datetime") %>% 
  mutate(shed = "W3")

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")

df_long2 %>% 
  ggplot()+
  geom_point(aes(x = pk_accuracy, 
                 y = accuracy, 
                 size = Q_mm_day, color = model),
             alpha = 0.5)+
  scale_color_manual(values = five_colors)+
  geom_abline()+
  theme_minimal()+
  lims(y = c(0, 1),
       x = c(0, 1))+
  facet_grid(~model)
```
```{r FB}
#trying to plot accuracy vs accuracy

#left join

df_long2_fb <- df_summary_time_fb %>%
  select(datetime, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy") %>% 
    inner_join(fb_limbs, by = "datetime")%>% 
  mutate(shed = "FB")

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")

df_long2_fb %>% 
  ggplot()+
  geom_point(aes(x = pk_accuracy, 
                 y = accuracy, 
                 size = roll_mean, shape = event_type, color = model),
             alpha = 0.5)+
  scale_color_manual(values = c("#bb4430", "#231f20", "#f3dfa2"))+
  geom_abline()+
  theme_minimal()+
  lims(y = c(0, 1),
       x = c(0, 1))+
  facet_grid(event_type~model)
```
```{r ZZ}
#trying to plot accuracy vs accuracy

#left join

df_long2_zz <- df_summary_time_zz %>%
  select(datetime, cheap_accuracy, pk_accuracy, random_accuracy, twi_accuracy) %>%
  tidyr::pivot_longer(cols = c(cheap_accuracy, random_accuracy, twi_accuracy), 
                      names_to = "model", values_to = "accuracy") %>% 
    inner_join(zz_limbs, by = "datetime")%>% 
  mutate(shed = "ZZ")

four_colors <- c("#bb4430", "#7ebdc2", "#231f20", "#f3dfa2")

df_long2_zz %>% 
  ggplot()+
  geom_point(aes(x = pk_accuracy, 
                 y = accuracy, 
                 size = roll_mean, shape = event_type, color = model),
             alpha = 0.5)+
  scale_color_manual(values = c("#bb4430", "#231f20", "#f3dfa2"))+
  geom_abline()+
  theme_minimal()+
  lims(y = c(0, 1),
       x = c(0, 1))+
  facet_grid(event_type~model)
```
Combine all of these, since they are not that different... breakup by watershed
```{r}
rbind(
df_long2 %>% select(datetime, pk_accuracy, model, accuracy, event_type, shed),
df_long2_fb %>% select(datetime, pk_accuracy, model, accuracy, event_type, shed),
df_long2_zz %>% select(datetime, pk_accuracy, model, accuracy, event_type, shed)) %>% 
  ggplot()+
  geom_point(aes(x = pk_accuracy, 
                 y = accuracy, shape = event_type, color = shed),
             alpha = 0.5,
             size = 2)+
  scale_color_manual(values = shed_colors)+
  geom_abline()+
  theme_minimal()+
  lims(y = c(0, 1),
       x = c(0, 1))+
  facet_grid(~model)


```


## UNUSED: Tile plot
```{r W3}

range01 <- function(x){(x-min(x))/(max(x)-min(x))}

W3_twi_seq <- w3_twi_sequence %>% 
  mutate(sequence = seq(1, length(ID), 1))

w3_topo_norm <- w3_topo %>% 
  group_by(name) %>% 
  mutate(norm_value = range01(value))

#tile plot, original vision
rbind(W3_cheap %>% mutate(model = "cheapest"),
      W3_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(w3_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  drop_na() %>% 
  ggplot() +
  geom_tile(aes(x = sequence, y = name, fill = norm_value))+
  scale_fill_gradient(low = "white", 
                      high = "black")+
  facet_grid(rows = "model")

#will it look better as a line?
#w3_explain <- 
rbind(W3_cheap %>% mutate(model = "cheapest"),
      W3_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(w3_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  drop_na() %>%
  mutate(shed = "W3") %>% 
  ggplot(aes(x = sequence, y = norm_value, color = name)) +
  geom_smooth(method = 'lm', se = FALSE)+
  geom_point()+
  facet_grid(name~model, scales = "free_x")+
  theme_classic()
```
```{r}
# Perform regressions and get significance
rbind(W3_cheap %>% mutate(model = "cheapest"),
      W3_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(w3_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  as_tibble() %>% 
group_by(model) %>% 
  do(model = lm(sequence ~ norm_value, data = .)) #%>%
  mutate(p_value = summary(model)$coefficients["x", "Pr(>|t|)"]) #%>%
  mutate(significant = p_value < 0.05)

W3_pk_seq %>% mutate(model = "cheapest") %>% 
  left_join(w3_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  as_tibble() %>% 
  filter(name == "uaa") %>% 
  lm(sequence ~ norm_value, data = .) %>% summary()
group_by(name) %>% 
  summarise(summary(lm(sequence ~ norm_value, data = .))$r.squared)
  do(model = lm(sequence ~ norm_value, data = .)) %>% print()
  mutate(p_value = summary(model)$coefficients["x", "Pr(>|t|)"]) #%>%
  mutate(significant = p_value < 0.05)
  
  
  
  
models <- data %>%
  group_by(group) %>%
  do(model = lm(y ~ x, data = .)) %>%
  mutate(p_value = summary(model)$coefficients["x", "Pr(>|t|)"]) %>%
  mutate(significant = p_value < 0.05)

# Merge significance information back to the original data
data_with_significance <- left_join(data, models %>% select(group, significant), by = "group")

# Plotting
ggplot(data_with_significance, aes(x = x, y = y, color = group)) +
  geom_point() +
  geom_smooth(data = . %>% filter(significant), method = "lm", se = FALSE) + # Only plot significant lines
  labs(title = "Trend Lines for Significant Relationships Only",
       subtitle = "Based on p-value < 0.05 for slope",
       x = "X-axis", y = "Y-axis") +
  theme_minimal()
```


```{r FB}

FB_twi_seq <- fb_twi_sequence %>% 
  mutate(sequence = seq(1, length(ID), 1))

fb_topo_norm <- fb_topo %>% 
  group_by(name) %>% 
  mutate(norm_value = range01(value))

#tile plot, original vision
rbind(FB_cheap %>% mutate(model = "cheapest"),
      FB_pk_seq %>% mutate(model = "pk")) %>%
  left_join(fb_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  drop_na() %>% 
  ggplot() +
  geom_tile(aes(x = sequence, y = name, fill = norm_value))+
  scale_fill_gradient(low = "white", 
                      high = "black")+
  facet_grid(rows = "model")

#will it look better as a line?
fb_explain <- rbind(FB_cheap %>% mutate(model = "cheapest"),
      FB_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(fb_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  mutate(shed = "FB") %>% 
  drop_na()
  ggplot() +
  geom_smooth(aes(x = sequence, y = norm_value, color = name))+
  facet_grid(~model, scales = "free_x")+
  theme_classic()
  
rbind(FB_cheap %>% mutate(model = "cheapest"),
      FB_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(w3_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  drop_na() %>%
  mutate(shed = "FB") %>% 
  ggplot(aes(x = sequence, y = norm_value, color = name)) +
  geom_smooth(method = 'lm', se = FALSE)+
  geom_point()+
  facet_grid(name~model, scales = "free_x")+
  theme_classic()
```
```{r ZZ}

ZZ_twi_seq <- zz_twi_sequence %>% 
  mutate(sequence = seq(1, length(ID), 1))

zz_topo_norm <- zz_topo %>% 
  group_by(name) %>% 
  mutate(norm_value = range01(value))

#tile plot, original vision
rbind(ZZ_cheap %>% mutate(model = "cheapest"),
      ZZ_pk_seq %>% mutate(model = "pk")) %>%
  left_join(zz_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  drop_na() %>% 
  ggplot() +
  geom_tile(aes(x = sequence, y = name, fill = norm_value))+
  scale_fill_gradient(low = "white", 
                      high = "black")+
  facet_grid(rows = "model")

#will it look better as a line?
zz_explain <- rbind(ZZ_cheap %>% mutate(model = "cheapest"),
      ZZ_pk_seq %>% mutate(model = "pk")) %>% #,
     # W3_random %>% mutate(model = "random"),
     # W3_twi_seq %>% mutate(model = "twi")) %>% 
  left_join(zz_topo_norm, by = "ID", relationship = "many-to-many") %>% 
  mutate(shed = "ZZ")
  drop_na() %>% 
  ggplot() +
  geom_smooth(aes(x = sequence, y = norm_value, color = name))+
  facet_grid(~model, scales = "free_x")+
  theme_classic()
```
```{r combine-as-lines}
rbind(w3_explain, fb_explain, zz_explain) %>% 
  ggplot() +
  geom_smooth(aes(x = sequence, y = norm_value, color = name), method = 'lm', se = FALSE)+
  facet_grid(model~shed, scales = "free_x")+
  theme_classic()

#lm(sequence ~ )

```



## UNUSED: Change in Q, versus accuracy
```{r dQ}
#calculate the delta Q
df_long2 %>% 
  mutate(dQ = abs(Q_mm_day - lag(Q_mm_day))) %>% 
  ggplot()+
  geom_point(aes(x = dQ, y = pk_accuracy, color = event_type))

df_long2_fb %>% 
  mutate(dQ = abs(roll_mean - lag(roll_mean))) %>% 
  ggplot()+
  geom_point(aes(x = dQ, y = pk_accuracy, color = event_type))+
  lims(x = c(0,0.75))

df_long2_zz %>% 
  mutate(dQ = abs(roll_mean - lag(roll_mean))) %>% 
  ggplot()+
  geom_point(aes(x = dQ, y = pk_accuracy, color = event_type))+
  lims(x = c(0,0.25))
```
```{r number-of-transitions}

df_long2 %>%
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_w3, by = "datetime") %>% 
  ggplot()+
  geom_point(aes(x = number_of_nodes, y = pk_accuracy, color = state_change),
             size = 2, alpha = 0.5)

df_long2_fb %>% 
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_fb, by = "datetime") %>% 
  ggplot()+
  geom_point(aes(x = number_of_nodes, y = pk_accuracy, color = state_change),
             size = 2, alpha = 0.5)

df_long2_zz %>% 
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_zz, by = "datetime") %>% 
  ggplot()+
  geom_point(aes(x = number_of_nodes, y = pk_accuracy, color = state_change),
             size = 2, alpha = 0.5)

rbind(df_long2 %>%
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_w3, by = "datetime") %>% 
        select(datetime, pk_accuracy, shed, number_of_nodes, state_change),
  df_long2_fb %>% 
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_fb, by = "datetime")%>% 
        select(datetime, pk_accuracy, shed, number_of_nodes, state_change),
  df_long2_zz %>% 
  filter(model == "cheap_accuracy") %>% 
  left_join(num_trans_zz, by = "datetime") %>% 
        select(datetime, pk_accuracy, shed, number_of_nodes, state_change)) %>% 
  ggplot()+
  geom_point(aes(x = number_of_nodes, y = pk_accuracy, color = state_change),
             size = 2, alpha = 0.5)+
  facet_wrap(~shed)
```
# Testing many random sequences
What if we test 100 different cheapest_insertions?
```{r 10-cheapest-ZZ}
many_cheap <- rep("cheapest_insertion", 1000)
for(i in 1:length(many_cheap)){
  chain <- chain_solution(ZZ_IDs, "ZZ", methods = many_cheap[i])
  
  output <- produce_metrics(chain, "ZZ", many_cheap[i]) %>% 
    mutate(chain = list(chain$ID))
  
  if(i == 1) many_chains <- output
  if(i > 1) many_chains <- rbind(many_chains, output)
}

many_chains
many_chains -> many_chainsZZ

hist(many_chains$Accuracy)

many_cheap <- rep("cheapest_insertion", 3)
for(i in 1:length(many_cheap)){
  chain <- chain_solution(W3_IDs, "W3", methods = many_cheap[i])
  
  output <- produce_metrics(chain, "W3", many_cheap[i])
  
  if(i == 1) many_chains <- output
  if(i > 1) many_chains <- rbind(many_chains, output)
}

many_chains

```

## more random sequence stuff
I have run like 500 random sequences for ZZ, now can I calc some kind of statistic that is how often the same nodes are in the same place in the sequence? Or maybe how often they are in the sequence in relation to another?

```{r}
tibble("seqs" = unlist(many_chainsZZ$chain[1:500]),
               "pos" = rep(seq(1, 24, 1), 500)) %>% 
  group_by(seqs) %>% 
  summarize(mean_pos = mean(pos),
            sd = sd(pos),
            min = min(pos),
            max = max(pos),
            range = max(pos) - min(pos)) %>% 
  arrange(mean_pos) %>% View()

seqq <- tibble("seqs" = unlist(many_chainsZZ$chain[1:500]),
               "pos" = rep(seq(1, 24, 1), 500)) %>% 
  group_by(seqs) %>% 
  summarize(mean_pos = mean(pos)) %>% 
  arrange(mean_pos)

average_sequence <- seqq$seqs

routes_zz_test <- tibble("up" = average_sequence,
                    "down" = lag(average_sequence)) %>% drop_na() %>% 
  select(up, down)

test <- calc_props(routes_zz_test, "ZZ")
```


# Bonus Figure: comparing wetting versus drying
```{r differentiated-states-functions}
#chunk to trouble shoot non-functioning instances to figure out why they are not working
#instance not working:
#run_scenario(routes_w3_pk, "pk", "W3", "daily")

routes <- pks_w3 %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

input <- rbind(bind23, bind24) %>%
      mutate(hour = hour(datetime)) %>% 
        filter(wshed == "W3", mins %in% c(0, 30)) %>%
      #filter(wshed == "W3", hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary) #%>% 
  #filter(datetime < stop24 & datetime > start24)


all_transitions <- function(up, down, input){
#inputs to function- comment out in final version
# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
#input <- filtered_input

#create output with the total and the sub, also the two input locations
output <- data.frame(up, down)

  
no_dupes <- input %>% 
      select(up,down, datetime) %>% #remove date
      # make it so that there cannot be a sequence without change
      # keep date column for indexing purposes later
      filter(row_number() == 1 | !apply(select(., up, down) == lag(select(., up, down)), 1, all)) %>% 
      #remove rows where one of the sensors is missing data
      drop_na()
#View(no_dupes)
#all flowing all the time?
check <- nrow(no_dupes)

if(check <= 2){
  output$wetting <- NA
  output$drying <- NA
  return(output)
}
else {
# Define window size
window_size <- 2

# Create sliding windows
windows <- rollapply(
  select(no_dupes, -datetime),
  width = window_size,
  by.column = FALSE,
  FUN = function(x) paste(as.vector(t(x)), collapse = "")
)

# Count and sort sequences
sequence_counts <- table(windows)
sorted_counts <- sort(sequence_counts, decreasing = TRUE)

# Display all sequences and their frequencies
sequence_df <- as.data.frame(sorted_counts, stringsAsFactors = FALSE)
#if(check > 1) colnames(sequence_df) <- c("Sequence", "Frequency")
sequence_df$t1_up <- as.numeric(substr(sequence_df$windows, 1, 1))
sequence_df$t1_down <- as.numeric(substr(sequence_df$windows, 2, 2))
sequence_df$t2_up <- as.numeric(substr(sequence_df$windows, 3, 3))
sequence_df$t2_down <- as.numeric(substr(sequence_df$windows, 4, 4))

sequence_df$sum_t1 <- sequence_df$t1_up + sequence_df$t1_down
sequence_df$sum_t2 <- sequence_df$t2_up + sequence_df$t2_down
sequence_df$direction <- "drying"
sequence_df$direction[sequence_df$sum_t1 < sequence_df$sum_t2] <- "wetting"
#new fixed code, should not drop non-hierarchical values
total_state_changes <- sequence_df %>% group_by(direction) %>% summarise(totals = sum(Freq))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- sequence_df %>% 
  filter(windows %in% supports) %>% group_by(direction) %>% summarise(hierarchical = sum(Freq)) 

sub <- total_state_changes %>% left_join(hierarchical_changes, by = c("direction")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))

#old code

# total_state_changes <- sequence_df %>% group_by(direction) %>% summarise(totals = sum(Freq))
# 
# supports <- c("0001","0111","1101", "0100")
# sub <- filter(sequence_df, windows %in% supports) %>%
#   group_by(direction) %>% summarise(hierarchicals = sum(Freq)) %>% 
#   left_join(total_state_changes, by = "direction") %>% 
#   mutate(prop = hierarchicals/totals)
#output$points <- sum(sub$Frequency)
output$drying <- sub$prop[1]
output$wetting <- sub$prop[2]
#write some way to score the sequence_df
#award one point for one of these configs:


#sub <- filter(sequence_df, Sequence %in% supports)

#create output with transitions
#error handling- in situation where both points flowed 100% of the time

return(output)}
}

#test function
all_transitions("r_23", "r_6", input)

#function to break up groups of continuous measurements, ensure that gaps are not considered
#contains calc_support function
iterate_groups_wd <- function(up, down, input, timestep){
  #create group column that identifies gaps in continuous data in time

# i <- 4
# up <- paste0("r_",routes$up[i])
# down <- paste0("r_",routes$down[i])
# timestep <- hours(1)
  input$group <- cumsum(c(TRUE, diff(input$datetime) != timestep))
  #View(input)

  for(u in 1:length(unique(input$group))){
  # u <- 1
  #   print(u)
    filtered_input <- input %>% filter(group == u)
    #this line throws error if 
    output <- all_transitions(up, down, filtered_input)
    

     if(u == 1) iterate_groups_alldat <- output
     if(u > 1) iterate_groups_alldat <- rbind(iterate_groups_alldat, output)
  }
  # final_iterate_groups_alldat <- iterate_groups_alldat %>% 
  #   drop_na() %>% 
  #   group_by(up, down) %>% 
  #   summarise(total = sum(total),
  #             points = sum(points))
  return(iterate_groups_alldat)
}

iterate_groups_wd("r_23", "r_6", input, minutes(30))
#function to take a list of routes and input dataset
#contains group iteration function
#for loop to iterate through full list of combinations of up and downstream locations
#IMPORTANT- calculate hierarchy and iterate groups only work if the input timestep is approriate
calculate_hierarchy_wd <- function(routes, input, timestep){
  for(x in 1:length(routes$up)){
  up <- paste0("r_",routes$up[x])
  down <- paste0("r_",routes$down[x])
  #print(x)
  
  out <- iterate_groups_wd(up, down, input, timestep)
    #out <- calc_support(up, down, input)


  if(x == 1) alldat <- out
  if(x > 1) alldat <- rbind(alldat, out)

  }
  final_output <- alldat %>% 
    drop_na() %>%
    # group_by(up, down) %>%
    # summarise(total = sum(total),
    #           points = sum(points)) %>% 
    # mutate(prop = points/total)
  return(final_output)
}

#calculate_hierarchy_wd(routes, input, minutes(30))
#calculate_hierarchy(routes, input, days(1))


#make a function to loop through the four possible timesteps, and combine the output just for ease of applying this many different variations
#fantastic 4 determines the input on its own
fantastic_four_wd <- function(routes, shed){
  theFour <- c("30mins", "daily")
  
  for(q in 1:length(theFour)){
    #if statements to detect timescale, calculate appropriate inputs
    timescale <- theFour[q]
  if(timescale == "30mins"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      filter(wshed == shed, mins %in% c(0, 30)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- minutes(30)
  } 
  else if(timescale == "daily"){
    input <- rbind(input_w3, input_fb, input_zz) %>%
      mutate(hour = hour(datetime)) %>% 
      filter(wshed == shed, hour %in% c(12), mins %in% c(0)) %>%
      select(datetime, binary, ID) %>%
      mutate(ID = paste0("r_", ID)) %>%
      pivot_wider(names_from = ID, values_from = binary)
    timestep <- days(1)
  } 
  else {
    stop("Not a timescale anticipated!")
  }
    out <- calculate_hierarchy_wd(routes, input, timestep)
    out$timescale <- theFour[q]
    
    if(q == 1) fanfar <- out
    if(q > 1) fanfar <- rbind(fanfar, out)
  }
  fanfar$shed <- shed
  return(fanfar)
}


fantastic_four_wd(routes_w3, "W3")

#run calc_support for all sheds and timesteps for relative position
all_position_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                      fantastic_four_wd(routes_fb, "FB"),
                      fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "Proportion of Time Flowing")

sults_so_far <- all_position_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying)) %>% 
  pivot_longer(cols = starts_with("avg"),
               names_to = "direction",
               values_to = "prop")

sults_so_far$direction[sults_so_far$direction == "avg_wet_prop"] <- "wetting"
sults_so_far$direction[sults_so_far$direction == "avg_dry_prop"] <- "drying"

#plot
sults_so_far %>% 
  #filter(timescale %in% c(possible_times[t])) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = direction, color = direction), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  labs(
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = c("#FFA400", "#93C2F1"),name = "Direction")+
  scale_color_manual(values = c("#FFA400", "#93C2F1"), name = "Direction")+
  ylim(c(0, 4))+
  xlim(c(0,1))+
  facet_grid(shed~timescale)


sults_so_far %>% 
  filter(shed == "W3", timescale == "30mins") %>% 
  group_by(direction) %>% 
  summarise(mean(prop))

#for each scenario create the routes, inputs, and specify the timestep

calc_props <- function(routes, shed){
  full_combos <- fantastic_four_wd(routes, shed)
total_state_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    group_by(up, down, timescale, shed) %>% 
    summarise(totals = sum(Frequency))
supports <- c("0001","0111","1101", "0100")

hierarchical_changes <- full_combos %>% 
    filter(Sequence != 0011, Sequence != 1100) %>% 
    filter(Sequence %in% supports) %>% 
    group_by(up, down, timescale, shed) %>%  
    summarise(hierarchical = sum(Frequency)) 

un_split <- total_state_changes %>% 
  left_join(hierarchical_changes, by = c("up", "down", "shed", "timescale")) %>% 
  mutate(prop = hierarchical/totals) %>% 
  mutate_all(~replace(., is.na(.), 0))
return(un_split)
}
```

```{r}

routes <- W3_cheap %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)


all_tsp_wd <- rbind(fantastic_four_wd(routes_w3, "W3"),
                      fantastic_four_wd(routes_fb, "FB"),
                      fantastic_four_wd(routes_zz, "ZZ")) %>% 
  mutate("hierarchy" = "TSP")

sults_so_far_tsp <- all_tsp_wd %>% 
  group_by(up, down, timescale, shed, hierarchy) %>%
    summarise(avg_wet_prop = mean(wetting),
              avg_dry_prop = mean(drying)) %>% 
  pivot_longer(cols = starts_with("avg"),
               names_to = "direction",
               values_to = "prop")

sults_so_far_tsp$direction[sults_so_far_tsp$direction == "avg_wet_prop"] <- "wetting"
sults_so_far_tsp$direction[sults_so_far_tsp$direction == "avg_dry_prop"] <- "drying"

#plot
sults_so_far_tsp %>% 
  #filter(timescale %in% c(possible_times[t])) %>% 
  ggplot(aes(x = prop, y = after_stat(density))) +
geom_density(aes(fill = direction, color = direction), alpha = 0.5)+
    geom_density(alpha = 0.5, lty = 3)+
      geom_vline(xintercept = 0.5, lty = 2, alpha = 0.5)+
    theme_bw()+
  labs(
       x = "Proportion of time followed",
       y = "Density")+
  scale_fill_manual(values = c("#FFA400", "#93C2F1"),name = "Direction")+
  scale_color_manual(values = c("#FFA400", "#93C2F1"), name = "Direction")+
  ylim(c(0, 4))+
  xlim(c(0,1))+
  facet_grid(shed~timescale)
```


```{r proportion-of-time-flowing-analysis}
routes_w3 <- pks_w3 %>% 
    filter(ID %in% W3_IDs) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_fb <- pks_fb %>%
    filter(ID %in% FB_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

routes_zz <- pks_zz %>%
    filter(ID %in% ZZ_IDs) %>% 
  filter(pk != 1) %>% 
  arrange(desc(pk)) %>% 
  mutate(down = lag(ID)) %>% 
  rename("up" = ID) %>% drop_na() %>% 
  select(up, down)

# actually determining how often the nodes follow proportion of time flowing sequence
all_pk <- rbind(calc_props(routes_w3, "W3"),
                calc_props(routes_fb, "FB"),
                calc_props(routes_zz, "ZZ")) %>% 
  mutate("method" = "Flow Permanence")
```


Exploring more ways to try and find a sequence- all from chat gpt, of medium to low usefulness
```{r}
# Example data: binary matrix (time x nodes)
set.seed(123)
X <- matrix(rbinom(1000, 1, 0.3), ncol = 5)  # 1000 timesteps, 5 nodes
colnames(X) <- paste0("Node", 1:5)

# Conditional probability: P(Node j = 1 at t+1 | Node i = 1 at t)
cond_prob <- function(i, j, lag = 1) {
  xi <- X[1:(nrow(X)-lag), i]
  xj <- X[(1+lag):nrow(X), j]
  mean(xj[xi == 1])
}

cond_prob(1, 2)  # P(Node2(t+1) = 1 | Node1(t) = 1)

```
```{r}
#calculating cross correlation
ccf(X[,1], X[,2], lag.max = 10, main = "Cross-correlation Node1 vs Node2")

```
```{r}
p_load(depmixS4)

# Fit HMM with 2 hidden states to one nodes series
mod <- depmix(response = Node1 ~ 1, data = data.frame(Node1 = X[,1]), nstates = 2, family = binomial())
fit <- fit(mod)
summary(fit)

```

```{r}
data_mat <- 
input_w3 %>%
  dplyr::select(datetime, ID, binary) %>% 
  mutate(binary = as.numeric(binary)) %>% 
  pivot_wider(names_from = ID, values_from = binary) %>% 
  dplyr::select(-datetime) %>%
 drop_na() %>% 
  data.matrix()

ccf(data_mat[,2], data_mat[,5], lag.max = 1500, main = "Cross-correlation Node1 vs Node2")

```

Train an LTSM
```{r}
p_load(keras)

# Install TensorFlow backend if you dont have it already
#keras::install_keras() 


set.seed(123)
# Example data: 1000 timesteps, 5 nodes
X <- matrix(rbinom(5000, 1, 0.3), ncol = 5)
X <- data_mat

timesteps <- nrow(X)
nodes <- ncol(X)

# Parameters
seq_len <- 10  # length of input sequence
n_samples <- timesteps - seq_len

X_train <- array(0, dim = c(n_samples, seq_len, nodes))
y_train <- array(0, dim = c(n_samples, nodes))

for (i in 1:n_samples) {
  X_train[i,,] <- X[i:(i+seq_len-1), ]
  y_train[i,]  <- X[i+seq_len, ]
}

dim(X_train)  # (samples, timesteps, features)
dim(y_train)  # (samples, features)


#define model
model <- keras_model_sequential() %>%
  layer_lstm(units = 32, input_shape = c(seq_len, nodes), return_sequences = FALSE) %>%
  layer_dense(units = nodes, activation = "sigmoid")  # sigmoid for binary outputs

model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

summary(model)

history <- model %>% fit(
  X_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_split = 0.2
)

plot(history)

#make predictions
preds <- model %>% predict(X_train[1:5,,])
preds_binary <- ifelse(preds > 0.5, 1, 0)

print(preds_binary)


```

```{r}
influence_matrix <- matrix(0, nrow = nodes, ncol = nodes,
                           dimnames = list(colnames(X), colnames(X)))

for (i in 1:nodes) {
  for (j in 1:nodes) {
    if (i != j) {
      # Take some input sequence
      seq_input <- X_train[1,,]
      
      # Baseline prediction
      base_pred <- predict(model, array(seq_input, dim = c(1, seq_len, nodes)))
      
      # Mask node i
      masked_input <- seq_input
      masked_input[,i] <- 0
      masked_pred <- predict(model, array(masked_input, dim = c(1, seq_len, nodes)))
      
      # Influence score: drop in prediction probability for node j
      influence_matrix[i,j] <- base_pred[1,j] - masked_pred[1,j]
    }
  }
}

influence_matrix


net_influence <- rowSums(influence_matrix) - colSums(influence_matrix)
order_nodes <- order(net_influence, decreasing = TRUE)

activation_order <- colnames(X)[order_nodes]
activation_order


routes <-  tibble(ID = as.numeric(activation_order)) %>%
  mutate(sequence = seq(1, length(activation_order), 1)) %>% 
  drop_na() 

# actually determining how often the nodes follow proportion of time flowing sequence
props <- calc_props(routes, "W3")
conflicts_prefer(dplyr::filter)

props <- props %>% 
  filter(timescale == "30mins")

hist(props$prop)

calc_model_result(W3_cheap$ID, "W3") %>% rename("cheapest" = pred_out)

produce_metrics(routes, "W3", "LTSM")

```

Maybe can use the matrix for graph theory stuff?

```{r}
# Example: build directed weighted graph from adjacency matrix
adj <- influence_matrix  # toy 5x5
#diag(adj) <- 0
g <- graph_from_adjacency_matrix(adj, mode = "directed", weighted = TRUE)

# Centrality measures
strength_out <- strength(g, mode = "out")   # influence exerted
strength_in  <- strength(g, mode = "in")    # influence received
pagerank_vals <- page_rank(g, weights = E(g)$weight)$vector

# Communities
communities <- cluster_walktrap(as.undirected(g, mode = "collapse", weighted = TRUE))
membership(communities)

# Cycle detection
is_dag(g)  # TRUE if graph is acyclic
scc <- components(g, mode = "strong")  # strongly connected components

# Plot graph
plot(g, edge.width = E(g)$weight*5, vertex.size = 30,
     vertex.label = V(g)$name, layout = layout_with_fr)


topo <- topo_sort(g, mode = "out")
as_ids(topo)

```

```{r}
# Stream Network Wetting Sequence Toolkit (R)
# ------------------------------------------
#
# What this file provides
# 1) Inference of a pairwise event precedence matrix from binary (dry=0 / wet=1) time series per node.
# 2) Conversion of that precedence matrix into an asymmetric cost matrix for sequencing.
# 3) Solvers to find a minimum-cost *Hamiltonian path* (free start & end) under an asymmetric cost model:
#    - (A) Exact HeldKarp dynamic program (works up to ~2022 nodes)
#    - (B) Heuristic fallback for larger n (greedy + 2-opt)
#    - (C) Optional integration with the TSP package if you prefer Concorde/heuristics
# 4) Visualizations: precedence heatmap and graph with ranked wetting order.
#
# How to use quickly
# - If you already have an asymmetric cost matrix C (n x n), call:
#     res <- optimal_sequence_from_cost(C)
#   which returns list(order=..., cost=...).
# - If you instead have binary time series X (n x T), call:
#     P <- event_precedence_matrix(X)
#     C <- cost_from_precedence(P, eps=1e-6, transform="neglog")
#     res <- optimal_sequence_from_cost(C)
# - Visualize with:
#     plot_precedence_heatmap(P)
#     plot_ranked_graph(C, res$order)
#
# Dependencies
# - Required: none beyond base R & stats
# - Recommended: ggplot2, igraph, ggraph, TSP (optional)
#   Install if needed: install.packages(c("ggplot2", "igraph", "ggraph", "TSP"))

suppressWarnings({
  quietly_require <- function(pkg){ if (!requireNamespace(pkg, quietly=TRUE)) FALSE else TRUE }
  HAS_GG  <- quietly_require("ggplot2")
  HAS_IG  <- quietly_require("igraph")
  HAS_GGR <- quietly_require("ggraph")
  HAS_TSP <- quietly_require("TSP")
})

# ---------------------------------------------------------------------
# Part 1. From binary time series -> Pairwise precedence matrix P
# ---------------------------------------------------------------------
# P[i,j] = fraction of observed wetting events where node i activates before node j
# (ties split as 0.5 unless tie_half_credit=FALSE)

rising_edges <- function(x){
  # return indices t where 0->1 transition occurs at time t (1-based)
  if (length(x) < 2) return(integer(0))
  which(x[-length(x)]==0 & x[-1]==1) + 1
}

event_precedence_matrix <- function(binary_times, at_value = 1L, tie_half_credit = TRUE){
  X <- as.matrix(binary_times)
  if (!all(X %in% c(0L,1L))) stop("binary_times must contain only 0/1 values")
  n <- nrow(X); Tt <- ncol(X)
  rises <- lapply(seq_len(n), function(i) rising_edges(X[i,]))
  all_events <- sort(unique(unlist(rises)))
  if (length(all_events)==0){
    # fallback: single global event near first 1 anywhere
    first1 <- apply(X, 1, function(v){ w <- which(v==at_value); if(length(w)) min(w) else Inf })
    if (all(is.infinite(first1))) stop("No wetting (1) observed in any node.")
    all_events <- min(first1)
  }
  wins <- matrix(0, n, n)
  counts <- matrix(0, n, n)
  for (t0 in all_events){
    act <- rep(Inf, n)
    for (i in seq_len(n)){
      r <- rises[[i]]
      if (length(r)){
        k <- findInterval(t0, r) + 1  # first rising >= t0
        if (k>=1 && k<=length(r)) act[i] <- r[k]
      }
    }
    valid <- which(is.finite(act))
    if (length(valid) < 2) next
    for (ii in seq_along(valid)) for (jj in seq_along(valid)){
      i <- valid[ii]; j <- valid[jj]
      if (i==j) next
      if (act[i] < act[j]) { wins[i,j] <- wins[i,j] + 1; counts[i,j] <- counts[i,j] + 1 }
      else if (act[i] > act[j]) { wins[j,i] <- wins[j,i] + 1; counts[i,j] <- counts[i,j] + 1 }
      else {
        if (tie_half_credit){ wins[i,j] <- wins[i,j] + 0.5; wins[j,i] <- wins[j,i] + 0.5 }
        counts[i,j] <- counts[i,j] + 1
      }
    }
  }
  P <- matrix(0.5, n, n)
  diag(P) <- 0.5
  for (i in seq_len(n)) for (j in seq_len(n)){
    if (i==j) next
    denom <- wins[i,j] + wins[j,i]
    if (denom > 0) P[i,j] <- wins[i,j] / denom else P[i,j] <- 0.5
  }
  rownames(P) <- colnames(P) <- rownames(binary_times)
  P
}

cost_from_precedence <- function(P, eps=1e-6, transform=c("neglog","one_minus")){
  transform <- match.arg(transform)
  if (transform=="neglog") C <- -log(pmax(P, eps)) else C <- 1 - P
  diag(C) <- Inf
  C
}

# ---------------------------------------------------------------------
# Part 2. Minimum-cost Hamiltonian PATH for asymmetric costs
# ---------------------------------------------------------------------
# (A) Exact HeldKarp DP (works up to ~2022 nodes). Returns optimal order & cost.

optimal_sequence_from_cost <- function(C){
  C <- as.matrix(C); n <- nrow(C)
  if (n != ncol(C)) stop("C must be square")
  if (any(is.na(C))) stop("C contains NA")
  if (n > 22) return(optimal_sequence_heuristic(C))
  # dp is a named list keyed by "mask|j"
  key <- function(mask, j) paste(mask, j, sep="|")
  full <- bitwShiftL(1L, n) - 1L
  dp <- new.env(parent=emptyenv()); prev <- new.env(parent=emptyenv())
  for (j in 1:n){ assign(key(bitwShiftL(1L, j-1L), j), 0, envir=dp); assign(key(bitwShiftL(1L, j-1L), j), NA_integer_, envir=prev) }
  for (mask in 1:full){
    for (j in 1:n){
      if (bitwAnd(mask, bitwShiftL(1L, j-1L))==0L) next
      kj <- key(mask,j); base <- get0(kj, envir=dp, ifnotfound=Inf)
      if (!is.finite(base)) next
      not_in <- bitwXor(mask, full)
      k <- 1L
      while (not_in>0L){
        lsb <- bitwAnd(not_in, -not_in)
        k <- as.integer(log(lsb, base=2)) + 1L
        not_in <- not_in - lsb
        new_mask <- bitwOr(mask, bitwShiftL(1L, k-1L))
        nk <- key(new_mask, k)
        cand <- base + C[j,k]
        if (cand < get0(nk, envir=dp, ifnotfound=Inf)){
          assign(nk, cand, envir=dp); assign(nk, j, envir=prev)
        }
      }
    }
  }
  # choose best end
  best_cost <- Inf; best_end <- NA_integer_
  for (j in 1:n){ v <- get0(key(full,j), envir=dp, ifnotfound=Inf); if (v < best_cost){ best_cost <- v; best_end <- j } }
  # reconstruct
  order <- integer(0); mask <- full; j <- best_end
  repeat{
    order <- c(j, order)
    pj <- get0(key(mask,j), envir=prev, ifnotfound=NA_integer_)
    if (is.na(pj)) break
    mask <- bitwXor(mask, bitwShiftL(1L, j-1L)); j <- pj
  }
  list(order=order, cost=best_cost)
}

# (B) Heuristic for larger n: greedy insertion + 2-opt (direction-aware for ATSP via arc swaps)
optimal_sequence_heuristic <- function(C){
  n <- nrow(C)
  rem <- seq_len(n)
  path <- rem[which.min(rowSums(C, na.rm=TRUE))]  # start from node with smallest outgoing sum
  rem <- setdiff(rem, path)
  while (length(rem)){
    best <- NULL; best_delta <- Inf; best_pos <- NA_integer_
    for (v in rem){
      for (pos in 0:length(path)){
        if (pos==0) delta <- C[v, path[1]] else if (pos==length(path)) delta <- C[path[length(path)], v] else delta <- C[path[pos], v] + C[v, path[pos+1]] - C[path[pos], path[pos+1]]
        if (delta < best_delta){ best_delta <- delta; best <- v; best_pos <- pos }
      }
    }
    if (best_pos==0) path <- c(best, path) else if (best_pos==length(path)) path <- c(path, best) else path <- c(path[1:best_pos], best, path[(best_pos+1):length(path)])
    rem <- setdiff(rem, best)
  }
  improve <- TRUE
  path_cost <- function(p){ sum(mapply(function(a,b) C[a,b], p[-length(p)], p[-1])) }
  cur <- path; curc <- path_cost(cur)
  while (improve){
    improve <- FALSE
    for (i in 1:(n-2)) for (k in (i+1):(n-1)){
      a<-cur[i]; b<-cur[i+1]; c<-cur[k]; d<-cur[k+1]
      delta <- (C[a,c] + C[b,d]) - (C[a,b] + C[c,d])
      if (delta < -1e-12){ cur <- c(cur[1:i], c, rev(cur[(i+1):k]), d, cur[(k+2):n]); curc <- curc + delta; improve <- TRUE }
    }
  }
  list(order=cur, cost=curc)
}

# (C) Optional: use TSP package (if installed) to get a path via dummy city trick
optimal_sequence_TSP <- function(C){
  if (!HAS_TSP) stop("Package 'TSP' not installed")
  AT <- TSP::ATSP(C)
  # Convert to symmetric TSP and insert a dummy city, then cut at dummy to get a path
  ST <- TSP::as.TSP(AT)
  ST2 <- TSP::insert_dummy(ST, label="DUMMY", method="median")
  sol <- TSP::solve_TSP(ST2, method = if ("concorde" %in% TSP::list_available_solvers()) "concorde" else "nearest_insertion")
  path <- TSP::cut_tour(sol, at = "DUMMY")
  labs <- TSP::labels(path)
  # map labels back to ATSP city indices
  # The conversion doubles cities; keep original names that match rownames(C)
  idx <- match(rownames(C), labs)
  idx <- idx[!is.na(idx)]
  # Build order by visiting labels in 'labs' that are in rownames(C)
  ord <- which(labs %in% rownames(C))
  order_names <- labs[ord]
  order <- match(order_names, rownames(C))
  cost <- sum(C[cbind(order[-length(order)], order[-1])])
  list(order=order, cost=cost)
}

# ---------------------------------------------------------------------
# Part 3. Diagnostics: rank quality vs observed events
# ---------------------------------------------------------------------
# For each event, compute Spearman/Kendall between activation times and a proposed order.

rank_diagnostics <- function(binary_times, order){
  X <- as.matrix(binary_times); n <- nrow(X)
  rises <- lapply(seq_len(n), function(i) rising_edges(X[i,]))
  all_events <- sort(unique(unlist(rises)))
  if (!length(all_events)) return(data.frame(event=integer(0), kendall=numeric(0), spearman=numeric(0)))
  res <- lapply(all_events, function(t0){
    act <- rep(Inf, n)
    for (i in seq_len(n)){
      r <- rises[[i]]; if (length(r)){ k <- findInterval(t0, r)+1; if (k>=1 && k<=length(r)) act[i] <- r[k] }
    }
    valid <- is.finite(act)
    if (sum(valid) < 3) return(c(kendall=NA_real_, spearman=NA_real_))
    # lower rank value means earlier in order
    ord_rank <- match(seq_len(n), order)
    kendall <- suppressWarnings(cor(act[valid], ord_rank[valid], method="kendall"))
    spearman <- suppressWarnings(cor(act[valid], ord_rank[valid], method="spearman"))
    c(kendall=kendall, spearman=spearman)
  })
  M <- do.call(rbind, res)
  data.frame(event = all_events, kendall = M[,1], spearman = M[,2])
}

# ---------------------------------------------------------------------
# Part 4. Visualizations
# ---------------------------------------------------------------------

plot_precedence_heatmap <- function(P){
  if (!HAS_GG) { warning("ggplot2 not installed; using base image()"); return({ image(P[nrow(P):1,], axes=FALSE, main="Precedence P(i->j)"); box() }) }
  df <- as.data.frame(as.table(P))
  names(df) <- c("i","j","val")
  ggplot2::ggplot(df, ggplot2::aes(x=j, y=i, fill=val)) +
    ggplot2::geom_tile(color="white") +
    ggplot2::scale_fill_viridis_c(limits=c(0,1)) +
    ggplot2::labs(x="j", y="i", fill="P(i before j)", title="Event precedence matrix") +
    ggplot2::theme_minimal()
}

plot_ranked_graph <- function(C, order, k_edges = 3){
  if (!HAS_IG){ warning("igraph not installed; skipping plot"); return(invisible(NULL)) }
  n <- nrow(C); nm <- if(is.null(rownames(C))) as.character(seq_len(n)) else rownames(C)
  G <- igraph::graph_from_adjacency_matrix(C, mode="directed", weighted=TRUE, diag=FALSE)
  # emphasize along-path edges
  path_e <- matrix(c(order[-length(order)], order[-1]), ncol=2)
  E(G)$color <- "grey80"; E(G)$width <- 1
  for (i in seq_len(nrow(path_e))){
    e <- igraph::get.edge.ids(G, vp=as.vector(path_e[i,]))
    if (e>0){ E(G)$color[e] <- "red"; E(G)$width[e] <- 2.5 }
  }
  V(G)$label <- nm
  V(G)$color <- "steelblue"
  V(G)$size <- 12
  V(G)$rank <- match(seq_len(n), order)
  lay <- igraph::layout_with_sugiyama(G)$layout
  plot(G, layout=lay, main="Ranked wetting path (red edges)")
}

# ---------------------------------------------------------------------
# Part 5. Small worked example
# ---------------------------------------------------------------------
if (identical(Sys.getenv("RUN_WETSEQ_EXAMPLE"), "1")){
  set.seed(1)
  n <- 6; Tt <- 50
  # Simulate staggered wetting in blocks
  true_order <- sample(n)
  X <- matrix(0L, n, Tt)
  for (evt in c(10, 30)){
    delays <- sort(sample(0:5, n, replace=TRUE))
    X[cbind(seq_len(n), pmin(Tt, evt + delays))] <- 1L
    for (i in seq_len(n)) for (t in (evt+delays[i]):min(Tt, evt+delays[i]+5)) X[i,t] <- 1L
  }
  rownames(X) <- paste0("N", seq_len(n))
  P <- event_precedence_matrix(X)
  C <- cost_from_precedence(P, transform="neglog")
  res <- optimal_sequence_from_cost(C)
  print(list(order=rownames(X)[res$order], cost=res$cost))
  if (HAS_GG) print(plot_precedence_heatmap(P))
  if (HAS_IG) plot_ranked_graph(C, res$order)
}


#New stuff
# --- Pairwise counts/tests
pairwise_event_counts <- function(binary_times){
  X <- as.matrix(binary_times); n <- nrow(X)
  rising_edges <- function(x) if (length(x)<2) integer(0) else which(x[-length(x)]==0 & x[-1]==1)+1
  rises <- lapply(seq_len(n), function(i) rising_edges(X[i,]))
  all_events <- sort(unique(unlist(rises)))
  stopifnot(length(all_events) > 0)
  W <- matrix(0L,n,n); L <- matrix(0L,n,n); TIE <- matrix(0L,n,n)
  for (t0 in all_events){
    act <- rep(Inf,n)
    for (i in seq_len(n)){
      r <- rises[[i]]; if (length(r)){ k <- findInterval(t0, r)+1; if (k>=1 && k<=length(r)) act[i] <- r[k] }
    }
    for (i in seq_len(n)) for (j in seq_len(n)){
      if (i==j || !is.finite(act[i]) || !is.finite(act[j])) next
      if (act[i] < act[j]) W[i,j] <- W[i,j] + 1L
      else if (act[i] > act[j]) L[i,j] <- L[i,j] + 1L
      else TIE[i,j] <- TIE[i,j] + 1L
    }
  }
  labs <- rownames(binary_times); rownames(W)<-colnames(W)<-labs
  rownames(L)<-colnames(L)<-labs; rownames(TIE)<-colnames(TIE)<-labs
  list(wins=W, losses=L, ties=TIE)
}

pairwise_precedence_tests <- function(binary_times, alpha=0.05, conf.level=0.95){
  cnt <- pairwise_event_counts(binary_times); W<-cnt$wins; L<-cnt$losses
  n <- nrow(W); labs <- rownames(W); out <- list()
  for (i in seq_len(n)) for (j in seq_len(n)) if (i!=j){
    s <- W[i,j]; r <- L[i,j]; nn <- s+r
    if (nn>0){
      bt <- suppressWarnings(binom.test(s, nn, 0.5, conf.level=conf.level))
      out[[length(out)+1]] <- data.frame(
        i=labs[i], j=labs[j], s=s, r=r, n=nn,
        phat = s/nn, p_two_sided = bt$p.value,
        ci_low = bt$conf.int[1], ci_high = bt$conf.int[2]
      )
    }
  }
  do.call(rbind, out)
}

precedence_graph <- function(binary_times, alpha=0.05){
  tests <- pairwise_precedence_tests(binary_times, alpha)
  keep <- subset(tests, phat>0.5 & p_two_sided < alpha)
  if (!requireNamespace("igraph", quietly=TRUE)) return(list(graph=NULL, edges=keep, tests=tests))
  g <- igraph::graph_from_data_frame(keep[,c("i","j")], directed=TRUE)
  list(graph=g, edges=keep, tests=tests)
}

# ---------------------------------------------------------------------
# Part 3. Diagnostics: statistical evidence that "earlier" nodes activate first
# ---------------------------------------------------------------------
# This section addresses: "how can I know that a node earlier in the sequence
# actually tends to activate before a node later in the sequence?"
#
# We provide three types of checks:
# 1. Pairwise consistency: for each pair (i,j), how often does i activate before j.
# 2. Event-level correlation: for each wetting event, compute correlation between
#    observed activation times and the proposed rank order.
# 3. Order confidence: binomial test for each adjacent pair in the proposed order
#    (plus first vs last).

pairwise_consistency <- function(binary_times, order){
  P <- event_precedence_matrix(binary_times)
  n <- length(order)
  out <- matrix(NA, n, n, dimnames=list(order, order))
  for (ii in seq_len(n)) for (jj in seq_len(n)){
    if (ii==jj) next
    i <- order[ii]; j <- order[jj]
    out[ii,jj] <- P[i,j]
  }
  out
}

rank_diagnostics <- function(binary_times, order){
  X <- as.matrix(binary_times); n <- nrow(X)
  rises <- lapply(seq_len(n), function(i) rising_edges(X[i,]))
  all_events <- sort(unique(unlist(rises)))
  if (!length(all_events)) return(data.frame(event=integer(0), kendall=numeric(0), spearman=numeric(0)))
  res <- lapply(all_events, function(t0){
    act <- rep(Inf, n)
    for (i in seq_len(n)){
      r <- rises[[i]]; if (length(r)){ k <- findInterval(t0, r)+1; if (k>=1 && k<=length(r)) act[i] <- r[k] }
    }
    valid <- is.finite(act)
    if (sum(valid) < 3) return(c(kendall=NA_real_, spearman=NA_real_))
    ord_rank <- match(seq_len(n), order)
    kendall <- suppressWarnings(cor(act[valid], ord_rank[valid], method="kendall"))
    spearman <- suppressWarnings(cor(act[valid], ord_rank[valid], method="spearman"))
    c(kendall=kendall, spearman=spearman)
  })
  M <- do.call(rbind, res)
  data.frame(event = all_events, kendall = M[,1], spearman = M[,2])
}

# ---------------------------------------------------------------------
# New: Order confidence testing
# ---------------------------------------------------------------------
order_confidence <- function(binary_times, order){
  # reuse event precedence counts
  cnt <- event_precedence_matrix(binary_times)
  nm <- rownames(cnt)
  stopifnot(length(order)>=2)
  adjacent <- lapply(seq_len(length(order)-1), function(k){
    a <- order[k]; b <- order[k+1]
    s <- cnt[a,b]; r <- cnt[b,a]; nn <- s+r
    if (nn>0){
      bt <- suppressWarnings(binom.test(s, nn, 0.5))
      data.frame(pair = sprintf("%s -> %s", a, b), phat = s/nn,
                 ci_low = bt$conf.int[1], ci_high = bt$conf.int[2],
                 p_two_sided = bt$p.value, n = nn)
    } else {
      data.frame(pair = sprintf("%s -> %s", a, b), phat = NA,
                 ci_low = NA, ci_high = NA, p_two_sided = NA, n = 0)
    }
  })
  adjacent <- do.call(rbind, adjacent)
  a <- order[1]; b <- order[length(order)]; s <- cnt[a,b]; r <- cnt[b,a]; nn <- s+r
  if (nn>0){
    bt <- suppressWarnings(binom.test(s, nn, 0.5))
    firstlast <- data.frame(first=a, last=b, phat=s/nn, ci_low=bt$conf.int[1],
                            ci_high=bt$conf.int[2], p_two_sided=bt$p.value, n=nn)
  } else {
    firstlast <- data.frame(first=a, last=b, phat=NA, ci_low=NA, ci_high=NA,
                            p_two_sided=NA, n=0)
  }
  list(adjacent=adjacent, first_vs_last=firstlast)
}

# ---------------------------------------------------------------------
# Interpretation tips
# ---------------------------------------------------------------------
# - pairwise_consistency(): If your sequence is correct, entries above the diagonal
#   should mostly be >0.5 (earlier nodes precede later ones in data).
# - rank_diagnostics(): Kendall/Spearman near +1 means strong agreement between
#   the proposed global order and actual event timings.
# - order_confidence(): For each adjacent pair, check phat (fraction of times the
#   earlier node wets first), CI, and p-value. Also check first vs last for a
#   strong sanity check.


order_violations <- function(binary_times, order){
  cnt <- pairwise_event_counts(binary_times); W<-cnt$wins; L<-cnt$losses
  n <- length(order); viol <- 0; total <- 0
  for (a in 1:(n-1)) for (b in (a+1):n){
    i<-order[a]; j<-order[b]; s<-W[i,j]; r<-L[i,j]
    if (s+r>0){ total <- total+1; if (s/(s+r) <= 0.5) viol <- viol+1 }
  }
  list(violations=viol, total_pairs=total, rate=if(total>0) viol/total else NA_real_)
}

```
```{r}
#create matrix for input
w3_matrix <- 
  filter(data_23, wshed == "W3") %>%
  filter(datetime >= ymd_hms("2023-07-22 18:00:00"),
         datetime <= ymd_hms("2023-09-18 11:00:00")) %>% 
    #filter(is.na(binary))
  select(datetime, ID, binary) %>% 
  #drop_na() %>% 
  #drop_na() %>% 
  pivot_wider(names_from = datetime, values_from = binary) #%>% 
w3_matrix2 <- longest_complete_chunk %>% 
  #drop_na() %>% View()
  select(-ID) %>%
  data.matrix()# %>% View()

P <- event_precedence_matrix(w3_matrix2)                 # pairwise P(i before j)
C <- cost_from_precedence(P, transform = "neglog")
res <- optimal_sequence_from_cost(C)

plot_precedence_heatmap(P)
plot_ranked_graph(C, res$order)

# You already have: X, res$order from the optimization
conf <- order_confidence(w3_matrix2, res$order)
conf$adjacent          # per-adjacent-pair support (phat, CI, p-value)
conf$first_vs_last     # strongest check: earliest vs latest

order_violations(X, res$order)   # fraction of pairwise contradictions

pg <- precedence_graph(X, alpha=0.05)
pg$edges                # only statistically supported directions
# If igraph installed:
# plot(pg$graph, vertex.size=20, edge.arrow.size=0.4)


# Example: 3 nodes  10 time steps
X <- matrix(c(
  # Node A: wets early
  0,0,1,1,1,1,1,1,1,1,
  # Node B: wets later
  0,0,0,0,1,1,1,1,1,1,
  # Node C: wets last
  0,0,0,0,0,0,1,1,1,1
), nrow=3, byrow=TRUE)

rownames(X) <- c("A","B","C")

# Suppose we hypothesize the order A -> B -> C
order <- c("A","B","C")

# --- Pairwise consistency ---
pc <- pairwise_consistency(X, order)
print(pc)

# --- Rank diagnostics ---
rd <- rank_diagnostics(X, order)
print(rd)

# --- Order confidence tests ---
oc <- order_confidence(X, order)
print(oc$adjacent)
print(oc$first_vs_last)

```

```{r}
find_largest_complete_chunk <- function(data) {
  # 1. Identify complete rows
  complete_rows <- complete.cases(data)
  
  # 2. Use rle() to find consecutive sequences
  rle_result <- rle(complete_rows)
  
  # 3. Find the longest complete chunk and its position
  complete_runs_lengths <- rle_result$lengths[rle_result$values]
  if (length(complete_runs_lengths) == 0) {
    message("No complete chunks found.")
    return(NULL)
  }
  max_length <- max(complete_runs_lengths)
  max_index_in_rle <- which(rle_result$values)[which.max(complete_runs_lengths)]
  
  # Calculate the starting row of the longest chunk
  start_row <- sum(rle_result$lengths[1:(max_index_in_rle - 1)]) + 1
  
  # 4. Extract and return the chunk
  data[start_row:(start_row + max_length - 1), ]
}

# Run the function on your data frame
longest_complete_chunk <- find_largest_complete_chunk(w3_matrix)
print(longest_complete_chunk)
#   id value_a value_b
# 4  4       4       4
# 5  5       5       5
# 6  6       7       6

```

Trying gpt one more time
```{r}
p_load(arulesSequences)

#create matrix, where rows are timesteps and columns are individual nodes

X <- input_w3 %>% filter(datetime >= ymd_hms("2023-07-30 00:00:00"),
                    datetime <= ymd_hms("2023-09-17 00:00:00")) %>% 
  select(datetime, ID, binary) %>% 
  pivot_wider(names_from = ID, values_from = binary) %>% #filter(if_any(everything(), is.na)) %>% View()
  select(-datetime) %>% 
  data.matrix()

# Convert to sequence data: which nodes are active each timestep
events <- apply(X, 1, function(row) colnames(X)[which(row==1)])
events

# Build sequence data frame
seqdata <- do.call(rbind, lapply(seq_along(events), function(t) {
  data.frame(
    sequenceID = 1,      # single global sequence here; use multiple IDs if multiple runs
    eventID    = t,      # timestep index
    size       = length(events[[t]]),
    items      = paste(events[[t]], collapse=",")
  )
}))

seqdata

# Write to temporary file in the expected basket format
tmpfile <- tempfile()
write.table(seqdata,
            file = tmpfile,
            sep = " ",
            row.names = FALSE,
            col.names = FALSE,
            quote = FALSE)

# Read back in as timedsequences
seqs <- read_baskets(con = tmpfile,
                     info = c("sequenceID","eventID","SIZE"))

# Run CSPADE (Apriori-style sequence mining)
patterns <- cspade(seqs, parameter = list(support = 0.5, maxlen = 3))
inspect(patterns)
# Convert to "timedsequences"
seqs <- as((seqdata), "timedsequences")

# Run Apriori-based sequential pattern mining (CSPADE)
patterns <- cspade(seqs, parameter = list(support = 0.2, maxlen = 3))
inspect(patterns)

seqs <- as(seqdata, "transactions")
patterns <- cspade(seqs, parameter = list(support = 0.3, maxlen = 3))  
inspect(patterns)

```

That algorithm was not quite suitable, but maybe this will be
```{r}
make_serial_windows <- function(mat, window_size=4) {
  n <- nrow(mat)
  sequences <- lapply(1:(n-window_size+1), function(i) {
    window <- mat[i:(i+window_size-1), , drop=FALSE]
    # collect active nodes in order of time
    seq <- unlist(apply(window, 1, function(row) colnames(mat)[which(row==1)]))
    seq
  })
  return(sequences)
}

windows <- make_serial_windows(X, window_size=4)
windows


# Convert to seqdata format
seqdata <- data.frame(
  sequenceID = rep(seq_along(windows), sapply(windows, length)),
  eventID    = unlist(lapply(windows, seq_along)),
  items      = unlist(windows)
)

# Must coerce items to factor
seqdata$items <- as.factor(seqdata$items)

# Write to temporary file in basket format
tmpfile <- tempfile()
write.table(seqdata,
            file = tmpfile,
            sep = " ",
            row.names = FALSE,
            col.names = FALSE,
            quote = FALSE)

# Read as timedsequences
seqs <- read_baskets(con = tmpfile, info = c("sequenceID","eventID"))

# Mine sequential patterns
patterns <- cspade(seqs, parameter = list(support=0.3, maxlen=4))
inspect(patterns[1:5])

# Convert to data frame
df_patterns <- as(patterns, "data.frame")

# Count number of steps in each pattern
df_patterns$length <- sapply(df_patterns$sequence, function(x) {
  # count number of {} blocks
  length(gregexpr("\\{", x)[[1]])
})

# Keep only those with length >= 2
df_patterns_filtered <- subset(df_patterns, length >= 2)

head(df_patterns_filtered, 10)


# Count bigrams (A -> B transitions)
library(stringr)

transitions <- data.frame(from=character(), to=character(), count=integer())
#this runs way too long
for (s in df_patterns$sequence) {
  steps <- str_extract_all(s, "\\{[^}]+\\}")[[1]]
  steps <- gsub("[{}]", "", steps)
  steps <- unlist(strsplit(steps, ","))
  if (length(steps) >= 2) {
    for (i in 1:(length(steps)-1)) {
      transitions <- rbind(
        transitions,
        data.frame(from=steps[i], to=steps[i+1], count=1)
      )
    }
  }
}

# Aggregate counts
transitions <- transitions %>%
  group_by(from, to) %>%
  summarise(weight = sum(count), .groups="drop") %>%
  arrange(desc(weight))

head(transitions, 10)


```
```{r}
make_transition_matrix <- function(mat) {
  n_nodes <- ncol(mat)
  trans <- matrix(0, n_nodes, n_nodes,
                  dimnames = list(colnames(mat), colnames(mat)))
  
  for (t in 1:(nrow(mat)-1)) {
    active_now  <- which(mat[t, ] == 1)
    active_next <- which(mat[t+1, ] == 1)
    for (i in active_now) {
      for (j in active_next) {
        trans[i, j] <- trans[i, j] + 1
      }
    }
  }
  return(trans)
}

Tmat <- make_transition_matrix(X)
Tmat

Pmat <- prop.table(Tmat, margin=1)  # row-stochastic (each row sums to 1)
round(Pmat, 2)

library(igraph)

g <- graph_from_adjacency_matrix(Pmat, mode="directed", weighted=TRUE)
plot(g,
     edge.width = E(g)$weight * 5,
     edge.arrow.size = 0.5,
     vertex.size = 30,
     vertex.color = "skyblue",
     vertex.label.color = "black")

```

#ITERATING SMART ONCE AGAIN
Doing by BFS algorithm that I accidentally invented lol
```{r iterating-smart-W3}
#pick a sensor randomly, then test every other sensor to see which has the best hierarchical behavior with that one. Proceed down the list without replacement
options(dplyr.summarise.inform = FALSE)
options(dplyr.mutate_all.inform = FALSE)


#combos <- unique(combos$up)
#with modern function, do not need input
#[1] "2025-09-10 10:12:20 EDT"
combos <- W3_IDs
#find the starting point

for(t in 1:length(combos)){
 #t <- 1
begin <- combos[t]
options_initial <- combos[combos != begin]
options <- combos[combos != begin]

routes <- data.frame("up" = rep(begin, length(options_initial)),
                     "down" = options_initial)
 for(x in 1:length(options)){
 #x <- 1
 #calc_props(routes[x], "W3")
        out <- calc_props(routes, "W3") %>% filter(timescale == "30mins")
        
        kep <- out[out$prop == max(out$prop),] #find the max prop value
        kep <- kep[1,] #if tied, keep the first one/row
  options <- options[options != substr(kep$down[1], 3, 4)] #create new options list
  routes <- tibble("up" = substr(rep(kep$down[1], length(options)), 3, 4),
                     "down" = options)
  print(paste0("x = ",x))

  if(x == 1) big_keep <- kep
  if(x > 1) big_keep <- rbind(big_keep, kep)
 }


big_keep$start <- begin

if(t == 1) biggest_keep <- big_keep
  if(t > 1) biggest_keep <- rbind(biggest_keep, big_keep)
}
Sys.time()
#[1] "2025-09-10 10:26:42 EDT"


smart_iterate_w3_new <- biggest_keep
write_csv(smart_iterate_w3_new, "breadthfs_results.csv")
View(smart_iterate_w3_new)
smart_iterate_w3_new %>% group_by(start) #%>% 
  ggplot()+
  geom_density(data = smart_iterate_w3_new, 
               aes(x = prop, y = after_stat(density), color = as.factor(start)))+
  geom_density(data = filter(all_pk, timescale == "30mins" | shed == "W3"))+
  aes(x = prop, y = after_stat(density))#+
  
  
  smart_iterate_w3_new %>% 
    select(start, up)
    group_by(start) %>% 
    nest(sequence = as.list(up))

```


This is actually a breadth-first-search algorithm....
```{r}

starts <- unique(smart_iterate_w3_new$start)

starts[1]
smart_iterate_w3_new %>% 
  filter(start == starts[1]) %>% 
  as_tibble() %>% 
  select(up, down) %>% 
  #mutate(up = substr(up, 3, 4)) %>% 
  convert_to_IDseq() %>% 
  mutate(ID = as.numeric(substr(ID, 3, 4)))

#testing every method
for(i in 1:length(starts)){

  chain <- smart_iterate_w3_new %>% 
    filter(start == starts[i]) %>% 
    as_tibble() %>% 
    select(up, down) %>% 
    convert_to_IDseq() %>% 
    mutate(ID = as.numeric(substr(ID, 3, 4)))
  
  output <- produce_metrics(chain, "W3", starts[i])
  
  if(i == 1) many_chains <- output
  if(i > 1) many_chains <- rbind(many_chains, output)
}

all_methods_acc <- many_chains

View(all_methods_acc)
all_methods_acc %>% filter(Accuracy == max(Accuracy))


smart_iterate_w3_new

```
