---
title: "TopModel Test"
output: html_document
date: '2022-09-12'
---

9/12/2022
Downloading and playing with TopModel package, just finished reading Hornberger paper.

```{r}
install.packages('topmodel')
```

```{r, echo = false, warning=FALSE}
library(tidyverse)
library(topmodel)
library(raster)
library(rgdal)
library(spdplyr)
library(lubridate)
```

```{r}
data(huagrahuma)

h <- huagrahuma
test <- topmodel(h$parameters, h$topidx, h$delay, h$rain, h$ETp, verbose = F, Qobs = NA)

time <- seq(1, length(test), 1)

plot(time, test)
points(time, h$Qobs, col = "red")
```
Example from the documentation: not that good
```{r}
DEM <- read.table("DEM.txt")

DEM <- as.matrix(DEM)

# Remove the values outside the catchment:

DEM[DEM==-9999] <- NA

# You may want to plot the DEM to see whether everything looks OK:

image(DEM)

# Then calculate the topographic index, the resolution should be in [m].
# Here we use the DEM from Huagrahuma as an example:

data(huagrahuma.dem)
DEM <- sinkfill(huagrahuma.dem, res=25, degree=0.1)
topindex <- topidx(DEM, resolution=25)

# The values need to be split into a set of classes, since topmodel() is a semidistributed model that lumps hydrologically similar areas into the same hydrological response units.
# Here we define 16 hydrological response units:

topidx1 <- make.classes(topindex$atb,16)
topidx2 <- make.classes(topindex$area,16)

# the delay function is a bit more tricky because this requires cumulative fractions, but you generate it as follows:

n <- 5 # number of classes; a higher number will result in a smoother histogram
delay <- flowlength(huagrahuma.dem)*25 #TOD: add the outlet coordinates; otherwise the flowlength will be calculated to the edge of the map.
delay <- make.classes(delay, n)
delay <- delay[n:1,]
delay[,2] <- c(0, cumsum(delay[1:(n-1),2]))

############ PART 1: running the rainfall-runoff model ##############

## Load the example dataset from the Huagrahuma catchment
## and attach it to the search path

data(huagrahuma)
attach(huagrahuma)

## Initial exploration of the data:

str(huagrahuma)
topidx
parameters
rain

plot(rain, type="h")

## run the model and visualise the outcome:

Qsim <- topmodel(parameters, topidx, delay, rain, ET0)
plot(Qsim, type="l", col="red")
points(Qobs)

## Evaluate the model with a performance metric

NSeff(Qobs, Qsim)

############ PART 2: Sensitivity analysis ##############

## let's try first to vary only one parameter
## The function runif() samples randomly from a uniform distribution

parameters["m"] <- runif(1, min = 0, max = 0.1)
parameters["m"]

## Run the model and evaluate with the Nash – Sutcliffe efficiency metric:

Qsim <- topmodel(parameters, topidx, delay, rain, ET0)
NSeff(Qobs, Qsim)

## What value do you get? Do you think this is a good simulation?
## Verify by plotting:

plot(Qsim, type="l", col="red")
points(Qobs)

## Now sample all parameters at random. We take a sample size of 100

n <- 100

qs0 <- runif(n, min = 0.0001, max = 0.00025)
lnTe <- runif(n, min = -2, max = 3)
m <- runif(n, min = 0, max = 0.1)
Sr0 <- runif(n, min = 0, max = 0.2)
Srmax <- runif(n, min = 0, max = 0.1)
td <- runif(n, min = 0, max = 3)
vch <- runif(n, min = 100, max = 2500)
vr <- runif(n, min = 100, max = 2500)
k0 <- runif(n, min = 0, max = 10)
CD <- runif(n, min = 0, max = 5)
dt <- 0.25

parameters <- cbind(qs0,lnTe,m,Sr0,Srmax,td,vch,vr,k0,CD,dt)

## run the model and evaluate with the Nash – Sutcliffe efficiency metric:
## Note: the function accepts a table of parameter sets
## (one parameter set per row)

NS <- topmodel(parameters, topidx, delay, rain, ET0, Qobs = Qobs)
max(NS)

## visualisation of the sensitivity using dotty plots:

plot(lnTe, NS, ylim = c(0,1))

############ PART 3: GLUE uncertainty analysis ##############

## choose a behavioural threshold and remove the “bad” parameter sets:

parameters <- parameters[NS > 0.3,]
NS <- NS[NS > 0.3]

## generate predictions for the behavioural parameter sets:

Qsim <- topmodel(parameters,topidx,delay,rain,ET0)

## (have a look at the predictions for the first time step:)

hist(Qsim[1,])

## construct weights based on the performance measure

weights <- NS - 0.3
weights <- weights / sum(weights)

## make prediction boundaries by weighted quantile calculation
## (we need the Hmisc package for that)

limits <- apply(Qsim, 1, "wtd.quantile", weights = weights,
probs = c(0.05,0.95), normwt=T)

plot(limits[2,], type="l")
points(limits[1,], type="l")
points(Qobs, col="red")

## how many measurements fall outside of the prediction limits?

outside <- (Qobs > limits[2,]) | (Qobs < limits[1,])
summary(outside)

## width of the prediction boundaries

mean(limits[2,] - limits[1,]) / mean(Qobs, na.rm=T)
```

9/14/22
Reading the references from topmodel documentation, downloaded HB data, going to take a crack at deriving some of the components.
9/26/22
Haven't worked on this at all since last update. Going to try to make a map and calculate topographic wetness index

```{r, warning=FALSE}
#setwd("C:/Users/John/Documents/TopModel/HB/knb-lter-hbr.211.2")
#dem <- raster("dem1m.tif")
setwd("C:/Users/John/Documents/TopModel/HB/hbef_wsheds")

w3 <- readOGR(dsn='.',layer = 'hbef_wsheds')

w3 <- w3 %>% filter(WS == "WS3")
plot(w3)

w3_p <- spTransform(w3, crs(dem))
dem_crop <- crop(dem, w3_p)
w3_dem <- mask(dem_crop, w3_p)

plot(w3_dem)

#convert DEM to matrix
mat <- raster::as.matrix(dem_crop)
mat[mat==-9999] <- NA
topindex <- topidx(mat, resolution=1)

topidx1 <- make.classes(topindex$atb,16)
topidx2 <- make.classes(topindex$area,16)

# the delay function is a bit more tricky because this requires cumulative fractions, but you generate it as follows:

n <- 5 # number of classes; a higher number will result in a smoother histogram
which(mat == min(mat), arr.ind = TRUE)

delay <- flowlength(mat, c(837, 176))*1 #TOD: add the outlet coordinates; otherwise the flowlength will be calculated to the edge of the map.
delay <- make.classes(delay, n)
delay <- delay[n:1,]
delay[,2] <- c(0, cumsum(delay[1:(n-1),2]))
```

```{r}
h <- huagrahuma
#using a combination of variables from example data and HB
#precip
rain <- read.csv("HB/dailyWatershedPrecip1956-2021.csv")
rain$DATE <- ymd(rain$DATE)
rain2 <- filter(rain, watershed == "W3") %>% 
  filter(DATE > ymd("2020-01-01") & DATE < ymd("2021-01-01"))

#discharge
q <- read.csv("HB/w3_stmflow_2013-2022_5min.csv")
q$DATETIME <- ymd_hms(q$DATETIME)
q2 <- q %>%
  mutate(date = floor_date(DATETIME, unit = "day")) %>%
  group_by(date) %>%
  summarise(
    n = n(),
    q_total = sum(Discharge_ls),
    q_average = mean(Discharge_ls),
    .groups = "drop"
  ) %>% 
  filter(date > ymd("2020-01-01") & date < ymd("2021-01-01"))


test <- topmodel(h$parameters, topidx1, h$delay, rain2$Precip, h$ETp, verbose = F, Qobs = NA)

plot(test, type="l", col="red", ylim = c(0, 20))
points(q2$q_average)
```

