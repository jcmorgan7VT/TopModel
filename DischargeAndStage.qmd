---
title: "Process_discharge"
format: html
editor_options: 
  chunk_output_type: console
---

10/14/24: script to process salt dilutions

11/20/24: get data from excel
```{r}
library(googlesheets4)
test <- read_sheet("https://docs.google.com/spreadsheets/d/1Km4mXI5AUgNl1oZDUaRBBOnmHCKc2zrmsMA8KyPWWY0/edit?gid=0#gid=0")

test2 <- read_sheet("https://docs.google.com/spreadsheets/d/1Km4mXI5AUgNl1oZDUaRBBOnmHCKc2zrmsMA8KyPWWY0/edit?gid=2089142478#gid=2089142478")

read_sheet("https://docs.google.com/spreadsheets/d/1Km4mXI5AUgNl1oZDUaRBBOnmHCKc2zrmsMA8KyPWWY0/edit?gid=142396057#gid=142396057")
```


```{r}
#script uploaded to google drive

V <- 5e-5 #volume of slug [m^3]
k <- #slope of the line between cond and relative cond from calibration tank

#relative conductivity calculations
#secondary solution
X <- 10 #volume of salt slug solution added to 1 L of secondary solution [mL]
V0 <- 1000 #volume of water in secondary solution [mL]
RCsec <- X / (V0 + X) #relative cond of secondary solution
#calibration tank
EC0 <-  #background cond of calibration tank [micro S/cm]
#calculate relative cond at each time step
#amount of secondary solution added
Dsec <- 10 # [mL]
timesteps <- 6

RC <- seq(0, timesteps, 1)
RC <- RCsec * RC * Dsec /(V0 + RC * Dsec) + (RC * Dsec) #Vc, volume of secs at each timestep

#input actual conductivity at each timestep
EC <- c(EC0, 11.9, 13.1, 13.4, 15, 16.1, 19.1)

plot(EC, RC)
#calculate k
k <- lm(EC~RC)$coefficients[2]

#back to calculating Q
DeltaT <- 10 #[s]

##calculate sum of change in conductivity minus background
sum <- 

Q <- V / (7.7 * DeltaT * sum)
Q_Ls <- Q * 1000

######ATTEMPT 2
# 7/31/24

# measurement from 7/4/24, bagley brook trail MAIN TRIB

V <- 7.5e-5 #volume of slug [m^3]
k <- #slope of the line between cond and relative cond from calibration tank

  #relative conductivity calculations
  #secondary solution
  X <- 10 #volume of salt slug solution added to 1 L of secondary solution [mL]
V0 <- 1000 #volume of water in secondary solution [mL]
RCsec <- X / (V0 + X) #relative cond of secondary solution
#calibration tank
EC0 <- 9.4 #background cond of calibration tank [micro S/cm]
#calculate relative cond at each time step
#amount of secondary solution added
Dsec <- 10 # [mL]
timesteps <- 6

RC <- seq(0, timesteps, 1)
RC <- RCsec * RC * Dsec /(V0 + RC * Dsec) + (RC * Dsec) #Vc, volume of secs at each timestep

#input actual conductivity at each timestep
EC <- c(EC0, 9.4, 12.4, 12.4, 12, 12.4, 19.2)

plot(EC, RC)
#calculate k
k <- unname(lm(EC~RC)$coefficients[2])

#back to calculating Q
DeltaT <- 10 #[s]

#calculate sum of change in conductivity minus background
sum <- 196.3

Q <- V / (k * DeltaT * sum)
Q_Ls <- Q * 1000


```

```{r}


#measurement from bagley main trib 6_25
vol <- 100 #volume of slug [mL]
V <- vol * 1e-6 #volume of slug [m^3]

  #relative conductivity calculations
  #secondary solution
  X <- 10 #volume of salt slug solution added to 1 L of secondary solution [mL]
V0 <- 1000 #volume of water in secondary solution [mL]
RCsec <- X / (V0 + X) #relative cond of secondary solution
#calibration tank
EC0 <- 8.5 #background cond of calibration tank, should be the same as upstream [micro S/cm]
#calculate relative cond at each time step
#amount of secondary solution added
Dsec <- 10 # [mL]
timesteps <- 5

RC <- seq(0, timesteps, 1)
RC <- RCsec * RC * Dsec /(V0 + RC * Dsec) + (RC * Dsec) #Vc, volume of secs at each timestep

#input actual conductivity at each timestep
EC <- c(EC0, 8.5, 8.9, 8.9, 10.4, 11.2)

plot(EC, RC)
#calculate k
#slope of the line between cond and relative cond from calibration tank
# form of linear reg: Y ~ X
k <- unname(lm(EC~RC)$coefficients[2])

#back to calculating Q
DeltaT <- 10 #[s]

#calculate sum of change in conductivity minus background
sum <- 355.8

Q <- V / (k * DeltaT * sum)
Q_Ls <- Q * 1000

```

```{r}
#develop method to process read in files
#find files
folder_path <- "./salt_dilutions/FB_8_1"
files <- list.files(folder_path)
#sensor 256 appears to always measure a little higher, and 258 a little lower. Can experimentally determine if one of them is objectively correct, but for now I am going to assume they are both a little wrong.
#conductivity units: μS/cm
cond <- read_csv(paste0(folder_path,"/", files[1])) #%>% 
  mutate(cond_up = ceiling(cond_up),
         cond_down = floor(cond_down),
         delta_cond = cond_up - cond_down,
         time_down = mdy_hms(time_down),
         time_up = mdy_hms(time_up))

k <- read_csv(paste0(folder_path,"/", files[2])) %>% 
  rename(cond = `cond_k_μS/cm`) #%>% 
  #mutate(cond = ceiling(cond))

meta <- read_csv(paste0(folder_path,"/", files[3]))

#now taking these inputs and use them to calculate
vol <- meta$slugVolume_mL #volume of slug [mL]
V <- vol * 1e-6 #volume of slug [m^3]

#relative conductivity calculations
#secondary solution- set up for standard solution
X <- 10 #volume of salt slug solution added to 1 L of secondary solution [mL]
V0 <- 1000 #volume of water in secondary solution [mL]
RCsec <- X / (V0 + X) #relative cond of secondary solution

#calibration tank
EC0 <- k$cond[1] #background cond of calibration tank, should be the same as upstream [micro S/cm]
#calculate relative cond at each time step
#amount of secondary solution added
Dsec <- 10 # [mL]
timesteps <- na.omit(k$slugadded_mL) %>% length()

RC <- seq(0, timesteps, 1)
RC <- RCsec * RC * Dsec /(V0 + RC * Dsec) + (RC * Dsec) #Vc, volume of secs at each timestep

#input actual conductivity at each timestep
ECs <- drop_na(k)$cond
EC <- c(EC0, ECs)

plot(EC, RC)
#calculate k
#slope of the line between cond and relative cond from calibration tank
# form of linear reg: Y ~ X
k <- unname(lm(EC~RC)$coefficients[2])

#back to calculating Q
DeltaT <- 10 #[s]
(cond$time_down[1] - cond$time_down[2])

#calculate sum of change in conductivity minus background
sum <- 355.8

Q <- V / (k * DeltaT * sum)
Q_Ls <- Q * 1000

```

Skipping just to fitting a model to stage
```{r}
#creating falls brook twi values
#reading in final format data for summer 23
data_23 <- read_csv("./DataForMary/HB_stic.csv")
#reading in final format data for summer 24
data_24 <- read_csv("./summer2024/STICS2024.csv")

#plot locations of sensors
locs <- data_23 %>% 
  filter(wshed == "FB") %>% 
  select(ID, lat, long) %>% 
  unique()

locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")



dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
plot(m10)
lcc <- terra::project(locs_shape,crs(m10))

#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./fb_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./fb_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./fb_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./fb_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

flowacc_output <- "./fb_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")

slope_output <- "./fb_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")

twi_output <- "./fb_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

twi2 <- terra::rast(twi_output)
plot(twi2)

#just extract values from 10 m twi raster
FB_twi_ex <- extract(twi2, lcc)

```


```{r}
#chunk that reads in stage, converts to proper units
#read in stage, convert to a height
FB_air <- read_csv("./PressureTransducers_11_14_23/FB_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water <- read_csv("./PressureTransducers_11_14_23/FB_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  ggplot(aes(x = DATETIME, y = stage_cm))+
  geom_line()+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")
#convert pressure to height of water
#h = p / (ρ * g)
#h = height in m
#density of water = 997 kg/m³
#gravitational acceleration = 9.8 m/s²
#convert psi to pascals, 1 psi = 6894.76 bars

library(patchwork)
fbAirPlot <- ggplot(FB_air, aes(x = DATETIME, y = pressure_psi_air))+
  geom_line()+
  theme_classic()+
  ggtitle("Falls Brook Air pressure")
fbWaterPlot <- ggplot(FB_water, aes(x = DATETIME, y = pressure_psi_water))+
  geom_line()+
  theme_classic()+
  ggtitle("Falls Brook Water pressure")


fbAirPlot / fbWaterPlot

data_23$mins <- minute(data_23$datetime)

#Fit a logistic regression to falls brook stage
#outputs df with stage in cm
FB_stage <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701)

ready2split <-  data_23 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  left_join(twi_ex, by = "ID") %>% 
  rename("DATETIME" = datetime,
         "twi" = '10mdem_twi') %>% 
  right_join(select(FB_stage, c(DATETIME, stage_cm)), by = "DATETIME") %>% 
  #select(DATETIME, ID, binary, twi, Q_mm_day) %>% 
  mutate(twi = log(twi))
set.seed(123)
train_full <- ready2split %>% 
  slice_sample(prop = 0.8)

#create test dataset
test  <- anti_join(ready2split, train_full, by = c("DATETIME", "ID"))

train_values <- train_full %>% 
  select(binary, twi, stage_cm)

model <- glm(binary ~.,family=binomial(link='logit'),data=train_values)
summary(model)

john_reg <- function(input_logtwi, inputQ){
  #b0, or intercept from Kevin's email from Carrie, might need to redo regression
  b0 <- unname(model$coefficients[1])
  #all ofther coefs from Jensen et al. 2018
  twi_coef <- unname(model$coefficients[2])
  flow_coef <- unname(model$coefficients[3])

  b1x1 <- flow_coef * inputQ
  b2x2 <- twi_coef * input_logtwi

  #logistic regression from Jensen et al. 2018
  p <- exp(b0 + b1x1 + b2x2)/(1 + exp(b0 + b1x1 + b2x2))
  return(p)
}

tested <- ready2split %>% 
    mutate(percent_flowing = john_reg(log(twi), stage_cm) * 100,
           mins = minute(DATETIME)) %>% 
  filter(mins %in% c(0, 30))

#determine the number of sensor observations preserved in the test dataset at each timestep
tested %>% 
  group_by(DATETIME) %>% 
  summarise(count = length(unique(ID)))


add_thresholds <- function(df){
  df$per_90 <- 0
  df$per_90[df$percent_flowing >= 90] <- 1

  df$per_75 <- 0
  df$per_75[df$percent_flowing >= 75] <- 1

  df$per_50 <- 0
  df$per_50[df$percent_flowing >= 50] <- 1
  return(df)
}

#testing to make sure that compare function is doing what I want it to
add_thresholds(tested) %>% 
  group_by(DATETIME, stage_cm) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            count = length(unique(ID)))

out <-  add_thresholds(tested) %>% 
  group_by(ID) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement")

ggplot(out, aes(x = threshold, y = percent_agreement))+
  geom_boxplot()+
  ggtitle("Agreement between log regression and observations, FB")



hydro <-  add_thresholds(tested) %>% 
  group_by(DATETIME, stage_cm) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement")

pt1 <- hydro %>% 
filter(DATETIME > "2023-07-23 00:00:00" & DATETIME < "2023-9-20 00:00:00")
pt2 <- hydro %>% 
filter(DATETIME > "2023-09-23 00:00:00" & DATETIME < "2023-11-10 00:00:00")

rbind(pt1, pt2) %>% 
filter(DATETIME > "2023-10-15 00:00:00" & DATETIME < "2023-11-01 00:00:00") %>% 
ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm, color = percent_agreement), lwd = 1)+
  facet_wrap(~threshold, ncol = 1)+
  ylim(c(10, 11))+
  #theme_classic()+
  scale_color_gradientn(colors = rev(wes_palette("Zissou1", 100, type = "continuous")))


#does not work yet
points_w3 <- vect(out, geom=c("long", "lat"), crs = "+proj=longlat +datum=WGS84")
ggplot()+
  geom_spatvector(data = w3_stream_wgs, color = "grey", lwd = 1.5)+
  geom_spatvector(data = points_w3, aes(color = percent_agreement), size = 3)+
  facet_wrap(~threshold)+#aes(color = percent_flowing), size = 2)+
  #geom_text_repel(data = simple_table, aes(x = long, y = lat, label = number))+
  theme_void()+
  scale_color_binned(type = "viridis",
                      breaks = c(30, 60, 90)#,
                      #limits = c(0, 40)
                     )+
  labs(title = "Agreement with New model, 7/23 - 11/23")+
  theme(plot.margin=grid::unit(c(0,0,0,0), "mm"))
```

```{r}
#now do the same thing, but for zig zag
#plot locations of sensors
locs <- data_23 %>% 
  filter(wshed == "ZZ") %>% 
  select(ID, lat, long) %>% 
  unique()

locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")



dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
plot(m10)
lcc <- terra::project(locs_shape,crs(m10))

#save raster, because whitebox wants it is a files location instead of an object in R
writeRaster(m10, "./fb_dems/10mdem.tif", overwrite = TRUE)


breach_output <- "./fb_dems/10mdem_breach.tif"
wbt_breach_depressions_least_cost(
  dem = "./fb_dems/10mdem.tif",
  output = breach_output,
  dist = 10,
  fill = TRUE)

fill_output <- "./fb_dems/10mdem_fill.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

flowacc_output <- "./fb_dems/10mdem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")

slope_output <- "./fb_dems/10mdem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")

twi_output <- "./fb_dems/10mdem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

twi2 <- terra::rast(twi_output)
plot(twi2)

#just extract values from 10 m twi raster
twi_ex <- extract(twi2, lcc)

```

```{r}
#chunk that reads in stage, converts to proper units
#read in stage, convert to a height
ZZ_air <- read_csv("./PressureTransducers_11_14_23/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water <- read_csv("./PressureTransducers_11_14_23/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  ggplot(aes(x = DATETIME, y = stage_cm))+
  geom_line()+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")
#convert pressure to height of water
#h = p / (ρ * g)
#h = height in m
#density of water = 997 kg/m³
#gravitational acceleration = 9.8 m/s²
#convert psi to pascals, 1 psi = 6894.76 bars

library(patchwork)
fbAirPlot <- ggplot(ZZ_air, aes(x = DATETIME, y = pressure_psi_air))+
  geom_line()+
  theme_classic()+
  ggtitle("Falls Brook Air pressure")
fbWaterPlot <- ggplot(ZZ_water, aes(x = DATETIME, y = pressure_psi_water))+
  geom_line()+
  theme_classic()+
  ggtitle("Falls Brook Water pressure")


fbAirPlot / fbWaterPlot

data_23$mins <- minute(data_23$datetime)

#Fit a logistic regression to falls brook stage
#outputs df with stage in cm
ZZ_stage <- ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701)

ready2split <-  data_23 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  left_join(twi_ex, by = "ID") %>% 
  rename("DATETIME" = datetime,
         "twi" = '10mdem_twi') %>% 
  right_join(select(ZZ_stage, c(DATETIME, stage_cm)), by = "DATETIME") %>% 
  #select(DATETIME, ID, binary, twi, Q_mm_day) %>% 
  mutate(twi = log(twi))
set.seed(123)
train_full <- ready2split %>% 
  slice_sample(prop = 0.8)

#create test dataset
test  <- anti_join(ready2split, train_full, by = c("DATETIME", "ID"))

train_values <- train_full %>% 
  select(binary, twi, stage_cm)

model <- glm(binary ~.,family=binomial(link='logit'),data=train_values)
summary(model)

john_reg <- function(input_logtwi, inputQ){
  #b0, or intercept from Kevin's email from Carrie, might need to redo regression
  b0 <- unname(model$coefficients[1])
  #all ofther coefs from Jensen et al. 2018
  twi_coef <- unname(model$coefficients[2])
  flow_coef <- unname(model$coefficients[3])

  b1x1 <- flow_coef * inputQ
  b2x2 <- twi_coef * input_logtwi

  #logistic regression from Jensen et al. 2018
  p <- exp(b0 + b1x1 + b2x2)/(1 + exp(b0 + b1x1 + b2x2))
  return(p)
}

tested <- ready2split %>% 
    mutate(percent_flowing = john_reg(log(twi), stage_cm) * 100,
           mins = minute(DATETIME)) %>% 
  filter(mins %in% c(0, 30))

#determine the number of sensor observations preserved in the test dataset at each timestep
tested %>% 
  group_by(DATETIME) %>% 
  summarise(count = length(unique(ID)))


add_thresholds <- function(df){
  df$per_90 <- 0
  df$per_90[df$percent_flowing >= 90] <- 1

  df$per_75 <- 0
  df$per_75[df$percent_flowing >= 75] <- 1

  df$per_50 <- 0
  df$per_50[df$percent_flowing >= 50] <- 1
  return(df)
}

#testing to make sure that compare function is doing what I want it to
add_thresholds(tested) %>% 
  group_by(DATETIME, stage_cm) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            count = length(unique(ID)))

out <-  add_thresholds(tested) %>% 
  group_by(ID) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement")
out_zz

ggplot(out, aes(x = threshold, y = percent_agreement))+
  geom_boxplot()+
  ggtitle("Agreement between log regression and observations, FB")



hydro <-  add_thresholds(tested) %>% 
  group_by(DATETIME, stage_cm) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement")

pt1 <- hydro %>% 
filter(DATETIME > "2023-07-23 00:00:00" & DATETIME < "2023-9-20 00:00:00")
pt2 <- hydro %>% 
filter(DATETIME > "2023-09-23 00:00:00" & DATETIME < "2023-11-10 00:00:00")

rbind(pt1, pt2) %>% 
#filter(DATETIME > "2023-10-15 00:00:00" & DATETIME < "2023-11-01 00:00:00") %>% 
ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm, color = percent_agreement), lwd = 1)+
  facet_wrap(~threshold, ncol = 1)+
  #ylim(c(10, 11))+
  theme_classic()+
  scale_color_gradientn(colors = rev(wes_palette("Zissou1", 100, type = "continuous")))

```

```{r}
#what if I try other topographic metrics?
#topographic position index
wbt_relative_topographic_position(
  dem,
  output)

#also drainage area, flow accumulation
fb_flowacc <- "./fb_dems/1mdem_fb_flowacc.tif"
wbt_d8_flow_accumulation(input = fb_filled,
                         output = fb_flowacc)
plot(rast(fb_flowacc))
```

```{r}
#make a table with the mean agreement between the different topographic models. 


#make a table with the mean agreement between local persistency calculated based on a different number of locations and visits... test whether 1 observation of the whole network condition versus 10,000 works better. 
```

```{r}
#find the range of discharge values where a sensor goes from on to off, or the range of discharge values over which a sensor is flowing

data_23$mins <- minute(data_23$datetime)


pks <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()

#develop a way to determine a transition from one state to another
test <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  filter(ID == 1)

#iterate through the whole list, and if the binary are the same, get rid of the previous one
#from 2023-07-05 19:00:00 to 2023-07-06 05:30:00
start <- ymd_hms("2023-07-05 19:00:00")
stop <- ymd_hms("2023-07-06 05:30:00")
#instead of doing a for loop, make a lagged column, filter out everything that is not 0
test2 <- test %>% 
  filter(DATETIME > start & DATETIME < stop) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

test2$state_change <- "none"
test2$state_change[test2$transition == -1] <- "wetting"
test2$state_change[test2$transition == 1] <- "drying"

test2

#now apply to the whole dataset
test <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(ID == 1)%>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

# facet labels
hum_names <- as_labeller(
     c("avg_wettingQ" = "Mean",
            "med_wettingQ" = "Median",
            "min_wettingQ" = "Min",
            "max_wettingQ" = "Max"))

test %>% 
  group_by(ID) %>% 
  filter(state_change == "wetting") %>% 
  summarise(avg_wettingQ = mean(Q_mm_day),
            med_wettingQ = median(Q_mm_day),
            min_wettingQ = min(Q_mm_day),
            max_wettingQ = max(Q_mm_day)) %>% 
  left_join(pks, by = "ID") %>% 
  pivot_longer(cols = avg_wettingQ:max_wettingQ) %>% 
  ggplot(aes(x = pk, y = value))+
  geom_point()+
  #geom_smooth(method = "lm", se = FALSE)+
   # stat_poly_eq(label.x = 0.9, label.y = 0.9) +
  facet_wrap(~name, scale = "free_y", labeller = hum_names)+
  theme_classic()+
  labs(title = "Discharge required for stream wetting",
       x = "Local Persistency",
       y = "Discharge (mm/day)")
#calculate for each sensor the average discharge where wetting occurs

#difference in discharge as it dries, plotted against stream length and area
```


```{r}
#figure out how often the hierarchical framework is correct
#local persistency tells us what sequence sensors should turn on, how often is this right?
#what amount of discharge is associated with the sensors turning on each time, and how can I account for antecedent conditions?

#first filter to a single date and time, and see if what is flowing agrees with local persistency
time <- ymd_hms("2023-07-05 20:00:00")
#instead of doing a for loop, make a lagged column, filter out everything that is not 0
data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  filter(DATETIME == time) %>% 
    left_join(pks, by = "ID") %>% 
  arrange(pk) %>% 
  mutate(lagged = lag(binary),
         ID_lag = lag(ID),
         transition = (binary - lagged)) %>%
  filter(transition == 1) %>%
  print(n = 31)

#basically i need to keep the discharge and local persistency on the row where flow binary goes from 0 to 1
#add a lagged column
#labeller for plotting
hum_names2 <- as_labeller(
     c("mean_q" = "Mean",
            "med_q" = "Median"))
#works for a single time, now try for a few
start <- ymd_hms("2023-07-05 19:00:00")
stop <- ymd_hms("2023-08-05 20:30:00")
#instead of doing a for loop, make a lagged column, filter out everything that is not 0
data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(DATETIME > start & DATETIME < stop) %>%
    left_join(pks, by = "ID") %>% 
  arrange(pk) %>% 
  group_by(DATETIME) %>% 
  mutate(lagged = lag(binary),
         ID_lag = lag(ID),
         transition = (binary - lagged)) %>%
  filter(transition == 1) %>% 
  ungroup() %>% 
  group_by(ID_lag) %>% 
  summarise(mean_q = mean(Q_mm_day),
            med_q = median(Q_mm_day)) %>% 
  left_join(rename(pks, ID_lag = "ID"), by = "ID_lag") %>% 
      filter(ID_lag != 6) %>% #when not lagged in 26, actually should be 6, all of the values are shifted down one, meaning that at this discharge, every pk smaller is right at the threshold to flow
    pivot_longer(cols = c(mean_q, med_q)) %>% 
  ggplot(aes(x = pk, y = value))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)+
    stat_poly_eq(label.x = 0.9, label.y = 0.9) +
  facet_wrap(~name, scale = "free_y", labeller = hum_names2)+
  theme_classic()+
  labs(title = "threshold discharge based on persistency",
       x = "Local Persistency",
       y = "Discharge (mm/day)")

#this plot shows the discharge required for every sensor with higher flow persistency to turn on

#same data, but fit an exponential curve to it
fitting <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(DATETIME > start & DATETIME < stop) %>%
    left_join(pks, by = "ID") %>% 
  arrange(pk) %>% 
  group_by(DATETIME) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>%
  filter(transition == 1) %>% 
  ungroup() %>% 
  group_by(ID) %>% 
  summarise(mean_q = mean(Q_mm_day),
            med_q = median(Q_mm_day)) %>% 
  left_join(pks, by = "ID") %>% 
      filter(ID != 26)

model <- nls(mean_q ~ a*exp(-r * pk), data = fitting, start = list(a = 0.5, r = 0.2))

fitting %>% 
    add_predictions(model) %>% 
  ggplot(aes(x = pk, y = log(mean_q)))+
  geom_point()+
  geom_line(aes(y = pred), color = "blue")+
   # stat_poly_eq(label.x = 0.9, label.y = 0.9) +
  theme_classic()+
  labs(title = "threshold discharge based on persistency",
       x = "Local Persistency",
       y = "Mean Discharge (mm/day)")


#now calculate when and where the hierarchical model is wrong
#start with plot that is proportion of time when hierarchical model is wrong for each sensor
data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(DATETIME > start & DATETIME < stop) %>%
    left_join(pks, by = "ID") %>% 
  arrange(pk) %>% 
  group_by(DATETIME, Q_mm_day) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>%
  filter(transition == 1) %>% 
      filter(ID != 26) %>% 
  ungroup() %>% 
  group_by(ID) %>% 
  summarise(count = sum(transition)) %>% 
  ggplot(aes(x = as.integer(ID), y = count/6007, group = as.integer(ID)))+
  geom_bar(stat="identity")+
  labs(title = "Proportion of time that hierarchical framework fails",
       x = "Sensor ID",
       y = "Proportion of time")+
  theme_classic()

#figuring out when and where botter and durighetto doesn't work is not going to be easy... 
# I think that what will matter is antecedent conditions

#I could make a column of lagged of lagged discharge

gggg <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  filter(ID == 1)

length(gggg$ID)
```

```{r}
#calculate flow duration curve, and what range of that this dataset captures
```

```{r}
#determine the range of topography my study watersheds represent
```

```{r}
#calculate expansion exponent after prancevic and kirchner 2019
# L = wetted length
# Qo = discharge at the outlet
# B = network scaling exponent
# L = Qo^B
#then implement JP idea: plot that is diagnostic of whether
# y, tranmissivity scaling exponent for W3 from P&K19 = 1.28
# A = local contributing area
# T = A^-y

# all I need to calculate is the local drainage area, and discharge when the sensors go dry. Actual and theoretical.

## Discharge when the sensors go dry (Actual)
hum_names <- as_labeller(
     c("avg_dryingQ" = "Mean",
            "med_dryingQ" = "Median",
            "min_dryingQ" = "Min",
            "max_dryingQ" = "Max"))

maxe <- 10
positions <- data.frame(x = c(0,maxe,maxe, 0,0,maxe ),
                        y = c(0,maxe,0, 0,maxe,maxe ),
                        fill = c(rep("Yes",3), rep("No", 3)))
test %>% 
  group_by(ID) %>% 
  filter(state_change == "drying") %>% 
  summarise(avg_dryingQ = mean(Q_mm_day),
            med_dryingQ = median(Q_mm_day),
            min_dryingQ = min(Q_mm_day),
            max_dryingQ = max(Q_mm_day)) %>% 
  left_join(pks, by = "ID") %>% 
  pivot_longer(cols = avg_dryingQ:max_dryingQ) %>% 
  left_join(W3_uaa_ex, by = "ID") %>% 
  rename(uaa = `10mdem_flowacc`) %>% 
  mutate(hypothetical_T = uaa^-1.28) %>% # T = A^-y, used y from P&K
  filter(ID != 13) %>% #remove 13, something wrong with drainage area, way too small
  filter(name == "avg_dryingQ") %>% 
  ggplot(aes(x = hypothetical_T, y = value))+
  geom_text(aes(label = ID))+
  geom_abline(intercept = 0, slope = 1, color="black", 
                 linetype="dashed")+
  geom_polygon(data = positions, aes(x = x,
                   y = y,
                   fill = fill), alpha = 0.2)+
  scale_fill_manual(values = c( "salmon", "midnightblue"), name = "")+
  #geom_smooth(method = "lm", se = FALSE)+
  #  stat_poly_eq(label.x = 0.9, label.y = 0.9) +
  #facet_wrap(~name, scale = "free_y", labeller = hum_names)+
  theme_classic()+
  lims(x = c(0, maxe),
       y = c(0, maxe))+
  labs(title = "Does Topography Predict Transmissivity?",
       x = "Theoretical Transmissivity",
       y = "Actual Tranmissivity (Median Q when Si dries)")+
    theme(legend.position="top")

  

## Local drainage area- UAA for each logger location
W3_uaa_ex
# Si = some location in the watershed
#plot with calculated tranmissivity on the X, and Q where Si should go dry

#I also need an estimate of Q at each location... is there some leap in logic that could help me get here from local persistency?

#or could I build a relationship between drainage area, discharge, and which locations are active...

#determine how much discharge each sensor contributes?

# Find each time that a sensor wets, and determine what the difference in discharge is between before and after wetting. Rough estimate of how much discharge is being contributed at each location?
```

11/14/24- After illuminating meeting with JP and Kevin, I have a second iteration of the JPG and JCM graph:
```{r}
data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(DATETIME > start & DATETIME < stop) %>%
    left_join(pks, by = "ID") %>% 
  arrange(pk) %>% 
  group_by(DATETIME) %>% 
  mutate(lagged = lag(binary),
         ID_lag = lag(ID),
         transition = (binary - lagged)) %>%
  filter(transition == 1) %>% 
  ungroup() %>% 
  group_by(ID_lag) %>% 
  summarise(mean_q = mean(Q_mm_day),
            med_q = median(Q_mm_day)) %>% 
  left_join(rename(pks, ID_lag = "ID"), by = "ID_lag") %>% 
      filter(ID_lag != 6) %>% #when not lagged in 26, actually should be 6, all of the values are shifted down one, meaning that at this discharge, every pk smaller is right at the threshold to flow
    pivot_longer(cols = c(mean_q, med_q)) %>% 
  ggplot(aes(x = pk, y = value))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)+
    stat_poly_eq(label.x = 0.9, label.y = 0.9) +
  facet_wrap(~name, scale = "free_y", labeller = hum_names2)+
  theme_classic()+
  labs(title = "threshold discharge based on persistency",
       x = "Local Persistency",
       y = "Discharge (mm/day)")
```

Analysis idea from Nov 18, 2024. 
To test the assumption that Tranmissivity decreases as you move downvalley, test whether the change in discharge from one time step to the next when a sensor dries is related to it's position in the network. This might show where this tranmissivity hypothesis fails due to some condition of the subsurface.
```{r}

test <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(ID == 1)%>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

# facet labels
hum_names <- as_labeller(
     c("avg_wettingQ" = "Mean",
            "med_wettingQ" = "Median",
            "min_wettingQ" = "Min",
            "max_wettingQ" = "Max"))
number_of_dries <- test %>% 
  group_by(ID) %>% 
  filter(state_change == "drying")

#calculating the average and sd of delta Q for each sensor when they go dry
data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(ID == 1)%>% 
  mutate(lagged = lag(binary),
         lagged_datetime = lag(DATETIME),
         lagged_Q = lag(Q_mm_day),
         transition = abs(binary - lagged)) %>% 
  filter(transition == 1) %>% 
  mutate(drying_diff = (lagged_Q - Q_mm_day)) %>% 
  select(DATETIME, ID, drying_diff) %>% 
  group_by(ID) %>% 
  summarise(mean = mean(drying_diff),
            sd = sd(drying_diff),
            med = median(drying_diff)) %>% #left join to drainage area
  left_join(W3_uaa_ex, by = "ID") %>% 
  ggplot(aes(x = log(`10mdem_flowacc`), y = mean))+
  geom_point()+
  theme_classic()+
  labs(title = "Mean ΔQ required for stream drying",
       x = "ln(Drainage Area) (m^2)",
       y = "Mean |ΔQ| (mm/day)")#+
  #geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2)

#calculating the average and sd of delta Q for each sensor when they go dry
data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(ID == 1)%>% 
  mutate(lagged = lag(binary),
         lagged_datetime = lag(DATETIME),
         lagged_Q = lag(Q_mm_day),
         transition = abs(binary - lagged)) %>% 
  filter(transition == 1) %>% 
  mutate(drying_diff = (lagged_Q - Q_mm_day)) %>% 
  select(DATETIME, ID, drying_diff) %>% 
  group_by(ID) %>% 
  summarise(mean = mean(drying_diff),
            sd = sd(drying_diff),
            med = median(drying_diff)) %>% #left join to drainage area
  left_join(W3_uaa_ex, by = "ID") %>% 
  ggplot(aes(x = log(`10mdem_flowacc`), y = mean))+
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2, color = "lightgrey")+
  geom_point()+
  theme_classic()+
  labs(title = "ΔQ required for stream drying",
       x = "ln(Drainage Area) (m^2)",
       y = "Mean |ΔQ| (mm/day)")


#calculating the average and sd of delta Q for each sensor when they go dry
data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(ID == 1)%>% 
  mutate(lagged = lag(binary),
         lagged_datetime = lag(DATETIME),
         lagged_Q = lag(Q_mm_day),
         transition = (binary - lagged)) %>% 
  filter(transition == 1) %>% 
  mutate(drying_diff = (lagged_Q - Q_mm_day)) %>% 
  select(DATETIME, ID, drying_diff) %>% 
  group_by(ID) %>% 
  summarise(mean = mean(drying_diff),
            sd = sd(drying_diff),
            med = median(drying_diff)) %>% #left join to drainage area
  left_join(W3_uaa_ex, by = "ID") %>% 
  ggplot(aes(x = log(`10mdem_flowacc`), y = mean))+
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2, color = "lightgrey")+
  geom_point()+
  theme_classic()+
  geom_hline(yintercept = 0, lty = "dashed")+
  labs(title = "ΔQ required for stream drying",
       x = "ln(Drainage Area) (m^2)",
       y = "Mean ΔQ (mm/day)")

#calculating the average and sd of delta Q for each sensor when they go dry
data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(ID == 1)%>% 
  mutate(lagged = lag(binary),
         lagged_datetime = lag(DATETIME),
         lagged_Q = lag(Q_mm_day),
         transition = (binary - lagged)) %>% 
  filter(transition == 1) %>% 
  mutate(drying_diff = (lagged_Q - Q_mm_day)) %>% 
  select(DATETIME, ID, drying_diff) %>% 
  group_by(ID) %>% 
  summarise(mean = mean(drying_diff),
            sd = sd(drying_diff),
            med = median(drying_diff)) %>% #left join to drainage area
  left_join(W3_uaa_ex, by = "ID") %>% 
  ggplot(aes(x = (`10mdem_flowacc`), y = mean))+
  geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2, color = "lightgrey")+
  geom_point()+
  theme_classic()+
  geom_hline(yintercept = 0, lty = "dashed")+
  labs(title = "ΔQ required for stream drying",
       x = "Drainage Area (m^2)",
       y = "Mean ΔQ (mm/day)")

data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #filter(ID == 1)%>% 
  mutate(lagged = lag(binary),
         lagged_datetime = lag(DATETIME),
         lagged_Q = lag(Q_mm_day),
         transition = abs(binary - lagged)) %>% 
  filter(transition == 1) %>% 
  mutate(drying_diff = (lagged_Q - Q_mm_day)) %>% 
  select(DATETIME, ID, drying_diff) %>% 
  group_by(ID) %>% 
  summarise(mean = mean(drying_diff),
            sd = sd(drying_diff),
            med = median(drying_diff)) %>% #left join to drainage area
  left_join(W3_uaa_ex, by = "ID") %>% 
  ggplot(aes(x = log(`10mdem_flowacc`), y = med))+
  geom_point()+
  theme_classic()+
  labs(title = "Median ΔQ required for stream drying",
       x = "ln(Drainage Area) (m^2)",
       y = "Median |ΔQ| (mm/day)")#+
```

Calculate how dynamic a location is: how often does it switch states? How many times does it switch states in a certain time frame?