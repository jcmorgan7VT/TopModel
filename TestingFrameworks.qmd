---
title: "Test Frameworks"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

9/8/24 Test different theoretical frameworks using my data.

```{r reading-in-STIC-obs}
#loading packages
library(pacman)
p_load(tidyverse, terra, tidyterra, whitebox, scales, wesanderson, caret, plotly,
       ggnewscale, sf, rgeoboundaries, elevatr, patchwork)

#reading in final format data for summer 23
data_23 <- read_csv("./DataForMary/HB_stic.csv")
#reading in final format data for summer 24
data_24 <- read_csv("./summer2024/STICS2024.csv")
```

9/10/24 Make figures showing the full period of record

```{r full-record-w3-plot}
binary <- c("#DB995A",
            "#586BA4"
  )

data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

head(filter(data_23, wshed == "W3", deployment == 1))
ggplot()+
  geom_tile(data = filter(data_23, wshed == "W3", mins %in% c(0, 30)),
            aes(x = datetime, y = ID, fill = wetdry))+
  geom_tile(data = filter(data_24, wshed == "W3", mins %in% c(0, 30)),
            aes(x = datetime, y = number, fill = wetdry))+
  #facet_grid(~deployment, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c("dry", "wet"),
                    labels = c("No flow", "flowing"),
                    name = ""
                    )+
  labs(title = "Streamflow permanence in W3",
       x = "")+
  theme_classic()
```

FB

```{r full-record-FB-plot}

head(filter(data_24, wshed == "FB", sensor == 21736589))

ggplot()+
  geom_tile(data = filter(data_23, wshed == "FB", mins %in% c(0, 30)),
            aes(x = datetime, y = ID, fill = wetdry))+
  geom_tile(data = filter(data_24, wshed == "FB", mins %in% c(0, 30)),
            aes(x = datetime, y = number, fill = wetdry))+
  #facet_grid(~deployment, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c("dry", "wet"),
                    labels = c("No flow", "flowing"),
                    name = ""
                    )+
  labs(title = "Streamflow permanence in FB",
       x = "")+
  theme_classic()
```

ZZ

```{r full-record-zz-plot}
ggplot()+
  geom_tile(data = filter(data_23, wshed == "ZZ", mins %in% c(0, 30)),
            aes(x = datetime, y = ID, fill = wetdry))+
  geom_tile(data = filter(data_24, wshed == "ZZ", mins %in% c(0, 30)),
            aes(x = datetime, y = number, fill = wetdry))+
  #facet_grid(~deployment, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c("dry", "wet"),
                    labels = c("No flow", "flowing"),
                    name = ""
                    )+
  labs(title = "Streamflow permanence in ZZ",
       x = "")+
  theme_classic()
```

Start with Carrie's model- how well does it capture the dynamics that I observed?

```{r}
#read in discharge from W3-- input to Carrie's model, discharge in L/s
#q <- read_csv("https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-hbr.1.17&entityid=efc477b3ef1bb3dd8b9355c9115cd849")
#write.csv(q, "HB_5minQ.csv")
q <- read_csv("HB_5minQ.csv")
#input discharge needs to be in mm/day?
#reference to understand difference between daily mean and instantaneous streamflow:
#https://hydrofunctions.readthedocs.io/en/master/notebooks/DailyMean_vs_Instant.html

#creating minute column, used to filter out higher temporal resolution measurements for plotting
data_23$mins <- minute(data_23$datetime)
data_24$mins <- minute(data_24$datetime)

#find the range of dates that I need discharge for
start <- min(data_23$datetime)
stop <- max(data_23$datetime)

#filtering discharge down to the range of dates
q_23 <- q %>% 
  filter(DATETIME > start & DATETIME < stop) %>% 
  #convert to mm/day.
  #converting instantaneous streamflow to mm/day by taking measurement, and scaling   it up as if that was the discharge for the whole day. It is not, it is just at that   moment, but should fix any units/order of magnitude issues
  mutate("Q_mm_day" = Discharge_ls * 0.001 * 86400 / 420000 * 1000) 
q_23$mins <- minute(q_23$DATETIME)

#removing times that are not coincident with STIC observations
q_23_f <- filter(q_23, mins %in% c(0, 30))

ggplot(q_23_f, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

```

```{r}
#calculate then extract the TWI at each of the sensor locations. May be tricky to make sure that they line up with flowlines, or places with greatest drainage area.
twi3 <- "./w3_dems/w3_dem_twi.tif"
#twi calculated in script topmodel_fromscratch_2_23_23.Rmd
twi3 <- rast(twi3)
plot(twi3)

#plot locations of sensors
locs <- data_23 %>% 
  filter(wshed == "W3") %>% 
  select(ID, lat, long) %>% 
  unique()

locs_shape <- vect(locs, 
                   geom=c("long", "lat"), 
                   crs = "+proj=longlat +datum=WGS84")
lcc <- terra::project(locs_shape,crs(twi3))

#extract twi values using terra::extract
twi_ex <- extract(twi3, lcc)

#these point locations are not manually corrected.
#in next chunk, calculate the percent chance that each spot is flowing based off of an input discharge
```

```{r}
#chunk that is actually running the model

#function to calculate probability of flow raster
willit <- function(input_logtwi, inputQ){
  #b0, or intercept from Kevin's email from Carrie, might need to redo regression
  b0 <- -35.43
  #all ofther coefs from Jensen et al. 2018
  twi_coef <- 15.57
  flow_coef <- 0.12

  b1x1 <- flow_coef * inputQ
  b2x2 <- twi_coef * input_logtwi

  #logistic regression from Jensen et al. 2018
  p <- exp(b0 + b1x1 + b2x2)/(1 + exp(b0 + b1x1 + b2x2))
  return(p)
}

test <- willit(log(twi3), 1)
plot(test)
#after running, threshold by some probability of flowing
#loop through the input dataframe of discharge, once for each sensor. make an indexed array perhaps?

test <- willit(twi_ex$w3_dem_twi, q_23_f$Q_mm_day)

#unmodified df to store output of for loop
mod <- q_23_f
#loop through discharge record for each STIC location
for(i in 1:length(twi_ex$ID)){
  mod$ID <- twi_ex$ID[i]
  mod$percent_flowing <- willit(log(twi_ex$w3_dem_twi[i]), mod$Q_mm_day)
  
  
  if(i == 1) final <- mod
  if(i > 1) final <- rbind(final, mod)

}

#thresholding resulting percent chances to get a binary of flowing/not flowing
#using three thresholds reported in Carrie's paper
final$per_90 <- 0
final$per_90[final$percent_flowing >= 90] <- 1

final$per_75 <- 0
final$per_75[final$percent_flowing >= 75] <- 1

final$per_50 <- 0
final$per_50[final$percent_flowing >= 50] <- 1
#then compare prediction versus actual
w3_23 <- filter(data_23, wshed == "W3") %>% 
  rename("DATETIME" = datetime)
#make a binary wet versus dry column
w3_23$binary <- 1
w3_23$binary[w3_23$wetdry == "dry"] <- 0

#join the two tables
forp <- left_join(w3_23, final, by = c("DATETIME", "ID"))
```

```{r}
#plot to compare observed versus predicted at all sensors
#hard to do for binary data, instead maybe calculate the percentage of time that it is correct?
#compare binary column to 3 different percent chance of flow columns
length(which(forp$binary == forp$per_90))/length(forp$DATETIME)
IDs <- unique(forp$ID)

for(i in 1:length(IDs)){
  inter <- filter(forp, ID == IDs[i])
  result <- length(which(inter$binary == inter$per_90))/length(inter$DATETIME)
  mod <- data.frame("ID" = IDs[i],
                    "percent_agree" = result)
  
  if(i == 1) final <- mod
  if(i > 1) final <- rbind(final, mod)

}

#plot through space
#Make a simple map for each watershed to serve as the template 
sheds <- vect('./HB/hbef_wsheds')
plot(sheds)
#method to subset shapefiles- use base r ways to subset
w3 <- sheds[2,]
plot(w3)

#read in shapefiles for each of the streams
w3_stream <- vect("./HB/hbstream/hb42_master_startend.shp")
w3_stream_wgs <- project(w3_stream, "+proj=longlat +datum=WGS84")
data_w3 <- unique(w3_23 %>% 
  dplyr::select(ID, lat, long)) %>% 
  left_join(final, by = "ID")

points_w3 <- vect(data_w3, geom=c("long", "lat"), crs = "+proj=longlat +datum=WGS84")

#create simple table that contains labels for points
# simple_table_w3 <- fb_n %>% 
#   select(number, sensor, lat, long) %>% 
#   unique() %>% 
#   mutate("status" = "Good (23)")
# simple_table_fb <- simple_table_fb[order(simple_table_fb$number),]


ggplot()+
  geom_spatvector(data = w3_stream_wgs, color = "grey", lwd = 1.5)+
  geom_spatvector(data = points_w3, aes(color = percent_agree), size = 3)+#aes(color = percent_flowing), size = 2)+
  #geom_text_repel(data = simple_table, aes(x = long, y = lat, label = number))+
  theme_void()+
  labs(title = "Agreement with Carrie's model")+
  theme(plot.margin=grid::unit(c(0,0,0,0), "mm"))



```

```{r}
#run Carrie's model again on a courser twi dataset
#instead of using 10m dem on HB database, use coarsened 1m dem
#aggregate function, by factor of 10- 1 m resolution to 10 m res.
#tried just aggregating twi raster, but it looks funny
twi3_c <- aggregate(twi3, 10)
#should aggregate dem, then do all of the transformations to it
#read in 1 m dem
dem <- "./HB/1m hydro enforced DEM/dem1m.tif"
m1 <- rast(dem)
m10 <- aggregate(m1, 10)
plot(m10, xlim = c(281400, 282000),
     ylim = c(4870600, 4871200))
#first crop and mask to W3 shape
sheds <- vect('./HB/hbef_wsheds')
w3 <- sheds[2,]
dem_crop <- terra::crop(m10, w3)
dem_mask <- terra::mask(dem_crop, w3)
plot(dem_mask)
#save raster, because whitebox wants it is a files location instead of an object in R
#writeRaster(dem_mask, "./w3_dems/hbef_10mdem/10mdem_crop.tif")

cropped <- "./w3_dems/hbef_10mdem/10mdem_crop.tif"
twi <- terra::rast(cropped)
twi_crop <- terra::crop(twi, w3)
plot(twi_crop)

breach_output <- "./w3_dems/hbef_10mdem/10m_dem_breached.tif"
wbt_breach_depressions_least_cost(
  dem = cropped,
  output = breach_output,
  dist = 5,
  fill = TRUE)

fill_output <- "./w3_dems/hbef_10mdem/10m_dem_filled.tif"
wbt_fill_depressions_wang_and_liu(
  dem = breach_output,
  output = fill_output
)

flowacc_output <- "./w3_dems/hbef_10mdem/10m_dem_flowacc.tif"
wbt_d_inf_flow_accumulation(input = fill_output,
                            output = flowacc_output,
                            out_type = "Specific Contributing Area")

slope_output <- "./w3_dems/hbef_10mdem/10m_dem_slope.tif"
wbt_slope(dem = fill_output,
          output = slope_output,
          units = "degrees")

twi_output <- "./w3_dems/hbef_10mdem/10m_dem_twi.tif"
wbt_wetness_index(sca = flowacc_output, #flow accumulation
                  slope = slope_output,
                  output = twi_output)

twi <- terra::rast(twi_output)
twi_crop <- terra::crop(twi, w3)
plot(twi_crop)
# 9/16/24
#There is something wrong with the 10m dem that I have. Will try to download it from data repo again. 
#Reached out to Kevin and JP, the 10m dem on the repo is bad. instead use corsened 1 m lidar
```

```{r}
#re-run Carrie's model on the coarser DEM, and compare to my observations
#extract values of twi at sensor locations
```

```{r}
# 9/25/24- rewrote code to get out of for loop world, vectorize all the operations

#writing function form of Carrie's model
#willit function already defined
willit <- function(input_logtwi, inputQ){
  #b0, or intercept from Kevin's email from Carrie, might need to redo regression
  b0 <- -35.43
  #all ofther coefs from Jensen et al. 2018
  twi_coef <- 15.57
  flow_coef <- 0.12

  b1x1 <- flow_coef * inputQ
  b2x2 <- twi_coef * input_logtwi

  #logistic regression from Jensen et al. 2018
  p <- exp(b0 + b1x1 + b2x2)/(1 + exp(b0 + b1x1 + b2x2))
  return(p)
}

#before passing to function, join Q, TWI, and observations into one big df
#then apply function to appropriate columns
data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0

everything_bagel <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  left_join(twi_ex2, by = "ID") %>% 
  rename("DATETIME" = datetime,
         "twi" = '10m_dem_twi') %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  mutate(percent_flowing = willit(log(twi), Q_mm_day) * 100) 
  #threshold resulting percent chances to get a binary of flowing/not flowing
  #using three thresholds reported in Carrie's paper
  everything_bagel$per_90 <- 0
  everything_bagel$per_90[everything_bagel$percent_flowing >= 90] <- 1

  everything_bagel$per_75 <- 0
  everything_bagel$per_75[everything_bagel$percent_flowing >= 75] <- 1

  everything_bagel$per_50 <- 0
  everything_bagel$per_50[everything_bagel$percent_flowing >= 50] <- 1
  #then compare prediction versus actual

#write a function to use in summarise
compare <- function(x, y, time){
  length(which(x == y))/length(time)*100
  }
  
tiny_bagel <- everything_bagel %>% 
  group_by(ID, lat, long) %>% 
  summarise(threshold_90per = compare(binary, per_90, DATETIME),
            threshold_75per = compare(binary, per_75, DATETIME),
            threshold_50per = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement")
  tiny_bagel$category <- "high >=90%"
  tiny_bagel$category <- "sometimes "

  everything_bagel$per_50[everything_bagel$percent_flowing >= 50] <- 1

ggplot(tiny_bagel, aes(x = threshold, y = percent_agreement))+
  geom_boxplot()

points_w3 <- vect(tiny_bagel, geom=c("long", "lat"), crs = "+proj=longlat +datum=WGS84")
ggplot()+
  geom_spatvector(data = w3_stream_wgs, color = "grey", lwd = 1.5)+
  geom_spatvector(data = points_w3, aes(color = percent_agreement), size = 3)+
  facet_wrap(~threshold)+#aes(color = percent_flowing), size = 2)+
  #geom_text_repel(data = simple_table, aes(x = long, y = lat, label = number))+
  theme_void()+
  scale_color_binned(type = "viridis",
                      breaks = c(30, 60, 90)#,
                      #limits = c(0, 40)
                     )+
  labs(title = "Agreement with Carrie's model, 7/23 - 11/23")+
  theme(plot.margin=grid::unit(c(0,0,0,0), "mm"))

```

```{r}
#coarse DEM
twi_ex2 <- extract(twi_crop, lcc)
plot(twi_crop)
#normal one
twi_ex <- extract(twi3, lcc)
plot(twi3)
#finally able to compare the coarsened to non-coarsened results
everything_bagel <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  left_join(twi_ex, by = "ID") %>% 
  rename("DATETIME" = datetime,
         "twi" = w3_dem_twi) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  mutate(percent_flowing = willit(log(twi), Q_mm_day) * 100) 
  #threshold resulting percent chances to get a binary of flowing/not flowing
  #using three thresholds reported in Carrie's paper
  everything_bagel$per_90 <- 0
  everything_bagel$per_90[everything_bagel$percent_flowing >= 90] <- 1

  everything_bagel$per_75 <- 0
  everything_bagel$per_75[everything_bagel$percent_flowing >= 75] <- 1

  everything_bagel$per_50 <- 0
  everything_bagel$per_50[everything_bagel$percent_flowing >= 50] <- 1
  #then compare prediction versus actual

#write a function to use in summarise
compare <- function(x, y, time){
  length(which(x == y))/length(time)
  }
  
tiny_bagel <- everything_bagel %>% 
  group_by(ID, lat, long) %>% 
  summarise(agree_90 = compare(binary, per_90, DATETIME),
            agree_75 = compare(binary, per_75, DATETIME),
            agree_50 = compare(binary, per_50, DATETIME))
#results between 1m resolution and 10 m resolution are very different, as I thought they should be, and the 
```

```{r}
#Carrie's model, when averaged across every site and every time is a coinflip
#fit my own regression to the dataset
ready2split <-  data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  left_join(twi_ex, by = "ID") %>% 
  rename("DATETIME" = datetime,
         "twi" = w3_dem_twi) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  #select(DATETIME, ID, binary, twi, Q_mm_day) %>% 
  mutate(twi = log(twi))
set.seed(123)
train_full <- ready2split %>% 
  slice_sample(prop = 0.8)

#create test dataset
test  <- anti_join(ready2split, train_full, by = c("DATETIME", "ID"))

train_values <- train %>% 
  select(binary, twi, Q_mm_day)

model <- glm(binary ~.,family=binomial(link='logit'),data=train)
summary(model)

```

```{r}
john_reg <- function(input_logtwi, inputQ){
  #b0, or intercept from Kevin's email from Carrie, might need to redo regression
  b0 <- -1.160316
  #all ofther coefs from Jensen et al. 2018
  twi_coef <- 0.089141
  flow_coef <- 0.619352

  b1x1 <- flow_coef * inputQ
  b2x2 <- twi_coef * input_logtwi

  #logistic regression from Jensen et al. 2018
  p <- exp(b0 + b1x1 + b2x2)/(1 + exp(b0 + b1x1 + b2x2))
  return(p)
}

tested <- test %>% 
    mutate(percent_flowing = john_reg(log(twi), Q_mm_day) * 100) 
#determine the number of sensor observations preserved in the test dataset at each timestep
tested %>% 
  group_by(DATETIME) %>% 
  summarise(count = length(unique(ID)))


add_thresholds <- function(df){
  df$per_90 <- 0
  df$per_90[df$percent_flowing >= 90] <- 1

  df$per_75 <- 0
  df$per_75[df$percent_flowing >= 75] <- 1

  df$per_50 <- 0
  df$per_50[df$percent_flowing >= 50] <- 1
  return(df)
}

#testing to make sure that compare function is doing what I want it to
add_thresholds(tested) %>% 
  group_by(DATETIME, Q_mm_day) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            count = length(unique(ID)))

out <-  add_thresholds(tested) %>% 
  group_by(ID) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement")

ggplot(out, aes(x = threshold, y = percent_agreement))+
  geom_boxplot()

points_w3 <- vect(out, geom=c("long", "lat"), crs = "+proj=longlat +datum=WGS84")
ggplot()+
  geom_spatvector(data = w3_stream_wgs, color = "grey", lwd = 1.5)+
  geom_spatvector(data = points_w3, aes(color = percent_agreement), size = 3)+
  facet_wrap(~threshold)+#aes(color = percent_flowing), size = 2)+
  #geom_text_repel(data = simple_table, aes(x = long, y = lat, label = number))+
  theme_void()+
  scale_color_binned(type = "viridis",
                      breaks = c(30, 60, 90)#,
                      #limits = c(0, 40)
                     )+
  labs(title = "Agreement with New model, 7/23 - 11/23")+
  theme(plot.margin=grid::unit(c(0,0,0,0), "mm"))
```

```{r}
#plot to see under what conditions Carrie and my model work well
#hydrograph, but colored by the percent agreement
hydro <-  add_thresholds(tested) %>% 
  group_by(DATETIME, Q_mm_day) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement")

hydro %>% 
filter(DATETIME > "2023-09-15 16:30:00" & DATETIME < "2023-09-18 16:30:00") %>% 
ggplot()+
  geom_line(aes(x = DATETIME, y = Q_mm_day, color = percent_agreement), lwd = 1.5)+
  facet_wrap(~threshold, ncol = 1)+
  theme_classic()+
  scale_colour_viridis_c()

```

```{r}
#final version of discharge through time colored by model fidelity, using wes anderson color palette
start <- "2023-08-07 00:00:00"
stop <- "2023-08-14 00:00:00"

ready2split %>% 
    mutate(percent_flowing = john_reg(log(twi), Q_mm_day) * 100) %>% 
  add_thresholds() %>% 
  group_by(DATETIME, Q_mm_day) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement") %>% 
  filter(threshold == "threshold_75") %>% 
#filter(DATETIME > start & DATETIME < stop) %>% 
ggplot()+
  geom_line(aes(x = DATETIME, y = Q_mm_day, color = percent_agreement), lwd = 1)+
  #facet_wrap(~threshold, ncol = 1)+
  theme_classic()+
  scale_color_gradientn(colors = rev(wes_palette("Zissou1", 100, type = "continuous")))+
  labs(title = "W3 75% threshold, log(TWI)",
       x = "",
       y = "Discharge (mm/day)")

```

```{r}
#now plot whether model is right or wrong at each location

#dataframe that shows whether the model was right or wrong 
devil_on_shoulder <- ready2split %>%
  mutate(percent_flowing = john_reg(log(twi), Q_mm_day) * 100) %>%
  add_thresholds() %>%
  #filter(DATETIME > start & DATETIME < stop) %>%
  mutate(
    agree_or_no_90 = binary - per_90,
    agree_or_no_75 = binary - per_75,
    agree_or_no_50 = binary - per_50) %>%
  pivot_longer(cols = starts_with("agree_"),
               names_to = "threshold",
               values_to = "percent_agreement")
View(devil_on_shoulder)

devil_on_shoulder$meaning <- "correct"
devil_on_shoulder$meaning[devil_on_shoulder$percent_agreement == -1] <- "commission"
devil_on_shoulder$meaning[devil_on_shoulder$percent_agreement == 1] <- "ommission"

start <- "2023-08-07 00:00:00 EST"
stop <- "2023-08-07 02:00:00 EST"

devil_on_shoulder %>% 
  filter(DATETIME > start & DATETIME < stop) %>% 
  filter(threshold == "agree_or_no_90") %>% 
  vect(geom=c("long", "lat"), crs = "+proj=longlat +datum=WGS84") %>% 
ggplot()+  
  geom_spatvector(data = w3_stream_wgs, color = "grey", lwd = 1.5)+
  geom_spatvector(aes(color = meaning), size = 3)+
  facet_wrap(~DATETIME)+#aes(color = percent_flowing), size = 2)+
  #geom_text_repel(data = simple_table, aes(x = long, y = lat, label = number))+
  theme_void()+
  labs(title = "Agreement with New model, 7/23 - 11/23")+
  theme(plot.margin=grid::unit(c(0,0,0,0), "mm"))
#instead of panels for each date and time, just do a grid


devil_on_shoulder %>% 
  #filter(DATETIME > start & DATETIME < stop) %>% 
  #filter(threshold == "agree_or_no_90") %>% 
  ggplot()+
  geom_tile(aes(x = DATETIME, y = ID, fill = meaning))+
  facet_grid(~threshold, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                    values = c("#586BA4","black", "#DB995A"),
                    breaks = c("commission", "correct", "ommission"),
                    name = ""
                    )+
  labs(title = "Streamflow permanence in W3",
       x = "")+
  theme_classic()+
    theme(legend.position="bottom")


```

```{r}
#Creates vectors having data points
expected_value <- factor(c(1,0,1,0,1,1,1,0,0,1))
predicted_value <- factor(c(1,0,0,1,1,1,0,0,0,1))

#Creating confusion matrix
example <- confusionMatrix(data=predicted_value, reference = expected_value)

#Display results 
example

#make a confusion matrix for the whole sample
conf_all <- add_thresholds(tested) #%>% 
o <- confusionMatrix(data = factor(conf_all$per_50), reference = factor(conf_all$binary))
unname(o$overall[1])
#calculate the accuracy for each threshold at each point in time
#write mini function to spit out just the accuracy
#trying to make a dataset ready to pass through a mutate with function that returns confusion matrix accuracy
#not vectorized solution, but it works
mega_thresholds <- function(df){
  for(i in 1:100){
  df$threshold <- i
  df$flowing <- 0
  df$flowing[df$percent_flowing >= df$threshold] <- 1
  
  
  if(i == 1) final <- df
  if(i > 1) final <- rbind(final, df)
  }
  return(final)
}
conf_all <- mega_thresholds(tested)

#accuracy field of confusion matrix is the same as result of compare function I wrote
#write function to produce accuracy
accuracy_calc <- function(model, reference){
  mat <- confusionMatrix(factor(model, levels = c(0,1)), factor(reference, levels = c(0,1)))
  acc <- unname(mat$overall[1])
  return(acc)
}

#calculating accuracy for the range of thresholds for each sensor
ID_acc <- conf_all %>% 
    group_by(threshold, ID) %>% 
    summarise(test = accuracy_calc(flowing, binary))
#plotting the output, the accuracy resulting from every threshold for each sensor
ggplot(ID_acc, aes(x = threshold, y = test))+
    geom_line()+
  facet_wrap(~ID)+
  labs(x = "Logistic Threshold",
       y = "Accuracy of prediction",
       title = "Accuracy based on threshold for each sensor")+
  theme_classic()

#make this same plot, but colored by ommission or commission as it contributes to accuracy
#Will need a new accuracy_calc function
om <- function(model, reference){
  mat <- confusionMatrix(factor(model, levels = c(0,1)), factor(reference, levels = c(0,1)))
  ommission <- unname(mat$table[2])
  return(ommission)
}

com <- function(model, reference){
  mat <- confusionMatrix(factor(model, levels = c(0,1)), factor(reference, levels = c(0,1)))
  commission <- unname(mat$table[3])
  return(commission)
}

#commission and ommission as proportions
ID_acc <- conf_all %>% 
    group_by(threshold, ID) %>% 
    reframe(test = accuracy_calc(flowing, binary),
            ommission = om(flowing, binary)/length(flowing),
            commission = com(flowing, binary)/length(flowing)) %>% 
  mutate(ommission = -ommission) %>% 
  pivot_longer(cols = c(commission, ommission), names_to = "type_of_error", values_to = "proportion")
#not proportions but counts
ID_acc2 <- conf_all %>% 
    group_by(threshold, ID) %>% 
    reframe(test = accuracy_calc(flowing, binary),
            ommission = om(flowing, binary),
            commission = com(flowing, binary)) %>%
  mutate(combined = commission - ommission)
  #pivot_longer(cols = c(commission, ommission), names_to = "type_of_error", values_to = "count")
ggplot(ID_acc2, aes(x = type_of_error, y = count))+
  geom_boxplot()
#plotting the output, the accuracy resulting from every threshold for each sensor
ID_acc2 %>% 
ggplot(aes(x = threshold, y = test))+
    geom_line()+
  geom_point(aes(color = combined))+
  facet_wrap(~ID)+
  labs(x = "Logistic Threshold",
       y = "Accuracy of prediction",
       title = "Accuracy based on threshold for each sensor, colored by commission (+)/ommission (-)")+
  theme_classic()+
  scale_color_gradient2(low = ("#DB995A"),
  mid = "white",
  high = ("#586BA4"))

```

```{r}
#plotted date versus discharge, for three different threshold values
start <- "2023-08-07 00:00:00"
stop <- "2023-08-14 00:00:00"


acc_calced <- conf_all %>%
  filter(threshold == 50) %>%
  group_by(DATETIME, Q_mm_day, threshold) %>%
  summarise(test = accuracy_calc(flowing, binary)) %>%
  filter(DATETIME > start & DATETIME < stop)
  ggplot()+
  geom_line(aes(x = DATETIME, y = Q_mm_day, color = test), lwd = 1)+
  facet_wrap(~threshold, ncol = 1)+
  theme_classic()+
  scale_color_gradientn(colors = rev(wes_palette("Zissou1", 100, type = "continuous")))
#make a confusion matrx for each time step, plot versus discharge (will show hysteresis)
#make a confusion matrix for rising and falling limb of each event, maybe even at each sensor
```

```{r}
#testing out package that will automatically extract rising and falling limb
#install.packages("hydroEvents")
library(hydroEvents)
#qdata = WQ_Q$qdata[[1]]
#BF_res = eventBaseflow(qdata$Q_cumecs)
#limbs(data = qdata$Q_cumecs, dates = NULL, events = BF_res, main = "with 'eventBaseflow'")
#Use this plot to manually extract rising and falling limbs
plot <- justQ %>%  
  ggplot(aes(x = DATETIME, y = Q_mm_day))+
  geom_line()+
  geom_point(aes(color = start))

ggplotly(plot)

justQ <- ready2split %>% 
  select(DATETIME, Q_mm_day) %>% 
  arrange(DATETIME) %>% 
  unique() %>% 
  drop_na() %>% 
  mutate(lab = as.numeric(row_number()))

justQ[5812:5826,]
  
BF_res <- eventBaseflow(justQ$Q_mm_day, BFI_Th = 0.5, min.diff = 1)
lick <- limbs(data = justQ$Q_mm_day, 
               dates =justQ$DATETIME, 
               events = BF_res, 
               to.plot = FALSE)

#more tidy attempt
events <- lick %>% 
  mutate(event = as.numeric(row_number())) %>% 
  select(ris.srt:event) %>% 
  pivot_longer(cols = ris.srt:fal.end) %>% 
  rename(lab = value) %>% 
  left_join(justQ, by = "lab") %>% 
  mutate(startOrstop = substr(name, 5,7),
         name = substr(name, 1,3)) 

#sometimes you have to use a for loop
id_events <- function(df, ){
  
}

number_of_events <- length(unique(events$event))
for(i in 1:number_of_events){
  one_event <- events %>% 
    filter(event == i)
  
  #rising start and end
  start_df <- one_event %>% 
    filter(name == "ris" & startOrstop == "srt")
  start <- start_df$DATETIME[1]
  print(start)
  # justQ %>% 
  #   filter(DATETIME >= start & DATETIME <= end)
#output a dataframe with all dates, event number id, rising or falling limb, and end
}

#so frustrated, the wheels are spinning, just ditch this for now and come back to it later.

```

```{r}
#i have a thing called events, how do I get the average accuracy for the range of datetimes between the start and stop of each?

```

```{r}
#what if I try to apply the botter and durighetto techniques?
#calculate local persistency
data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  group_by(ID) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  arrange(pk)
  # left_join(twi_ex, by = "ID") %>% 
  # rename("DATETIME" = datetime,
  #        "twi" = w3_dem_twi) %>% 
  # left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  # #select(DATETIME, ID, binary, twi, Q_mm_day) %>% 
  # mutate(twi = log(twi))

#each node needs to have some amount of stream length associated with it, and that turns on when that node turns on, until the node associated stream length matches the actual
#instead of stream length, how about discharge or stage?
#can I figure out a chunk of the discharge or stage that is associated with each one of these?

#plot persistency versus discharge

pks <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()
  
#how to determine the amount of discharge that increases due to one sensor location turning on
train_values <-  data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  left_join(pks, by = "ID") %>% 
  select(binary, pk, Q_mm_day)

model <- glm(binary ~.,family=binomial(link='logit'),data=train_values)
summary(model)

john_reg <- function(input_logtwi, inputQ){
  #b0, or intercept from Kevin's email from Carrie, might need to redo regression
  b0 <- unname(model$coefficients[1])
  #all ofther coefs from Jensen et al. 2018
  twi_coef <- unname(model$coefficients[2])
  flow_coef <- unname(model$coefficients[3])

  b1x1 <- flow_coef * inputQ
  b2x2 <- twi_coef * input_logtwi

  #logistic regression from Jensen et al. 2018
  p <- exp(b0 + b1x1 + b2x2)/(1 + exp(b0 + b1x1 + b2x2))
  return(p)
}

tested <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  left_join(pks, by = "ID") %>%
  mutate(percent_flowing = john_reg(pk, Q_mm_day) * 100)

add_thresholds <- function(df){
  df$per_90 <- 0
  df$per_90[df$percent_flowing >= 90] <- 1

  df$per_75 <- 0
  df$per_75[df$percent_flowing >= 75] <- 1

  df$per_50 <- 0
  df$per_50[df$percent_flowing >= 50] <- 1
  return(df)
}

out <-  add_thresholds(tested) %>% 
  group_by(ID) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement")

ggplot(out, aes(x = threshold, y = percent_agreement))+
  geom_boxplot()+
  ggtitle("Durighetto local persistency, W3")
```

After meeting with Kevin today- can we predict the local persistency?

```{r}
#using model to predict local persistency
#should I use a linear model?
data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
    slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup() %>% 
  left_join(twi_ex2, by = "ID") %>% 
  left_join(twi_ex, by = "ID") %>% 
  ggplot(aes(x = `10m_dem_twi`, y = pk))+
  geom_point()+
      geom_smooth(method = 'lm')

    

data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    slice_sample(prop = 0.8) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  left_join(pks, by = "ID") %>% 
  select(binary, pk, Q_mm_day)

model <- glm(binary ~.,family=binomial(link='logit'),data=train_values)
summary(model)

john_reg <- function(input_logtwi, inputQ){
  #b0, or intercept from Kevin's email from Carrie, might need to redo regression
  b0 <- unname(model$coefficients[1])
  #all ofther coefs from Jensen et al. 2018
  twi_coef <- unname(model$coefficients[2])
  flow_coef <- unname(model$coefficients[3])

  b1x1 <- flow_coef * inputQ
  b2x2 <- twi_coef * input_logtwi

  #logistic regression from Jensen et al. 2018
  p <- exp(b0 + b1x1 + b2x2)/(1 + exp(b0 + b1x1 + b2x2))
  return(p)
}

tested <- data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  rename("DATETIME" = datetime) %>% 
  left_join(select(q_23_f, c(DATETIME, Q_mm_day)), by = "DATETIME") %>% 
  left_join(pks, by = "ID") %>%
  mutate(percent_flowing = john_reg(pk, Q_mm_day) * 100)
```

Modeling local persistency seems weird. Yesterday talking with Kevin I feel like we strayed even further afield. I think it makes more sense to keep the narrative around trying to predict whether or not a location has water, and we assume is flowing.

```{r}
#have not applied durighetto technique to other watersheds, do that here
#use persistency and stage to model whether the streams are flowing
#falling for the classic blunder- need to re-read data in
#reading in and formatting STIC observations
data_23 <- read_csv("./DataForMary/HB_stic.csv")
data_23$binary <- 1
data_23$binary[data_23$wetdry == "dry"] <- 0
data_23$mins <- minute(data_23$datetime)

data_23 %>% 
  filter(wshed == "W3", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  group_by(ID) %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  arrange(pk)

#read in and format Falls Brook Data
FB_air <- read_csv("./PressureTransducers_11_14_23/FB_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water <- read_csv("./PressureTransducers_11_14_23/FB_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_stage <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701)

#plot persistency versus discharge
#first calculate local persistency (pk)
pks <- data_23 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  right_join(select(FB_stage, c(DATETIME, stage_cm)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()
  
#how to determine the amount of discharge that increases due to one sensor location turning on
train_values <-  
  data_23 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    slice_sample(prop = 0.8, replace = FALSE) %>% 
  rename("DATETIME" = datetime) %>% 
  right_join(select(FB_stage, c(DATETIME, stage_cm)), by = "DATETIME") %>% 
  left_join(pks, by = "ID") %>% 
  select(binary, pk, stage_cm)

model <- glm(binary ~.,family=binomial(link='logit'),data=train_values)
summary(model)

john_reg <- function(input_logtwi, inputQ){
  #b0, or intercept from Kevin's email from Carrie, might need to redo regression
  b0 <- unname(model$coefficients[1])
  #all ofther coefs from Jensen et al. 2018
  twi_coef <- unname(model$coefficients[2])
  flow_coef <- unname(model$coefficients[3])

  b1x1 <- flow_coef * inputQ
  b2x2 <- twi_coef * input_logtwi

  #logistic regression from Jensen et al. 2018
  p <- exp(b0 + b1x1 + b2x2)/(1 + exp(b0 + b1x1 + b2x2))
  return(p)
}

tested <- 
  data_23 %>% 
  filter(wshed == "FB", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  rename("DATETIME" = datetime) %>% 
  right_join(select(FB_stage, c(DATETIME, stage_cm)), by = "DATETIME") %>% 
  left_join(pks, by = "ID") %>% 
  mutate(percent_flowing = john_reg(pk, stage_cm) * 100)


add_thresholds <- function(df){
  df$per_90 <- 0
  df$per_90[df$percent_flowing >= 90] <- 1

  df$per_75 <- 0
  df$per_75[df$percent_flowing >= 75] <- 1

  df$per_50 <- 0
  df$per_50[df$percent_flowing >= 50] <- 1
  return(df)
}

out <-  add_thresholds(tested) %>% 
  group_by(ID) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement")

ggplot(out, aes(x = threshold, y = percent_agreement))+
  geom_boxplot()+
  ggtitle("Durighetto local persistency, FB")
```

```{r}
#now make the graph from above, but for zig zag
#assume that all of the files are read in from above

#read in and format ZZ stage data
ZZ_air <- read_csv("./PressureTransducers_11_14_23/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water <- read_csv("./PressureTransducers_11_14_23/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_stage <- ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701)


#plot persistency versus discharge
#first calculate local persistency (pk)
pks <- data_23 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    group_by(ID) %>% 
  rename("DATETIME" = datetime) %>% 
  right_join(select(ZZ_stage, c(DATETIME, stage_cm)), by = "DATETIME") %>% 
  summarise(pk = sum(binary)/length(binary)) %>% 
  select(ID, pk) %>% 
  ungroup()
  
#how to determine the amount of discharge that increases due to one sensor location turning on
train_values <-  
  data_23 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
    #slice_sample(prop = 0.8, replace = FALSE) %>% 
  rename("DATETIME" = datetime) %>% 
  right_join(select(ZZ_stage, c(DATETIME, stage_cm)), by = "DATETIME") %>% 
  left_join(pks, by = "ID") %>% 
  select(binary, pk, stage_cm)

model <- glm(binary ~.,family=binomial(link='logit'),data=train_values)
summary(model)

john_reg <- function(input_logtwi, inputQ){
  #b0, or intercept from Kevin's email from Carrie, might need to redo regression
  b0 <- unname(model$coefficients[1])
  #all ofther coefs from Jensen et al. 2018
  twi_coef <- unname(model$coefficients[2])
  flow_coef <- unname(model$coefficients[3])

  b1x1 <- flow_coef * inputQ
  b2x2 <- twi_coef * input_logtwi

  #logistic regression from Jensen et al. 2018
  p <- exp(b0 + b1x1 + b2x2)/(1 + exp(b0 + b1x1 + b2x2))
  return(p)
}

tested <- 
  data_23 %>% 
  filter(wshed == "ZZ", mins %in% c(0, 30)) %>% 
  select(datetime, ID, lat, long, binary) %>% 
  rename("DATETIME" = datetime) %>% 
  right_join(select(ZZ_stage, c(DATETIME, stage_cm)), by = "DATETIME") %>% 
  left_join(pks, by = "ID") %>% 
  mutate(percent_flowing = john_reg(pk, stage_cm) * 100)


add_thresholds <- function(df){
  df$per_90 <- 0
  df$per_90[df$percent_flowing >= 90] <- 1

  df$per_75 <- 0
  df$per_75[df$percent_flowing >= 75] <- 1

  df$per_50 <- 0
  df$per_50[df$percent_flowing >= 50] <- 1
  return(df)
}

out <-  add_thresholds(tested) %>% 
  group_by(ID) %>% 
  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement")

ggplot(out, aes(x = threshold, y = percent_agreement))+
  geom_boxplot()+
  ggtitle("Durighetto local persistency, ZZ")

#how does durighetto do through time?
start <- "2023-08-07 00:00:00"
stop <- "2023-08-14 00:00:00"
tested %>% 
  add_thresholds() %>% 
 # group_by(DATETIME, stage_cm) %>% 
    group_by(ID, DATETIME) %>% 

  summarise(threshold_90 = compare(binary, per_90, DATETIME),
            threshold_75 = compare(binary, per_75, DATETIME),
            threshold_50 = compare(binary, per_50, DATETIME)) %>% 
  pivot_longer(cols = starts_with("threshold"), 
               names_to = "threshold", 
               values_to = "percent_agreement") %>% 
filter(DATETIME > start & DATETIME < stop) %>% 
  filter(threshold == "threshold_50") %>%
  summarise(count = length(DATETIME)) %>% 
  print(n = 30)
  #filter(ID %in% seq(15,30,1)) %>% 
ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm, color = percent_agreement), lwd = 1)+
  #facet_wrap(~ID)+
  theme_classic()+
  scale_color_gradientn(colors = rev(wes_palette("Zissou1", 100, type = "continuous")))

```

