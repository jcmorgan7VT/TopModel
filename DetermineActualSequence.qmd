---
title: "DeterminingActualSequence"
format: html
editor_options: 
  chunk_output_type: console
---
10/16/25
I am revisiting paper 1 with new found purpose and clarity- it does not have to be as complicated. It can be simple, and elegant, and teach us about the system.

Later, I can come back and try to build this according to a set of design patterns/ in a more software design perspective.

# Setup and Prepare Inputs
```{r setup}
#loading packages
library(pacman)
p_load(tidyverse, terra, tidyterra, whitebox, scales, wesanderson, caret, plotly,ggnewscale, sf, elevatr, patchwork, ggspatial, zoo, igraph, TSP, ggnetwork, knitr, hydroEvents,ggbreak,
       maptools)

input_w3 <- read_csv("calc_support_inputs_w3.csv")
input_fb <- read_csv("calc_support_inputs_fb.csv")
input_zz <- read_csv("calc_support_inputs_zz.csv")
input_all <- rbind(input_w3, input_fb, input_zz)


```

## Discharge and Stage
### W3
```{r prepare-discharge-W3}
#from TestingFrameworks script

#read in discharge from W3-- input to Carrie's model, discharge in L/s
#q <- read_csv("https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-hbr.1.17&entityid=efc477b3ef1bb3dd8b9355c9115cd849")
#write.csv(q, "HB_5minQ.csv")
q <- read_csv("HB_5minQ.csv")

#find the range of dates that I need discharge for
start <- min(input_w3$datetime)
stop <- max(input_w3$datetime)

#filtering discharge down to the range of dates
q_23 <- q %>% 
  filter(DATETIME > start & DATETIME < stop) %>% 
  #convert to mm/day.
  #converting instantaneous streamflow to mm/day by taking measurement, and scaling   it up as if that was the discharge for the whole day. It is not, it is just at that   moment, but should fix any units/order of magnitude issues
  mutate("Q_mm_day" = Discharge_ls * 0.001 * 86400 / 420000 * 1000) 
q_23$mins <- minute(q_23$DATETIME)

#removing times that are not coincident with STIC observations
q_23_f <- filter(q_23, mins %in% c(0, 30))

ggplot(q_23_f, aes(x  = DATETIME, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#also read in provisional 2024 data
q_24 <- read_csv("w3_discharge_24.csv")

#find the range of dates that I need discharge for
start24 <- ymd_hms("2024-05-15 00:00:00 UTC")
stop24 <- max(input_w3$datetime)

#filtering discharge down to the range of dates
q_24 <- q_24 %>% 
  mutate(datetime = mdy_hm(datetime)) %>% 
  filter(datetime > start24 & datetime < stop24)  
q_24$mins <- minute(q_24$datetime)

#removing times that are not coincident with STIC observations
q_24_f <- filter(q_24, mins %in% c(0, 30))

ggplot(q_24_f, aes(x  = datetime, y = Q_mmperday))+
  geom_line()+
  labs(title = "Discharge from W3, May to July 2024",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#get discharge ready to bind to my other data
q_23_bind <- 
  q_23_f %>% 
  select(DATETIME, Q_mm_day) %>% 
  rename("datetime" = DATETIME
         )

q_24_bind <- 
  q_24_f %>% 
  select(datetime, Q_mmperday) %>% 
  rename("Q_mm_day" = Q_mmperday)
```

#### 2023
```{r W3-2023-id_events}
# Example: discharge dataframe with datetime and discharge
discharge_df <- q_23_plotting %>% 
  select(datetime, Q_mm_day)

bf = baseflowB(q_23_plotting$Q_mm_day, alpha = 0.925)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)
#min.diff = 30
PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 0.5, min.diff = 96)
#plot the events
plotEvents(data = q_23_plotting$Q_mm_day, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")

# Copy of your event table
events <- limbs(data = q_23_plotting$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)
events <- events %>% 
  mutate(ris.srt = ris.srt - 48,
         fal.end = fal.end + 48)
events[1,6] <- 1
events[10,9] <- 4511

#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

#save output for 2023
discharge_df_23 <- discharge_df


discharge_df_23 %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_type))+
  geom_point()

discharge_df_23 %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_id))+
  geom_point()

ttt <- 
input_w3 %>%
  filter(datetime < stop_23,
                     datetime >= start_23) %>% 
  inner_join(discharge_df_23,  by = c("datetime")) %>% 
    filter(binary == 1) %>% 
    group_by(ID, event_id) %>% 
    summarise(start_of_flow = min(datetime)) %>% 
    arrange(start_of_flow) %>% 
  drop_na()

#write a for loop to go through event ids, calculate the sequence for each event, and then find an "average" sequence
for(i in 1:length(unique(ttt$event_id))){
  event <- unique(ttt$event_id)[i]
  
  sequenced_event <- filter(ttt, event_id == event) %>% 
    arrange(start_of_flow) %>% 
      rowid_to_column("Sequence")
  
  if(i == 1) all_events <- sequenced_event
  if(i > 1) all_events <- rbind(all_events, sequenced_event)
}

# add the end of each event
end_of_events <- discharge_df_23 %>% 
    group_by(event_id) %>% 
    summarise(start_of_event = min(datetime),
              end_of_event = max(datetime))

```
```{r W3-2023-plots}
#For each event, plot the grid of presence/absence data in any order

start_23 <- ymd_hms("2023-7-20 00:00:00")
stop_23 <- ymd_hms("2023-10-22 00:00:00")

binary_colors <- c("#DB995A", "#586BA4")

end_of_events <- discharge_df_23 %>% 
    group_by(event_id) %>% 
    summarise(start_of_event = min(datetime) + days(1),
              end_of_event = max(datetime) - days(1))


all_events_ready <- all_events %>% 
  select(-start_of_flow)

# tile plot of presence and absence data
input_w3  %>% filter(datetime < stop_23,
                     datetime >= start_23) %>% 
  left_join(discharge_df_23) %>% 
  left_join(end_of_events, by = "event_id") %>% 
  filter(event_type != "baseflow") %>% 
  left_join(all_events_ready) %>% 
  ggplot(aes(x = datetime))+
  geom_tile(aes(y = Sequence, fill = as.character(binary)))+
  scale_y_continuous("Observed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary_colors,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  geom_vline(aes(xintercept = start_of_event))+
    geom_vline(aes(xintercept = end_of_event))+

  facet_wrap(~event_id, scales = "free")+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 90))

#create a second plot with each of the identified events highlighed on hydrograph
end_of_events <- discharge_df_23 %>% 
    group_by(event_id) %>% 
    summarise(start_of_event = min(datetime),# + days(1),
              end_of_event = max(datetime))# - days(1))

input_w3  %>% filter(datetime < stop_23,
                     datetime >= start_23) %>% 
  left_join(discharge_df_23) %>% 
  left_join(end_of_events, by = "event_id") %>% 
  #filter(event_type != "baseflow") %>% 
  left_join(all_events_ready) %>% 
  ggplot(aes(x = datetime))+
  geom_rect(aes(xmin = start_of_event, xmax = end_of_event, ymin = 0, ymax = 50, fill = as.character(event_id)), na.rm = TRUE)+
  geom_line(aes(y = Q_mm_day))+
  scale_fill_discrete(na.value = "transparent")

```


#### 2024
```{r W3-2024-id_events}
#identifying events
bf = baseflowB(q_24_bind$Q_mm_day, alpha = 0.925)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(q_24_bind$Q_mm_day - bf$bf, threshold = 0.25, min.diff = 30)
#plot the events
plotEvents(data = q_24_bind$Q_mm_day, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = q_24_bind$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- q_24_bind

# Copy of your event table
events <- limbs(data = q_24_bind$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm
events <- events %>% 
  mutate(ris.srt = ris.srt - 48,
         fal.end = fal.end + 48)
events[1,6] <- 1
events[8,9] <- 3030

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

discharge_df_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_type))+
  geom_point()



#now, take this new dataframe and for each event find the start of flow
ttt <- 
input_w3 %>%
  filter(datetime < stop24,
                     datetime >= start24) %>% 
  inner_join(discharge_df_24,  by = c("datetime")) %>% 
    filter(binary == 1) %>% 
    group_by(ID, event_id) %>% 
    summarise(start_of_flow = min(datetime)) %>% 
    arrange(start_of_flow) %>% 
  drop_na()

#write a for loop to go through event ids, calculate the sequence for each event, and then find an "average" sequence
for(i in 1:length(unique(ttt$event_id))){
  event <- unique(ttt$event_id)[i]
  
  sequenced_event <- filter(ttt, event_id == event) %>% 
    arrange(start_of_flow) %>% 
      rowid_to_column("Sequence")
  
  if(i == 1) all_events <- sequenced_event
  if(i > 1) all_events <- rbind(all_events, sequenced_event)
}

# add the end of each event
end_of_events <- discharge_df_24 %>% 
    group_by(event_id) %>% 
    summarise(start_of_event = min(datetime),
              end_of_event = max(datetime))

```
```{r W3-2024-plots}
#For each event, plot the grid of presence/absence data in any order

binary_colors <- c("#DB995A", "#586BA4")

end_of_events <- discharge_df_24 %>% 
    group_by(event_id) %>% 
    summarise(start_of_event = min(datetime) + days(1),
              end_of_event = max(datetime) - days(1))


all_events_ready <- all_events %>% 
  select(-start_of_flow)

# tile plot of presence and absence data
input_w3  %>% filter(datetime < stop24,
                     datetime >= start24) %>% 
  left_join(discharge_df_24) %>% 
  left_join(end_of_events, by = "event_id") %>% 
  filter(event_type != "baseflow") %>% 
  left_join(all_events_ready) %>% 
  ggplot(aes(x = datetime))+
  geom_tile(aes(y = Sequence, fill = as.character(binary)))+
  scale_y_continuous("Observed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary_colors,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  geom_vline(aes(xintercept = start_of_event))+
    geom_vline(aes(xintercept = end_of_event))+

  facet_wrap(~event_id, scales = "free")+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 90))

#create a second plot with each of the identified events highlighed on hydrograph
end_of_events <- discharge_df_24 %>% 
    group_by(event_id) %>% 
    summarise(start_of_event = min(datetime) + days(1),
              end_of_event = max(datetime) - days(1))

input_w3  %>% filter(datetime < stop24,
                     datetime >= start24) %>% 
  left_join(discharge_df_24) %>% 
  left_join(end_of_events, by = "event_id") %>% 
  #filter(event_type != "baseflow") %>% 
  left_join(all_events_ready) %>% 
  ggplot(aes(x = datetime))+
  geom_rect(aes(xmin = start_of_event, xmax = end_of_event, ymin = 0, ymax = 50, fill = as.character(event_id)), na.rm = TRUE)+
  geom_line(aes(y = Q_mm_day))+
  scale_fill_discrete(na.value = "transparent")

```

Chunk straight from mapsForStoryboard, of my current best attempt to extract rising and falling limbs from discharge record for W3. Plan to use to compare how the different methods work during rising, falling, and baseflow.
```{r identifying-events}
start_23 <- ymd_hms("2023-7-20 00:00:00")
stop_23 <- ymd_hms("2023-10-22 00:00:00")


q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start_23 & DATETIME < stop_23) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

#calculate baseflow
#old values greyed
#bf = baseflowB(q_23_plotting$Q_mm_day, alpha = 0.980)
#0.925
bf = baseflowB(q_23_plotting$Q_mm_day, alpha = 0.925)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)
#min.diff = 30
PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 0.5, min.diff = 96)
#plot the events
plotEvents(data = q_23_plotting$Q_mm_day, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = q_23_plotting$Q_mm_day, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm
PoT_res$srt

q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  arrange(datetime) %>% 
  unique() %>% 
  mutate(lab = as.numeric(row_number())) %>% 
  mutate(group = data.table::rleid(lab, cols = PoT_res$srt))

ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))

ranges_extended <- ranges %>% 
  mutate(starts = starts - 48, 
         stops = stops + 48)
ranges_extended[1,1] <- 1
ranges_extended[15,2] <- 5762


ranges[26,2] <- 5762
#trying fuzzy join method?
p_load(fuzzyjoin)
id_events <- q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  mutate(ID = row_number()) |> fuzzy_left_join(ranges_extended, by = c(ID = "starts", ID = "stops"),
                              match_fun = list(`>=`, `<=`)) |> 
   mutate(event_ID = event_ID) |> 
   select(-starts, -stops)

id_events %>% 
  ggplot()+
  geom_point(aes(x = datetime, y = Q_mm_day, color = event_ID))

id_events <- q_23_plotting %>% 
  select(datetime, Q_mm_day) %>% 
  mutate(ID = row_number()) |> fuzzy_left_join(ranges, by = c(ID = "starts", ID = "stops"),
                              match_fun = list(`>=`, `<=`)) |> 
   mutate(event_ID = event_ID) |> 
   select(-starts, -stops)

id_events %>% 
  ggplot()+
  geom_point(aes(x = datetime, y = Q_mm_day, color = event_ID))


```

Doing a thing where I define a sequence based on the activation order during events
```{r}
discharge_df_23 %>% filter(event_id == 1)


#now, take this new dataframe and for each event find the start of flow
ttt <- 
input_w3 %>%
  filter(datetime <= stop_23) %>% 
  inner_join(discharge_df_23,  by = c("datetime")) %>% 
    filter(binary == 1) %>% 
    group_by(ID, event_id) %>% 
    summarise(start_of_flow = min(datetime)) %>% 
    arrange(start_of_flow) %>% 
  drop_na()

#write a for loop to go through event ids, calculate the sequence for each event, and then find an "average" sequence
for(i in 1:length(unique(ttt$event_id))){
  event <- unique(ttt$event_id)[i]
  
  sequenced_event <- filter(ttt, event_id == event) %>% 
    arrange(start_of_flow) %>% 
      rowid_to_column("Sequence")
  
  if(i == 1) all_events <- sequenced_event
  if(i > 1) all_events <- rbind(all_events, sequenced_event)
}

# add the end of each event
end_of_events <- discharge_df_23 %>% 
    group_by(event_id) %>% 
    summarise(start_of_event = min(datetime),
              end_of_event = max(datetime))

discharge_df_23 %>% 
  left_join(end_of_events, by = "event_id") %>% 
  filter(event_type != "baseflow") %>% 
  ggplot(aes(x = datetime, y = Q_mm_day, color = event_type))+
  geom_line()+
  facet_wrap(~event_id, scales = "free_x")
```


Combine the model results above, and the identified events
```{r}
comparison %>% 
  drop_na() %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  group_by(event_type, code) %>% 
  summarise(mean = mean(count)) %>% 
  ggplot()+
  geom_bar(aes(x = event_type, y = mean, fill = code), stat = "identity")

comparison %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  filter(code == "ommission") %>% 
  ggplot()+
  geom_line(aes(x = datetime, y = Q_mm_day, color = count))

#for the graph theory solution, the commission and ommission are symmetrical most of the time... I guess this makes sense if one is out of order, it causes another to be in order, like when I did the lagging method to calculate this
comparison %>% 
  inner_join(discharge_df, by = "datetime") %>% 
  pivot_wider(names_from = code, values_from = count) %>% 
  mutate(diff = commission - ommission) %>% 
  filter(diff != 0) %>% View()


```
### Stage for FB and ZZ
Process salt dilutions
```{r process-dilutions}
source("calcQ.R")
q_combined <- rbind(
calcQ("./salt_dilutions/FB/FB_8_1") %>% mutate("shed" = "FB"),
#calcQ("./salt_dilutions/FB/FB_6_23_fail") %>% mutate("shed" = "FB"),
calcQ("./salt_dilutions/ZZ/ZZ_8_1")%>% mutate("shed" = "ZZ"),
calcQ("./salt_dilutions/ZZ/ZZ_7_1")%>% mutate("shed" = "ZZ"),
#calcQ("./salt_dilutions/ZZ/ZZ_6_23") %>% mutate("shed" = "ZZ"),#different timestep, after fixing gave negative number
calcQ("./salt_dilutions/ZZ/ZZ_6_21")%>% mutate("shed" = "ZZ"),

#no k
calcQ_noK("./salt_dilutions/FB/FB_7_1", mean(c(0.0001120716, 2.621858e-06))) %>% mutate("shed" = "FB"),
calcQ_noK("./salt_dilutions/FB/FB_6_23_Success", mean(c(0.0001120716, 2.621858e-06))) %>% mutate("shed" = "FB"),

#missing FB 8_5, no k and no upstream logger
# ZZ_8_5, no k and no upstream logger
calcQ_noK_oneL("./salt_dilutions/FB/FB_8_5", mean(c(0.0001120716, 2.621858e-06))) %>% mutate("shed" = "FB"),
calcQ_noK_oneL("./salt_dilutions/ZZ/ZZ_8_5", mean(c(9.453437e-05, 4.017994e-06,
                                                    3.962093e-05, 9.693616e-05)))%>% mutate("shed" = "ZZ")
)
```

```{r FB-2023}
FB_air <- read_csv("./PressureTransducers_11_14_23/FB_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water <- read_csv("./PressureTransducers_11_14_23/FB_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  ggplot(aes(x = DATETIME, y = stage_cm))+
  geom_line()+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")

#exclude windows where they were not deployed
FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(stage_cm < 5)

#7/22/2023 00:00:00
#9/20/23 00 - 2023-09-22 00:00:00
#2023-11-12 00:30:00

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>%
  ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")

processed_fb_stage_23 <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
  filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  drop_na() %>%  mutate(Q = roll_mean * fb_stage_Q_model$coefficients[2] +  fb_stage_Q_model$coefficients[1])


processed_fb_stage %>% 
  ggplot(aes(x = DATETIME, y = Q))+
  geom_line()+
  theme_classic()

#output processed salt dilutions
# q_combined %>% filter(shed == "FB") %>% 
#   mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
#   rename(DATETIME = datetime) %>% 
#   left_join(processed_fb_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_fb_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_fb_stage$roll_mean - bf$bf, threshold = 0.4, min.diff = 20)
#plot the events
plotEvents(data = processed_fb_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_fb_stage

# Copy of your event table
events <- limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_df_23 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()


```
```{r FB-2024}
#Read in measurements from 2024

fb_air_25 <-
  rbind(read_csv("./pducers_summer25/fb_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)))

FB_air <- read_csv("./PressureTransducers_summer24/FB_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)) %>% 
  rbind(fb_air_25)

fb_water_25 <- 
  read_csv("./pducers_summer25/fb_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))


FB_water <- read_csv("./PressureTransducers_summer24/FB_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)) %>% 
  rbind(fb_water_25)

processed_fb_stage <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    #filter(DATETIME <= ymd_hms("2024-08-06 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()

#View(processed_fb_stage)

processed_fb_stage %>% 
  filter(DATETIME == ("2024-08-01 11:00:00"))

# processed salt dilutions
dilutions <- q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")
dilutions

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701,
         #apply a rolling mean to get rid of temperature artifacts
         roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+
  
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")+
  geom_point(data = dilutions,
             aes(x = DATETIME, y = stage_cm),
             color = "blue")
dilutions %>% 
  ggplot(aes(x = roll_mean, y = discharge))+
  geom_smooth(method = 'lm', se = FALSE, color = "grey")+
  geom_point()+
  labs(x = "Stage (cm)", y = "Discharge (L/s)",
       title = "FB Stage/Q")+
  #geom_text(aes(label = DATETIME), nudge_x = 1)+
  theme_classic()

fb_stage_Q_model <- lm(dilutions$discharge ~ dilutions$roll_mean)
#try exponential model
model_exp <- lm(log(dilutions$discharge)~ log(dilutions$roll_mean))
summary(model_exp)


processed_fb_stage_24 <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    filter(DATETIME <= ymd_hms("2024-08-06 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  drop_na() %>% 
  mutate(Q = roll_mean * fb_stage_Q_model$coefficients[2] +  fb_stage_Q_model$coefficients[1])

#exponential equation
processed_fb_stage_24 <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    filter(DATETIME <= ymd_hms("2024-08-06 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  drop_na() %>% 
  mutate(Q = exp(log(roll_mean) * model_exp$coefficients[2] +  model_exp$coefficients[1]))

processed_fb_stage %>% 
  ggplot(aes(x = DATETIME, y = Q))+
  geom_line()+
  theme_classic()

all_fb_Q <- rbind(processed_fb_stage_23, processed_fb_stage_24)

all_fb_Q %>% 
  ggplot(aes(x = DATETIME, y = Q))+
  geom_line(na.rm = FALSE)+
  theme_classic()
#output processed salt dilutions
q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_fb_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_fb_stage$roll_mean - bf$bf, threshold = 0.2, min.diff = 20)
#plot the events
plotEvents(data = processed_fb_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_fb_stage

# Copy of your event table
events <- limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_df_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()

fb_limbs <- rbind(stage_df_24, stage_df_23) %>% 
  rename("datetime" = DATETIME)
#BIG PROBLEM
```
```{r FB-2025}
#Read in measurements from 2024

FB_air <- read_csv("./pducers_summer25/fb_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

FB_water <- read_csv("./pducers_summer25/fb_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

# processed salt dilutions
dilutions <- q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")
dilutions

FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701,
         #apply a rolling mean to get rid of temperature artifacts
         roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+
  
  theme_classic()+
  labs(title = "Falls Brook Stage",
       x = "",
       y = "Stage (in)")+
  geom_point(data = dilutions,
             aes(x = DATETIME, y = stage_cm),
             color = "blue")





processed_fb_stage <- FB_water %>% 
  left_join(FB_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    filter(DATETIME <= ymd_hms("2024-07-21 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()


#output processed salt dilutions
q_combined %>% filter(shed == "FB") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_fb_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_fb_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_fb_stage$roll_mean - bf$bf, threshold = 0.2, min.diff = 20)
#plot the events
plotEvents(data = processed_fb_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_fb_stage

# Copy of your event table
events <- limbs(data = processed_fb_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_df_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()

fb_limbs <- rbind(stage_df_24, stage_df_23) %>% 
  rename("datetime" = DATETIME)
#BIG PROBLEM
```


```{r ZZ-2023}
#chunk that reads in stage, converts to proper units
#read in stage, convert to a height
ZZ_air <- read_csv("./PressureTransducers_11_14_23/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water <- read_csv("./PressureTransducers_11_14_23/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))


processed_zz_stage <- ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>%
  #filter(DATETIME <= ymd_hms("2024-07-21 10:30:00 UTC")) %>% 
   filter(DATETIME > ymd_hms("2023-07-22 00:00:00"),
         DATETIME < ymd_hms("2023-11-12 00:00:00")) %>% 
  filter(!(DATETIME >= ymd_hms("2023-09-20 00:00:00") & 
          DATETIME <= ymd_hms("2023-09-22 00:00:00"))) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  drop_na()

#View(processed_zz_stage)
processed_zz_stage %>%
  ggplot()+
    geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+  theme_classic()+
  labs(title = "ZZ Stage",
       x = "",
       y = "Stage (in)")


#identifying events
bf = baseflowB(processed_zz_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_zz_stage$roll_mean - bf$bf, threshold = 0.25, min.diff = 85)
#plot the events
plotEvents(data = processed_zz_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_zz_stage

# Copy of your event table
events <- limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_23 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()


#BIG PROBLEM
```
```{r ZZ-2024}
#chunk that reads in stage, converts to proper units
#read in stage, convert to a height
ZZ_air <- read_csv("./PressureTransducers_summer24/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))

ZZ_water <- read_csv("./PressureTransducers_summer24/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))


processed_zz_stage <- ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>%
  filter(DATETIME <= ymd_hms("2024-07-21 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()

processed_zz_stage %>% ggplot()+
    geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+  theme_classic()+
  labs(title = "ZZ Stage",
       x = "",
       y = "Stage (in)")

#output processed salt dilutions
# q_combined %>% filter(shed == "ZZ") %>% 
#   mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
#   rename(DATETIME = datetime) %>% 
#   left_join(processed_zz_stage, by = "DATETIME")


#identifying events
bf = baseflowB(processed_zz_stage$roll_mean, alpha = 0.99)


#subtract baseflow from discharge
#PoT_res = eventPOT(q_23_plotting$Q_mm_day - bf$bf, threshold = 1, min.diff = 1)

PoT_res = eventPOT(processed_zz_stage$roll_mean - bf$bf, threshold = 0.25, min.diff = 85)
#plot the events
plotEvents(data = processed_zz_stage$roll_mean, events = PoT_res, xlab = "Index", ylab = "Flow (ML/day)", colpnt = "#E41A1C", colline = "#377EB8", main = "eventPOT")



limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = TRUE)#now, extract just the start column, then get each window and run through scoring algorithm


ranges <- data.frame("starts" = PoT_res$srt) %>% 
  mutate("stops" = lead(starts)) %>% 
  mutate(event_ID = row_number(starts))


# Example: discharge dataframe with datetime and discharge
discharge_df <- processed_zz_stage

# Copy of your event table
events <- limbs(data = processed_zz_stage$roll_mean, 
               dates =NULL, 
               events = PoT_res, 
               to.plot = FALSE)#now, extract just the start column, then get each window and run through scoring algorithm

# Initialize the event_type column
discharge_df$event_type <- NA_character_
discharge_df$event_id <- NA_integer_

# Assign "rising" and "falling" from event definitions
for (i in seq_len(nrow(events))) {
  ev <- events[i, ]
  discharge_df$event_id[ev$ris.srt:ev$fal.end] <- i  # Event ID for rising+falling
  
  # Rising limb
  if (!is.na(ev$ris.srt) && !is.na(ev$ris.end) && ev$ris.srt <= ev$ris.end) {
    discharge_df$event_type[ev$ris.srt:ev$ris.end] <- "rising"
  }
  
  # Falling limb
  if (!is.na(ev$fal.srt) && !is.na(ev$fal.end) && ev$fal.srt <= ev$fal.end) {
    discharge_df$event_type[ev$fal.srt:ev$fal.end] <- "falling"
  }
  
  # Baseflow between events
  if (i < nrow(events)) {
    this_fal_end <- ev$fal.end
    next_ris_srt <- events$ris.srt[i + 1]
    if (this_fal_end + 1 <= next_ris_srt - 1) {
      baseflow_idx <- (this_fal_end + 1):(next_ris_srt - 1)
      discharge_df$event_type[baseflow_idx] <- "baseflow"
      discharge_df$event_id[baseflow_idx] <- NA  # baseflow is not part of any event
    }
  }
}

# Optional: label all remaining NA values as "baseflow" (e.g., before first or after last event)
discharge_df$event_type[is.na(discharge_df$event_type)] <- "baseflow"

stage_24 <- discharge_df

discharge_df %>% 
  ggplot(aes(x = DATETIME, y = roll_mean, color = event_type))+
  geom_point()

zz_limbs <- rbind(stage_24, stage_23)%>% 
  rename("datetime" = DATETIME)
#BIG PROBLEM

```

```{r}
#Read in measurements from 2024

zz_air_25 <-
  rbind(read_csv("./pducers_summer25/zz_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)))

ZZ_air <- read_csv("./PressureTransducers_summer24/ZZ_air.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_air = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)) %>% 
  rbind(fb_air_25)

zz_water_25 <- 
  read_csv("./pducers_summer25/zz_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME))


ZZ_water <- read_csv("./PressureTransducers_summer24/ZZ_water.csv", skip = 1) %>% 
  select(2:3) %>% 
  rename(DATETIME = 1,
         pressure_psi_water = 2) %>% 
  mutate(DATETIME = mdy_hms(DATETIME)) %>% 
  rbind(zz_water_25)

processed_zz_stage <- ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701) %>% 
    #filter(DATETIME <= ymd_hms("2024-08-06 10:30:00 UTC")) %>% 
  mutate(minutes = minute(DATETIME)) %>% 
  filter(minutes %in% c(0, 30)) %>% 
  mutate(roll_mean = rollapply(stage_cm ,24,mean,align='center',fill=NA)) %>% 
  drop_na()

View(processed_zz_stage)

processed_zz_stage %>% 
  filter(DATETIME == ("2024-08-01 11:00:00"))

# processed salt dilutions
dilutions <- q_combined %>% filter(shed == "ZZ") %>% 
  mutate(datetime = round_date(datetime, unit = minutes(30))) %>% 
  rename(DATETIME = datetime) %>% 
  left_join(processed_zz_stage, by = "DATETIME")
dilutions

ZZ_water %>% 
  left_join(ZZ_air, by = "DATETIME") %>% 
  mutate(diff_psi = pressure_psi_water - pressure_psi_air) %>% 
  mutate(stage_cm = ((diff_psi*6894.76) / (997 * 9.8)) * 100 * 0.393701,
         #apply a rolling mean to get rid of temperature artifacts
         roll_mean = rollapply(stage_cm ,48,mean,align='center',fill=NA)) %>% 
  ggplot()+
  geom_line(aes(x = DATETIME, y = stage_cm))+
    geom_line(aes(x = DATETIME, y = roll_mean), color = "grey")+
  
  theme_classic()+
  labs(title = "ZZ Stage",
       x = "",
       y = "Stage (in)")+
  geom_point(data = dilutions,
             aes(x = DATETIME, y = stage_cm),
             color = "blue")

dilutions %>% 
  ggplot(aes(x = roll_mean, y = discharge))+
  geom_point()+
  labs(x = "Stage (cm)", y = "Discharge (L/s)",
       title = "ZZ Stage/Q")+
  theme_classic()+
  geom_smooth(method = "lm")

exponential.model <- lm(log(dilutions$discharge)~ dilutions$roll_mean)
summary(exponential.model)

linear <- lm((dilutions$discharge)~ dilutions$roll_mean)
summary(linear)


#now apply model to convert to discharge across time
processed_zz_stage %>% 
  mutate(prediction = (roll_mean^exponential.model$coefficients[2])-exponential.model$coefficients[1]) %>% 
  ggplot(aes(x = DATETIME, y = prediction))+
  geom_line()
  
```


### Number of transitions in each component of hydrograph
```{r W3}
#make companion plot that shows the number of transitions during each part of hydrograph
test <- input_w3 %>% 
  filter(mins %in% c(0, 30)) %>%
  group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

test$state_change <- "none"
test$state_change[test$transition == -1] <- "wetting"
test$state_change[test$transition == 1] <- "drying"

num_trans_w3 <- test %>% 
  group_by(datetime, state_change) %>% 
  summarise(number_of_nodes = length(state_change))

test %>% 
  inner_join(discharge_df_23, by = "datetime") %>%
  group_by(state_change, event_type) %>% 
  summarise(count = n()) %>% 
  pivot_wider(names_from = state_change, values_from = count) %>% 
  kable(format = "markdown")


```
```{r FB}
num_trans_fb <- input_fb %>% 
  filter(mins %in% c(0, 30)) %>%
  group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

num_trans_fb$state_change <- "none"
num_trans_fb$state_change[num_trans_fb$transition == -1] <- "wetting"
num_trans_fb$state_change[num_trans_fb$transition == 1] <- "drying"

num_trans_fb <- num_trans_fb %>% 
  group_by(datetime, state_change) %>% 
  summarise(number_of_nodes = length(state_change))
```
```{r ZZ}
num_trans_zz <- input_zz %>% 
  filter(mins %in% c(0, 30)) %>%
  group_by(ID) %>% 
  mutate(lagged = lag(binary),
         transition = (binary - lagged)) %>% 
  filter(transition %in% c(-1, 1))

num_trans_zz$state_change <- "none"
num_trans_zz$state_change[num_trans_zz$transition == -1] <- "wetting"
num_trans_zz$state_change[num_trans_zz$transition == 1] <- "drying"

num_trans_zz <- num_trans_zz %>% 
  group_by(datetime, state_change) %>% 
  summarise(number_of_nodes = length(state_change))
```

#Determine the wettest and driest times for each watershed
```{r determine-when-it-goes-from-all-dry-to-all-wet}
#make binary columns for wet and dry
rbind(input_w3, input_fb, input_zz)


df1 <- input_w3 %>% 
  group_by(datetime) %>% 
  summarise("percent_flowing" = sum(binary)/length(binary)) %>% 
  filter(percent_flowing <= 0.1) %>% 
  mutate(date = date(datetime)) %>% 
  select(-datetime, -percent_flowing) %>% unique() %>% 
  rowid_to_column("dry_ID") %>% 
  rename("dry_date" = date)

df2 <- input_w3 %>% 
  group_by(datetime) %>% 
  summarise("percent_flowing" = sum(binary)/length(binary)) %>% 
  filter(percent_flowing >= 0.9) %>% 
  mutate(date = date(datetime)) %>% 
  select(-datetime, -percent_flowing) %>% unique()%>% 
  rowid_to_column("wet_ID") %>% 
  rename("wet_date" = date)

#dataframe that tells us when the network went from fully dry to fully wet and vice versa
crossed_up <- cross_join(df1, df2) %>% 
  mutate(difference = wet_date - dry_date) %>% 
  group_by(dry_date) %>% 
  filter(abs(difference) == min(abs(difference))) %>% 
  filter(abs(difference) <= 10 )
  #summarise(min = min(difference),
   #         max = max(difference)


f1 <- Vectorize(function(x, y) list(as.numeric(difftime(y, x, units = "d"))))
outer(df1$dry_date, df2$wet_date, FUN = f1)

#maximum percentage of sensors flowing is 100%
max(spots_flowing$percent_flowing)
#times when stream was at it's maximum extent
spots_flowing$datetime[spots_flowing$percent_flowing == max(spots_flowing$percent_flowing)]
#minimum extent times
min(spots_flowing$percent_flowing)
no_flow <- spots_flowing$datetime[spots_flowing$percent_flowing == min(spots_flowing$percent_flowing)]


filter(q_23_f, DATETIME %in% no_flow)


spots_flowing$datetime[spots_flowing$percent_flowing == min(spots_flowing$percent_flowing)]

#plots for the wettest times for each watershed
plot_test <- filter(all_w3, 
               datetime == "2023-07-03 15:20:00 EDT")

ggplot()+
  geom_tile(data=test, aes(x=x, y=y), fill = "lightgrey")+
  geom_point(data = plot_test, aes(x = long, y = lat, color = wetdry), size = 2)+
  scale_color_manual(drop = FALSE,
                     values = binary,
                    breaks = c("dry", "wet"),
                    labels = c("No flow", "flowing"),
                    name = ""
                    )+
  geom_text(data = plot_test, aes(x = long, y = lat,label = ID), hjust=-0.00025, vjust=0.00025)+
  theme_void()+
  labs(title = "Wettest extent in W3")+
  coord_equal()
#plot of the driest extent in W3

plot_test <- filter(all_w3, 
               datetime == "2023-09-21 16:00:00 EDT")
ggplot()+
  geom_tile(data=test, aes(x=x, y=y), fill = "lightgrey")+
  geom_point(data = plot_test, aes(x = long, y = lat, color = wetdry), size = 2)+
  scale_color_manual(drop = FALSE,
                     values = binary,
                    breaks = c("dry", "wet"),
                    labels = c("No flow", "flowing"),
                    name = ""
                    )+
  geom_text(data = plot_test, aes(x = long, y = lat,label = ID), hjust=-0.00025, vjust=0.00025)+
  theme_void()+
  labs(title = "Driest extent in W3")+
  coord_equal()
```

```{r first-fully-dry-to-wet}
 # Function factory for secondary axis transforms
train_sec <- function(primary, secondary, na.rm = TRUE) {
  # Thanks Henry Holm for including the na.rm argument!
  from <- range(secondary, na.rm = na.rm)
  to   <- range(primary, na.rm = na.rm)
  # Forward transform for the data
  forward <- function(x) {
    rescale(x, from = from, to = to)
  }
  # Reverse transform for the secondary axis
  reverse <- function(x) {
    rescale(x, from = to, to = from)
  }
  list(fwd = forward, rev = reverse)
}

# sec <- with(economics, train_sec(unemploy, psavert))
# 
# ggplot(economics, aes(date)) +
#   geom_line(aes(y = unemploy), colour = "blue") +
#   geom_line(aes(y = sec$fwd(psavert)), colour = "red") +
#   scale_y_continuous(sec.axis = sec_axis(~sec$rev(.), name = "psavert"))

#add precipitation
#add hydrograph to plot
q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

    

start <- ymd_hms("2023-10-20 00:00:00")
stop <- ymd_hms("2023-10-22 00:00:00")

#precip <- 
  read_csv("./HB/dailyWatershedPrecip1956-2024.csv") %>% 
    filter(watershed == "W3") %>% 
        filter(DATE >= start & DATE <= stop) %>% 
    ggplot()+
    geom_bar(aes(x = DATE, y = Precip), stat = "identity")

precip1 <- read_csv("./HB/HBEF_W3precipitation_15min.csv") %>% 
    filter(DateTime >= start & DateTime <= stop) %>% 
    mutate(day = day(DateTime),
           hour = hour(DateTime)) %>% 
    group_by(day, hour) %>% 
    reframe(hourly_precip = sum(precip), across()) %>% 
    mutate(mins = minute(DateTime)) %>% 
    filter(mins == 0) %>%
  rename("datetime" = DateTime) %>% 
    left_join(q_23_plotting, by = "datetime") %>% 
  select(datetime, hourly_precip, Q_mm_day)


sec3 <- with(precip1, train_sec(hourly_precip, Q_mm_day))


water <- precip1 %>% 
    ggplot(aes(x = datetime))+
    geom_col(aes(y = hourly_precip))+
    geom_line(aes(y = sec3$fwd(Q_mm_day)), colour = "blue") +
    scale_y_continuous("Hourly Precipitation (mm)",
                       sec.axis = sec_axis(~sec3$rev(.),
                       name = "Discharge (mm/day)"))+
    labs(x = "",
         y = "Precipitation (mm)")+
      theme_classic()+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y = element_text(angle = 0),
        axis.title.y.right = element_text(angle = 0)
        )


test_IDS <- input_w3 %>% 
    filter(datetime > start & datetime < stop)

remainder <- test_IDS$ID

pks_ordered <- pks_w3 %>% 
filter(ID %in% remainder) %>% 
  arrange(desc(pk)) %>% 
  rowid_to_column("pk_order")

test <- input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(pks_ordered, by = "ID") #%>% 
sec2 <- with(test, train_sec(pk_order, pk))

test <- 
  input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
    mutate(day = day(datetime),
           hour = hour(datetime)) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(precip1, by = c("day", "hour")) %>% 
  left_join(pks_ordered, by = "ID") %>% 
  filter(mins.x == 0)
test2 <- test %>% 
  select(datetime.x, binary, hourly_precip, pk_order)
sec2 <- with(test2, train_sec(pk_order, hourly_precip))



tiles <- test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = pk_order, fill = as.character(binary)))+
  scale_y_continuous("Supposed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23"))+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 0))

p_load(patchwork)

(water)/tiles + plot_layout(heights = c(1, 5))


test2 %>% 
  ggplot(aes(x = datetime.x))+
  geom_tile(aes(y = (pk_order), fill = as.character(binary)))+
  geom_col(aes(y = sec2$fwd(hourly_precip)))+
  ylab("Supposed activation order")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$fwd(.), name = "Instantaneous Q (mm/day)")) +
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(title = "Very dry to very wet",
       subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-10-20 00:00:00"),
                            ymd_hms("2023-10-21 00:00:00"),
                            ymd_hms("2023-10-22 00:00:00")),
                   labels = c("10/20/23",
                              "10/21/23",
                              "10/22/23"))+
  theme_classic()+
    theme(legend.position="right")


```
```{r second}


d <- 1
start <- ymd_hms(paste0("2023-07-15 00:00:00"))
stop <- ymd_hms(paste0(crossed_up$wet_date[d]," 00:00:00"))
#add precipitation
#add hydrograph to plot
q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()

    




precip1 <- read_csv("./HB/HBEF_W3precipitation_15min.csv") %>% 
    filter(DateTime >= start & DateTime <= stop) %>% 
    mutate(day = day(DateTime),
           hour = hour(DateTime)) %>% 
    group_by(day, hour) %>% 
    reframe(hourly_precip = sum(precip), across()) %>% 
    mutate(mins = minute(DateTime)) %>% 
    filter(mins == 0) %>%
  rename("datetime" = DateTime) %>% 
    left_join(q_23_plotting, by = "datetime") %>% 
  select(datetime, hourly_precip, Q_mm_day)


sec3 <- with(precip1, train_sec(hourly_precip, Q_mm_day))


water <- precip1 %>% 
    ggplot(aes(x = datetime))+
    geom_col(aes(y = hourly_precip))+
    geom_line(aes(y = sec3$fwd(Q_mm_day)), colour = "blue") +
    scale_y_continuous("Hourly Precipitation (mm)",
                       sec.axis = sec_axis(~sec3$rev(.),
                       name = "Discharge (mm/day)"))+
    labs(x = "",
         y = "Precipitation (mm)")+
      theme_classic()+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y = element_text(angle = 0),
        axis.title.y.right = element_text(angle = 0)
        )


test_IDS <- input_w3 %>% 
    filter(datetime > start & datetime < stop)

remainder <- test_IDS$ID

pks_ordered <- pks_w3 %>% 
filter(ID %in% remainder) %>% 
  arrange(desc(pk)) %>% 
  rowid_to_column("pk_order")


#sec2 <- with(test, train_sec(pk_order, pk))

test <- 
  input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
    filter(mins == 0) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(precip1, by = "datetime") %>% 
  left_join(pks_ordered, by = "ID") 

sec2 <- with(test, train_sec(pk_order, hourly_precip))



tiles <- test %>% 
  ggplot(aes(x = datetime))+
  geom_tile(aes(y = pk_order, fill = as.character(binary)))+
  scale_y_continuous("Supposed Activation Order")+
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(#title = "Very dry to very wet",
       #subtitle = "W3, 10/20 - 10/22, 2023",
       x = "")+
# scale_x_continuous(breaks=c(ymd_hms("2023-07-20 00:00:00"),
#                             ymd_hms("2023-07-21 00:00:00"),
#                             ymd_hms("2023-07-22 00:00:00")),
#                    labels = c("7/20/23",
#                               "7/21/23",
#                               "7/22/23"))+
  theme_classic()+
    theme(legend.position="right",
          axis.title.y = element_text(angle = 0))


(water)/tiles + plot_layout(heights = c(1, 5))




```
```{r old-plot-presence/absence-with-discharge}

d <- 1
start <- ymd_hms(paste0(crossed_up$dry_date[d]," 00:00:00"))
stop <- ymd_hms(paste0(crossed_up$wet_date[d]," 00:00:00"))


test_IDS <- input_w3 %>% 
    filter(datetime > start & datetime < stop)

remainder <- test_IDS$ID

pks_ordered <- pks_w3 %>% 
filter(ID %in% remainder) %>% 
  arrange(desc(pk)) %>% 
  rowid_to_column("pk_order")

q_23_plotting <- q_23_f %>% 
    filter(DATETIME > start & DATETIME < stop) %>% 
  rename("datetime" = DATETIME)
q_23_plotting %>% 
ggplot(aes(x  = datetime, y = Q_mm_day))+
  geom_line()+
  labs(title = "Discharge from W3, July to Nov 2023",
       x = "",
       y = "Instantaneous Q (mm/day)")+
  theme_classic()


test <- input_w3 %>% 
    filter(datetime > start & datetime < stop) %>% 
  #filter(ID == 17)
  #filter(ID %in% W3_IDs) %>% 
  left_join(q_23_plotting, by = "datetime") %>% 
  left_join(pks_ordered, by = "ID")
sec2 <- with(test, train_sec(pk_order, Q_mm_day))

test %>% 
  ggplot(aes(x = datetime))+
  geom_tile(aes(y = (pk_order), fill = as.character(binary)))+
      geom_line(aes(y = sec2$fwd(Q_mm_day)), color = "white")+
  ylab("Supposed activation order")+
  scale_y_continuous(sec.axis = sec_axis(~sec2$rev(.), name = "Instantaneous Q (mm/day)")) +
  #facet_grid(~name, scales = "free") + 
  scale_fill_manual(drop = FALSE,
                     values = binary,
                    breaks = c(0, 1),
                    labels = c("Dry", "Wet"),
                    name = ""
                    )+
  labs(title = "Very dry to very wet",
       subtitle = "W3, 7/20 - 7/22, 2023",
       x = "")+
scale_x_continuous(breaks=c(ymd_hms("2023-07-20 00:00:00"),
                            ymd_hms("2023-07-21 00:00:00"),
                            ymd_hms("2023-07-22 00:00:00")),
                   labels = c("7/20/23",
                              "7/21/23",
                              "7/22/23"))+
  theme_classic()+
    theme(legend.position="right")

#add hydrograph to plot
crossed_up
```

#Calculate Baseflow permanence, to use instead of overall flow permanence

```{r baseflow-permanence}
baseflow_pk <- discharge_df_24 %>% 
  rbind(discharge_df_23) %>% 
  filter(event_type == "baseflow") %>% 
  inner_join(input_w3) %>% 
  group_by(ID) %>% 
  summarise(baseflow_pk = mean(binary))
arsenal <- "/Users/johnmorgan/Documents/VT Research/HBTopmodel"
write_csv(baseflow_pk, paste0(arsenal, "/baseflow_pk.csv"))

pks_ordered

input_w3 %>% 
  group_by(ID) %>% 
  summarise(pk = mean(binary))

```

# Methods to quantify how similar sequences work
```{r testing-GPT-solutions}
seqs <- list(
  event1 = c(1, 4, 5, 7, 8),
  event2 = c(4, 1, 5, 7, 8),
  event3 = c(1, 3, 4, 5, 7, 9),
  event4 = c(4, 5, 7)
)

lcs_length <- function(x, y) {
  m <- length(x)
  n <- length(y)
  dp <- matrix(0, m + 1, n + 1)

  for (i in seq_len(m)) {
    for (j in seq_len(n)) {
      if (x[i] == y[j]) {
        dp[i+1, j+1] <- dp[i, j] + 1
      } else {
        dp[i+1, j+1] <- max(dp[i+1, j], dp[i, j+1])
      }
    }
  }

  dp[m+1, n+1]
}

n <- length(seqs)
sim <- matrix(0, n, n, dimnames = list(names(seqs), names(seqs)))

for (i in 1:n) {
  for (j in i:n) {
    l <- lcs_length(seqs[[i]], seqs[[j]])
    sim[i, j] <- l
    sim[j, i] <- l
  }
}
sim

p_load(stringdist)
seq_strings <- sapply(seqs, paste, collapse = "-")
d <- stringdistmatrix(seq_strings, method = "osa")  # Optimal string alignment
p_load(scss)
LCS(seqs)



```

```{r}
data <- all_events_ready %>% 
  dplyr::select(event_id, ID) %>% 
  filter(event_id != 1) %>% 
  dplyr::rename("sensor" = "ID")

```

I have already found the sequence for each event, now the question is whether I should try to find a sequence where they "continually flow" from that point on... or just rock with what I've got

Just use all of the sequnces from events in my calc_props suite of functions, and in the modeling exercise